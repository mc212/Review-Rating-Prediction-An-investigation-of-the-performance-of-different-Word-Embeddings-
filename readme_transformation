"""
In this stage, cleaned data are transformed by different techniques consisting of bag of word, Word2Vec, Doc2Vec, TF-IDF, word embeddings, and Paragraph Vector-Distributed Memory (PV-DM) model.
I test the efficient of word2vec and doc2vec by creating a vector for each document then applied on a basic Logistic Regression using the sklearn library with default configuration. For the word2vec, to perform its word vector into one vector for a document, we consider several transformations as follows:
• Mean Word2Vec: compute average all word vector occurred in a single text;
• TF-IDF weighted Word2Vec: adopt TF-IDF as weights for each word embedding before taking average;
• Mean GloVe: average pre-trained GloVe word vectors from a text.
For doc2vec, we directly train doc vectors using the Paragraph Vector - Distributed Memory model [5]. We also concatenate the word2vec and doc2vec vectors to verify if having
more feature could elevate the performance. 
The source code was inherited from https://towardsdatascience.com/nlp-performance-of-differentword-embeddings-on-text-classification-de648c6262b

"""
