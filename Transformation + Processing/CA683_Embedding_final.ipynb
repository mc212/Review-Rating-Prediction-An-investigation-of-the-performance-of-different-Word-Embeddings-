{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA683_Embedding_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PZmLXFoKNp-9",
        "uXiZiMmrp5uF"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlQWu0m5Nar6"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efLnx3x0lX_b",
        "outputId": "c10a51ea-9a41-4ce7-cdfd-eaa3b9355483"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir('/content/drive/My Drive/CA683_Assignment/YelpDataset/20210411')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbDh7C5j-87p"
      },
      "source": [
        "path = './20210411_final_data_265062.csv' #/content/drive/MyDrive/CA683_Assignment/YelpDataset/20210411/20210411_final_data_265062.csv\n",
        "review_col_list2 = [\"stars\",\"text\"]\n",
        "df = pd.read_csv(path, usecols=review_col_list2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJVva1ksludE"
      },
      "source": [
        "#df =pd.read_pickle('./pickle_review_df_preprocessed_104756.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "m0agAPPUl8yG",
        "outputId": "1a41f53e-c41a-4875-c357-9ad708ce403f"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>10pm on a super bowl Sunday and they're alread...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Holy heck this place is amazing. I love their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Amazing shrimp taco.  The others were good but...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>the chips may well be the only thing worth goi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Great food and fun atmosphere.  Nothing bad to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stars                                               text\n",
              "0    1.0  10pm on a super bowl Sunday and they're alread...\n",
              "1    5.0  Holy heck this place is amazing. I love their ...\n",
              "2    4.0  Amazing shrimp taco.  The others were good but...\n",
              "3    3.0  the chips may well be the only thing worth goi...\n",
              "4    4.0  Great food and fun atmosphere.  Nothing bad to..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBA521uSmB5j"
      },
      "source": [
        "df = df[['stars','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfOL8nagmLDs"
      },
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoROGGunp1nG"
      },
      "source": [
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvdIJZP4j4xU"
      },
      "source": [
        "spacy.blank(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRlt2IUCg5K4"
      },
      "source": [
        "from string import punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZmLXFoKNp-9"
      },
      "source": [
        "## declare function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc5w7G6jmRv0"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import glob\n",
        "from smart_open import smart_open\n",
        "import os\n",
        "import gensim\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn import utils\n",
        "from collections import namedtuple, defaultdict\n",
        "import logging\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Convert text to lower-case and strip punctuation/symbols from words\n",
        "def normalize_text(text):\n",
        "\t\"\"\"\n",
        "\tRef: https://stackoverflow.com/questions/20802056/python-regular-expression-1\n",
        "\t:param text: string\n",
        "\t:return:\n",
        "\t\tclean string\n",
        "\t\"\"\"\n",
        "\tnorm_text = text.lower()\n",
        "\t# Replace breaks with spaces\n",
        "\tnorm_text = norm_text.replace('<br />', ' ')\n",
        "\tnorm_text = norm_text.replace('\\n', ' ')\n",
        "\t# Pad punctuation with spaces on both sides\n",
        "\t#norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", r\" \\1 \", norm_text)\\n\n",
        "\tnorm_text = norm_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\treturn norm_text\n",
        "\n",
        "\n",
        "def concat_files(dirname, folders):\n",
        "\t\"\"\"\n",
        "\tConcatenate text from files to one file, and return a file list.\n",
        "\t:param dirname: string of directory\n",
        "\t:param folders: list of folder names\n",
        "\t:return\n",
        "\t\tfiles: list of file paths\n",
        "\t\"\"\"\n",
        "\tfiles = []\n",
        "\n",
        "\tfor fol in folders:\n",
        "\t\toutput = fol.replace('/', '-') + '.txt'\n",
        "\t\ttxt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
        "\t\tprint('{} records in {}...'.format(len(txt_files), output))\n",
        "\t\tfiles.append(output)\n",
        "\n",
        "\t\twith smart_open(os.path.join(dirname, output), 'wb') as n:\n",
        "\t\t\tfor i, txt in enumerate(txt_files):\n",
        "\t\t\t\twith smart_open(txt, 'rb') as f:\n",
        "\t\t\t\t\tone_text = f.read().decode('utf-8')  # from binary to string\n",
        "\t\t\t\t\tone_text = normalize_text(one_text)  # convert to lower-case and strip punctuations\n",
        "\t\t\t\t\tn.write(one_text.encode('utf-8') + b'\\n')  # from string to binary + newline\n",
        "\n",
        "\treturn files\n",
        "\n",
        "\n",
        "def select_imdb(select_num, dirname, files, file_splits, file_sentiments):\n",
        "\t\"\"\"\n",
        "\tSubset and split IMDB dataset into train/test.\n",
        "\t:param select_num: num of rows to select\n",
        "\t:param dirname: directory of txt files\n",
        "\t:param files: list of string name of files\n",
        "\t:param file_splits: list of string on train/test split\n",
        "\t:param file_sentiments: list of string on pos/neg sentiment label\n",
        "\t:return:\n",
        "\t\tlist of namedtuple\n",
        "\t\"\"\"\n",
        "\n",
        "\tsent_doc = namedtuple('sent_doc', ['words', 'tags', 'split', 'sentiment'])\n",
        "\tall_doc = []\n",
        "\tdoc_id = 0\n",
        "\tfor i, fi in enumerate(files[:-1]):\n",
        "\t\ts_ = file_splits[i]\n",
        "\t\tse_ = file_sentiments[i]\n",
        "\n",
        "\t\twith smart_open(os.path.join(dirname, fi), 'rb', encoding='utf-8') as texts:\n",
        "\t\t\tfor line_no, line in enumerate(texts):\n",
        "\t\t\t\tif line_no < select_num:\n",
        "\t\t\t\t\ttokens = gensim.utils.to_unicode(line).split()\n",
        "\t\t\t\t\twords = tokens  # must be a list for doc2vec\n",
        "\t\t\t\t\ttags = [doc_id]  # must be a list for doc2vec\n",
        "\t\t\t\t\tdoc_id += 1\n",
        "\t\t\t\t\tsplit = s_\n",
        "\t\t\t\t\tsentiment = se_\n",
        "\t\t\t\t\tall_doc.append(sent_doc(words, tags, split, sentiment))\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\treturn all_doc\n",
        "\n",
        "\n",
        "class DocPreprocess(object):\n",
        "\n",
        "\tdef __init__(self,\n",
        "\t\t\t\t nlp,\n",
        "\t\t\t\t stop_words,\n",
        "\t\t\t\t docs,\n",
        "\t\t\t\t labels,\n",
        "\t\t\t\t build_bi=False,\n",
        "\t\t\t\t min_count=5,\n",
        "\t\t\t\t threshold=10,\n",
        "\t\t\t\t allowed_postags=['ADV', 'VERB', 'ADJ', 'NOUN', 'PROPN', 'NUM']):\n",
        "\n",
        "\t\tself.nlp = nlp  # spacy nlp object\n",
        "\t\tself.stop_words = stop_words  # spacy.lang.en.stop_words.STOP_WORDS\n",
        "\t\tself.docs = docs  # docs must be either list or numpy array or series of docs\n",
        "\t\tself.labels = labels # labels must be list or or numpy array or series of labels\n",
        "\t\tself.doc_ids = np.arange(len(docs))\n",
        "\t\tself.simple_doc_tokens = [gensim.utils.simple_preprocess(doc, deacc=True) for doc in self.docs]\n",
        "\n",
        "\t\tif build_bi:\n",
        "\t\t\tself.bi_detector = self.build_bi_detect(self.simple_doc_tokens, min_count=min_count, threshold=threshold)\n",
        "\t\t\tself.new_docs = self.make_bigram_doc(self.bi_detector, self.simple_doc_tokens)\n",
        "\t\telse:\n",
        "\t\t\tself.new_docs = self.make_simple_doc(self.simple_doc_tokens)\n",
        "\t\tself.doc_words = [self.lemmatize(doc, allowed_postags=allowed_postags) for doc in self.new_docs]\n",
        "\t\tself.tagdocs = [TaggedDocument(words=words, tags=[tag]) for words, tag in zip(self.doc_words, self.doc_ids)]\n",
        "\n",
        "\n",
        "\tdef build_bi_detect(self, simple_doc_tokens, min_count, threshold):\n",
        "\t\tbi_ = gensim.models.phrases.Phrases(simple_doc_tokens, min_count=min_count, threshold=threshold)\n",
        "\t\tbi_detector = gensim.models.phrases.Phraser(bi_)  # wrapper enhance efficiency\n",
        "\t\treturn bi_detector\n",
        "\n",
        "\n",
        "\tdef make_bigram_doc(self, bi_detector, simple_doc_tokens):\n",
        "\t\tbi_doc_tokens = [bi_detector[doc_tokens] for doc_tokens in simple_doc_tokens]\n",
        "\t\tbi_docs = []\n",
        "\t\tfor bi_tokens in bi_doc_tokens:\n",
        "\t\t\tbi_doc = \" \".join(bi_tokens)  # concatenate back to a sentence\n",
        "\t\t\tbi_docs.append(bi_doc)\n",
        "\t\treturn bi_docs\n",
        "\n",
        "\n",
        "\tdef make_simple_doc(self, simple_doc_tokens):\n",
        "\t\tsimple_docs = []\n",
        "\t\tfor doc_tokens in simple_doc_tokens:\n",
        "\t\t\tsimple = \" \".join(doc_tokens)  # concatenate back to a sentence\n",
        "\t\t\tsimple_docs.append(simple)\n",
        "\t\treturn simple_docs\n",
        "\n",
        "\n",
        "\tdef lemmatize(self, doc, allowed_postags):\n",
        "\t\t\"\"\"\n",
        "\t\tLemmatize words and remove stop_words.\n",
        "\t\t:param doc: text\n",
        "\t\t:param allowed_postags: list of pos tags\n",
        "\t\t:return:\n",
        "\t\t\tlist of tokens\n",
        "\t\t\"\"\"\n",
        "\t\tdoc = self.nlp(doc)\n",
        "\t\ttokens = [token.lemma_ for token in doc if (\n",
        "\t\t\t\ttoken.pos_ in allowed_postags) and (token.text not in self.stop_words)\n",
        "    ]\n",
        "\t\treturn tokens\n",
        "\n",
        "\n",
        "\n",
        "class DocModel(object):\n",
        "\n",
        "\tdef __init__(self, docs, **kwargs):\n",
        "\t\t\"\"\"\n",
        "\t\t:param docs: list of TaggedDocument\n",
        "\t\t:param kwargs: dictionary of (key,value) for Doc2Vec arguments\n",
        "\t\t\"\"\"\n",
        "\t\tself.model = Doc2Vec(**kwargs)\n",
        "\t\tself.docs = docs\n",
        "\t\tself.model.build_vocab([x for x in self.docs])\n",
        "\n",
        "\tdef custom_train(self, fixed_lr=False, fixed_lr_epochs=None):\n",
        "\t\t\"\"\"\n",
        "\t\tTrain Doc2Vec with two options, without fixed learning rate(recommended) or with fixed learning rate.\n",
        "\t\tFixed learning rate also includes implementation of shuffling training dataset.\n",
        "\t\t:param fixed_lr: boolean\n",
        "\t\t:param fixed_lr_epochs: num of epochs for fixed lr training\n",
        "\t\t\"\"\"\n",
        "\t\tif not fixed_lr:\n",
        "\t\t\tself.model.train([x for x in self.docs],\n",
        "\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
        "\t\t\t\t\t\t\t epochs=self.model.epochs)\n",
        "\t\telse:\n",
        "\t\t\tfor _ in range(fixed_lr_epochs):\n",
        "\t\t\t\tself.model.train(utils.shuffle([x for x in self.docs]),\n",
        "\t\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
        "\t\t\t\t\t\t\t\t epochs=1)\n",
        "\t\t\t\tself.model.alpha -= 0.002\n",
        "\t\t\t\tself.model.min_alpha = self.model.alpha  # fixed learning rate\n",
        "\n",
        "\n",
        "\tdef test_orig_doc_infer(self):\n",
        "\t\t\"\"\"\n",
        "\t\tUse the original doc as input for model's vector inference,\n",
        "\t\tand then compare using most_similar()\n",
        "\t\tto see if model finds the original doc id be the most similar doc to the input.\n",
        "\t\t\"\"\"\n",
        "\t\tidx = np.random.randint(len(self.docs))\n",
        "\t\tprint('idx: ' + str(idx))\n",
        "\t\tdoc = [doc for doc in self.docs if doc.tags[0] == idx]\n",
        "\t\tinferred_vec = self.model.infer_vector(doc[0].words)\n",
        "\t\tprint(self.model.docvecs.most_similar([inferred_vec]))  # wrap vec in a list\n",
        "\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "\n",
        "\n",
        "\tdef __init__(self, word_model):\n",
        "\t\tself.word_model = word_model\n",
        "\t\tself.vector_size = word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
        "\t\treturn self\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])\n",
        "\n",
        "\n",
        "class TfidfEmbeddingVectorizer(object):\n",
        "\n",
        "\tdef __init__(self, word_model):\n",
        "\n",
        "\t\tself.word_model = word_model\n",
        "\t\tself.word_idf_weight = None\n",
        "\t\tself.vector_size = word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\t\"\"\"\n",
        "\t\tFit in a list of docs, which had been preprocessed and tokenized,\n",
        "\t\tsuch as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
        "\t\tThen build up a tfidf model to compute each word's idf as its weight.\n",
        "\t\tNoted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
        "\t\t:param\n",
        "\t\t\tpre_processed_docs: list of docs, which are tokenized\n",
        "\t\t:return:\n",
        "\t\t\tself\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\ttext_docs = []\n",
        "\t\tfor doc in docs:\n",
        "\t\t\ttext_docs.append(\" \".join(doc))\n",
        "\n",
        "\t\ttfidf = TfidfVectorizer()\n",
        "\t\ttfidf.fit(text_docs)  # must be list of text string\n",
        "\n",
        "\t\t# if a word was never seen - it must be at least as infrequent\n",
        "\t\t# as any of the known words - so the default idf is the max of\n",
        "\t\t# known idf's\n",
        "\t\tmax_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
        "\t\tself.word_idf_weight = defaultdict(lambda: max_idf,\n",
        "\t\t\t\t\t\t\t\t\t\t   [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
        "\t\treturn self\n",
        "\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIwbybPUNzdg"
      },
      "source": [
        "## declare data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi1D2OL7mUwu"
      },
      "source": [
        "import numpy as np\n",
        "import gensim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXiH1Zrrm87L"
      },
      "source": [
        "all_docs = DocPreprocess(nlp, stop_words, df['text'], df['stars'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "eV8Tv5XYe2zb",
        "outputId": "8dc1176f-8185-47a9-c139-f42457885061"
      },
      "source": [
        "all_docs_df = pd.DataFrame(all_docs.doc_words)\n",
        "print('Shape of dm doc2vec...')\n",
        "display(all_docs_df.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of dm doc2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(275197, 27)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Save dm doc2vec as csv file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCuyhR4FjP2V"
      },
      "source": [
        "\n",
        "all_docs_df['combined'] = all_docs_df.iloc[:, list(range(27))].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxCmBL5VozFT"
      },
      "source": [
        "all_docs_df['combined'] = all_docs_df['combined'].str.replace(\"None\", \"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOE3kZjiuWiM",
        "outputId": "c21ebc3a-eb82-4516-bf67-8e0e5d6c534e"
      },
      "source": [
        "all_docs_df[all_docs_df.columns[-1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         pm super bowl sunday close weak wonder hard ro...\n",
              "1         holy place amazing love chicken taco far favor...\n",
              "2         amazing shrimp taco good shrimp good come     ...\n",
              "3         chip thing worth go salsa good expensive booze...\n",
              "4         great food fun atmosphere bad trip vegas reaso...\n",
              "                                ...                        \n",
              "275192    good bubble tea refreshing delicious great var...\n",
              "275193    yesterday order extra long bbq cheeseburger or...\n",
              "275194    great service great sushi great korean style f...\n",
              "275195    great place sushi seaweed salad salmon roll fr...\n",
              "275196    literally good indian long long time super hig...\n",
              "Name: combined, Length: 275197, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9l6aKCpTt9"
      },
      "source": [
        "all_docs_df.to_csv(os.path.join('./word_embedding/', 'all_docs_df_string.csv'), index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "3aaCe44rfxB1",
        "outputId": "0e2fad32-40af-4531-efe0-3c02692aa56a"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>10pm on a super bowl Sunday and they're alread...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Holy heck this place is amazing. I love their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Amazing shrimp taco.  The others were good but...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>the chips may well be the only thing worth goi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Great food and fun atmosphere.  Nothing bad to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stars                                               text\n",
              "0    1.0  10pm on a super bowl Sunday and they're alread...\n",
              "1    5.0  Holy heck this place is amazing. I love their ...\n",
              "2    4.0  Amazing shrimp taco.  The others were good but...\n",
              "3    3.0  the chips may well be the only thing worth goi...\n",
              "4    4.0  Great food and fun atmosphere.  Nothing bad to..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "KMo7yNj9fqft",
        "outputId": "10826f12-6bed-4147-ab87-4ea6b2d602f6"
      },
      "source": [
        "all_docs_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pm</td>\n",
              "      <td>super</td>\n",
              "      <td>bowl</td>\n",
              "      <td>sunday</td>\n",
              "      <td>close</td>\n",
              "      <td>weak</td>\n",
              "      <td>wonder</td>\n",
              "      <td>hard</td>\n",
              "      <td>rock</td>\n",
              "      <td>die</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>holy</td>\n",
              "      <td>place</td>\n",
              "      <td>amazing</td>\n",
              "      <td>love</td>\n",
              "      <td>chicken</td>\n",
              "      <td>taco</td>\n",
              "      <td>far</td>\n",
              "      <td>favorite</td>\n",
              "      <td>great</td>\n",
              "      <td>customer</td>\n",
              "      <td>service</td>\n",
              "      <td>round</td>\n",
              "      <td>awesome</td>\n",
              "      <td>experience</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>amazing</td>\n",
              "      <td>shrimp</td>\n",
              "      <td>taco</td>\n",
              "      <td>good</td>\n",
              "      <td>shrimp</td>\n",
              "      <td>good</td>\n",
              "      <td>come</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chip</td>\n",
              "      <td>thing</td>\n",
              "      <td>worth</td>\n",
              "      <td>go</td>\n",
              "      <td>salsa</td>\n",
              "      <td>good</td>\n",
              "      <td>expensive</td>\n",
              "      <td>booze</td>\n",
              "      <td>cheap</td>\n",
              "      <td>woman</td>\n",
              "      <td>draw</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great</td>\n",
              "      <td>food</td>\n",
              "      <td>fun</td>\n",
              "      <td>atmosphere</td>\n",
              "      <td>bad</td>\n",
              "      <td>trip</td>\n",
              "      <td>vegas</td>\n",
              "      <td>reasonable</td>\n",
              "      <td>pricing</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0       1        2           3        4   ...    22    23    24    25    26\n",
              "0       pm   super     bowl      sunday    close  ...  None  None  None  None  None\n",
              "1     holy   place  amazing        love  chicken  ...  None  None  None  None  None\n",
              "2  amazing  shrimp     taco        good   shrimp  ...  None  None  None  None  None\n",
              "3     chip   thing    worth          go    salsa  ...  None  None  None  None  None\n",
              "4    great    food      fun  atmosphere      bad  ...  None  None  None  None  None\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4rDdUxnntQd",
        "outputId": "9d111dbd-1b42-42ba-9699-69bb1a8b37bd"
      },
      "source": [
        "print('Demo of doc words...')\n",
        "all_docs.doc_words[5][:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Demo of doc words...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['price',\n",
              " 'high',\n",
              " 'food',\n",
              " 'good',\n",
              " 'service',\n",
              " 'awesome',\n",
              " 'gamble',\n",
              " 'hard',\n",
              " 'rock',\n",
              " 'eat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEFXXS4Xn3GF",
        "outputId": "38ca3611-1dcc-42ba-8b1c-358e59bf042e"
      },
      "source": [
        "all_docs.labels.iloc[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYOyExtcn-_D",
        "outputId": "5453b932-0a37-43da-8689-6892ea7eef8f"
      },
      "source": [
        "import multiprocessing\n",
        "import sys\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "workers = multiprocessing.cpu_count()\n",
        "print('number of cpu: {}'.format(workers))\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of cpu: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUl9GU4xKCFs"
      },
      "source": [
        "Word2Vec:\n",
        "size: Using a higher dimensionality than vocabulary size would more-or-less guarantee 'overfitting'. The training could tend toward an idiosyncratic vector for each word – essentially like a 'one-hot' encoding – that would perform better than any other encoding, because there's no cross-word interference forced by representing a larger number of words in a smaller number of dimensions.\n",
        "https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcI4NtO6oCn1"
      },
      "source": [
        "word_model = Word2Vec(all_docs.doc_words,\n",
        "                      min_count=5,\n",
        "                      size=200,\n",
        "                      window=5,\n",
        "                      workers=workers,\n",
        "                      iter=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59GxcWBsq3L-"
      },
      "source": [
        "#word_model1 = Word2Vec(all_docs.doc_words,\n",
        "                      min_count=5,\n",
        "                      size=100,\n",
        "                      window=5,\n",
        "                      workers=workers,\n",
        "                      iter=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VZrTZSy8q4q"
      },
      "source": [
        "## closed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ExiXJufI7Ri"
      },
      "source": [
        "_save_word2vec_format()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR1pEjJ0J6ke"
      },
      "source": [
        "class SaveEmbeddingVectorizer(object):\n",
        "\n",
        "\n",
        "\tdef __init__(self, word_model):\n",
        "\t\tself.word_model = word_model\n",
        "\t\tself.vector_size = word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
        "\t\treturn self\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "4TEhXpN1KmAW",
        "outputId": "104935e0-712f-43c0-bf29-af67671b4c8a"
      },
      "source": [
        "vec_tr = SaveEmbeddingVectorizer(word_model)\n",
        "word_vec = vec_tr.transform(all_docs.doc_words)\n",
        "\n",
        "print('Demo of word averaging doc vector...')\n",
        "display(word_vec[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:cannot compute average owing to no vector for ['这家我只想说真的很差', '什么玩意啊', '菜一般不说还巨贵', '服务态度巨差无比', '四个人吃饭水都不给我们喝', '太过分了', '盘子也不给我们清理', '以后再也不想来这里吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前段時間易手裝修後', '店內環境走年輕風格', '誇張的日本手繪海產圖案', '特別搶眼', '雪櫃內有鮮即食生蠔', '魚刺身等', '除了主打party', 'tray外', '也有單點壽司同日式刺身蓋飯']\n",
            "WARNING:root:cannot compute average owing to no vector for ['一想到巴黎的著名地標', '就是艾菲爾鐵塔', '凱旋門', '拉斯維加斯有一間飯店就是這樣啦', '飯店的門口就有鐵塔', '凱旋門喔', '不要說飯店外的建築', '天花板就是外面的天空', '會令人感受不出是在室內喔', '如果跟閃光們走在這', '很有甜蜜蜜的感覺喔']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ーー', 'ing李敏', '哦所以你你咋去搜个', 'xp啊']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Demo of word averaging doc vector...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.73100859,  0.53037333, -0.58329123,  1.62846267,  0.87991852,\n",
              "        0.27366176,  0.37439668,  0.31001189,  0.5886091 ,  0.25364378,\n",
              "       -0.31257653, -2.46649408,  1.61072803, -0.43540999,  0.32875502,\n",
              "        0.15425001, -0.03543835,  0.00551971, -1.01624215, -0.05144159,\n",
              "       -0.60872883,  0.47550207, -0.48558488,  0.35515547, -0.79492766,\n",
              "        0.20305444,  0.1457051 ,  0.06479654,  1.43793595,  0.67293614,\n",
              "        0.98830068, -0.49632972,  0.64017451, -0.28098071, -1.48678672,\n",
              "       -0.77395618,  0.40729362, -0.16168569,  0.5208267 ,  0.25416315,\n",
              "        1.6975286 ,  0.36621776, -0.32313335, -1.1038897 , -0.01257934,\n",
              "        0.33968061, -0.21247689,  0.1328427 , -0.59084111, -0.90501291,\n",
              "       -0.08454107,  1.32522094, -1.07139111,  0.8720004 , -0.4511869 ,\n",
              "        1.09826005,  0.02371061,  0.85070676,  1.84448028, -0.88466096,\n",
              "        0.76252353, -0.23367882,  0.25538737,  0.19234794,  0.50928164,\n",
              "        0.8666243 , -0.04602426, -0.15665436,  0.18787341,  0.44421849,\n",
              "        0.40740186, -0.82032061, -0.97390276,  0.79648095,  0.49144036,\n",
              "       -0.95147175,  1.04362333, -0.25424314,  0.80665869, -0.39110982,\n",
              "       -0.575692  ,  1.06030452, -0.121626  , -0.73041803,  0.1197454 ,\n",
              "       -0.54770732, -0.69334513,  1.60030258,  0.31754154,  0.79214644,\n",
              "        0.12595156, -0.33731556,  0.34210622, -0.35988128,  1.73341882,\n",
              "       -0.18833211, -0.53934711, -1.15561938, -0.75069714, -0.76260138,\n",
              "        0.61503923, -0.58403188, -0.1025337 ,  0.28153157,  0.78174567,\n",
              "       -0.66559035, -0.34814185,  0.92439026, -0.45527518,  0.45488033,\n",
              "       -0.67984784,  0.10726588, -0.40822235, -0.21500292, -0.69207114,\n",
              "        1.26808977,  0.0893074 ,  0.17935847,  0.30570918,  0.42604738,\n",
              "        0.85638678, -0.80796742,  1.21044517,  1.67181766,  0.32763827,\n",
              "        0.7397002 , -1.05722046, -0.11264874,  0.22106057,  0.03417599,\n",
              "       -0.24382438, -0.40796959, -0.58053976, -0.71063489, -0.59781796,\n",
              "       -0.54278988,  1.1355356 , -0.70578456, -0.46353334, -0.70288235,\n",
              "        0.55593669,  0.47708112, -0.53150254,  0.18034528,  1.08877027,\n",
              "        0.06136208,  0.24156913, -0.06826109, -0.2600283 ,  0.63206744,\n",
              "       -0.16180986, -0.47234312,  0.19586442,  1.01776433, -2.19843292,\n",
              "       -0.37164819,  0.59469354,  0.06910881,  0.78510815, -1.44408417,\n",
              "        0.02942184,  1.23005235, -0.56351721,  0.02518475, -0.3462908 ,\n",
              "        0.79749048, -0.02568322,  0.28685704, -0.12677522, -0.07758206,\n",
              "        0.47002125,  0.68281764, -0.30960944,  1.40788782, -0.06584705,\n",
              "       -0.10770874,  1.35157526,  0.8263793 , -0.06491861,  0.09569702,\n",
              "       -0.56880641, -0.38218459,  0.31872618, -0.40712216,  0.28803813,\n",
              "        0.05622922,  0.57397771,  1.37335753,  1.21520686,  1.11720252,\n",
              "       -0.12323731,  0.6684224 , -0.83714855, -1.24918306, -0.47368929,\n",
              "        0.27137941,  0.43515339,  0.27228937, -0.59560412, -0.02106233])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdL526ULK1xV"
      },
      "source": [
        "np.savetxt(os.path.join('./','word_vec.csv'), word_vec, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWjMcpFs27hp",
        "outputId": "edbd0e4a-481b-4adc-819d-55012f870557"
      },
      "source": [
        "word_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1063674, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfOmwiES8vxG"
      },
      "source": [
        "## declare function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbW-lZkZoMPM"
      },
      "source": [
        "class MeanEmbeddingVectorizer(object):\n",
        "\n",
        "\n",
        "\tdef __init__(self, word_model):\n",
        "\t\tself.word_model = word_model\n",
        "\t\tself.vector_size = word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
        "\t\treturn self\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZPYSqlmohyj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "860f838a-fec0-4468-ad01-0bf3c36318ae"
      },
      "source": [
        "mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
        "doc_vec = mean_vec_tr.transform(all_docs.doc_words)\n",
        "\n",
        "print('Demo of word averaging doc vector...')\n",
        "display(doc_vec[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:cannot compute average owing to no vector for ['お店の接客レヘルは最低てす', 'それと', 'とにかく遅いてす', 'オーターを聞きに来るまて', '飲み物か出て来るまて', 'ヒサか来るまて', '時間てした', '他のお店に行きましょう']\n",
            "WARNING:root:cannot compute average owing to no vector for ['クロワッサンも美味しいけと', '種類あるキーシュかおススメ', 'おススメ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['係la就中意吃', '雖然唔吃辣', '但係佢唔辣既都好吃', '特別佢個綠豆湯', '超解渴', '仲有個震騰騰既蒸蛋', '好過蘿蔔臣', '佢話我太短喔', '要夠長先得喔']\n",
            "WARNING:root:cannot compute average owing to no vector for ['非常nice的面馆咧', '店里的小姐姐很nice', '面就更不用说啦', '光看图片就超级有食欲了', '味道更加好哦', '美味美味美味呀', '值得一试哦', '总之真的很好吃就对了', '店面很大', '装修很有味道', '个人非常喜欢到颜值高的店', '享用美食', '嘻嘻嘻']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道很正', '面很足', '配菜少了点', '绿豆汤不错', '但不喜欢里面的糖的比例', '肥肠味道一般', '牛肉比较淡', '小菜还可以', '徵儿说干拌牛肉不该加那么多辣椒', '然后猪嘴肉太淡了', '徵儿说应该加酱油而不是辣酱', '我指出应该是辣椒面而不是辣椒酱', '徵儿说吃到最后完全没有感觉']\n",
            "WARNING:root:cannot compute average owing to no vector for ['没想到能在vegas吃到豌杂面', '味道正宗', '吃货一本满足', '好吃到哭', '而且面的选择也多', '即使不吃辣也有其它面可以选', '还有抄手', '不过我们这次没有尝试', '下次会再来试试', '而且停车也不成问题', '位置很好找', '我们是中午来的', '也没有等位', '总体很满意']\n",
            "WARNING:root:cannot compute average owing to no vector for ['从面到附送的小菜', '绿豆汤', '全都超级棒', '在三几顿西餐自助后来这吃碗面', '有种灵魂瞬间回归的感觉', '墙裂', '分推', '来vegas玩', '喜欢川味的小朋友', '请一定一定打卡', '滋味小面', '感觉下次再来vegas', '有一半是冲着这个面来的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['挺不错的', '味道都挺好', '食物卖相也不错', '唯一遗憾蒸蛋没啦', '跑来中国城吃饭', '周围也蛮好停车的', '点了一个面一个红油抄手', '一个小菜', '又送了一个木耳', '酸酸辣辣的', '很多地方的红油抄手都会偏甜', '就感觉很不正宗', '但是他们家的完全不会', '所以觉得很好哦', '然后面里的炒黄豆很好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['時に遅めのランチて訪問', 'この時間ても', '分くらい待ったから本当に人気店', 'コースて注文', '生カキも食へられたし', 'お肉も美味しいし', 'コスハ良し', '味よし', '店員さんも感しよかったし', 'おススメ', 'ラスヘカス旅行時は行く価値あり']\n",
            "WARNING:root:cannot compute average owing to no vector for ['点的是兰州拉面', '红油抄手和干炒牛河', '兰州拉面就是机器制作的挂面类的', '不是拉面', '红油抄手基本上没有馅', '全都是皮', '干炒牛河相对来说比较甜', '总体来说又贵又不好吃', '建议的话可以往郊区走一点', '可以吃到便宜又正宗的中餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヘカスの大通りに面している', 'メキシカン料理', '味は普通に美味しい', '店内のテコレーションかお洒落']\n",
            "WARNING:root:cannot compute average owing to no vector for ['如果是台灣的朋友看到這篇留言', '不要猶豫了一定要去', '完全的台灣味', '吃了會想哭', '一百分的服務和餐點', '份量完全飽', '但是烹調不馬虎非常用心製作', '這是我們在美國吃到最好吃的東西', '我永遠不會忘記的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['家在陕西超赞', '超好吃', '严重推荐', '大盘鸡', '羊肉串', '孜然羊肉', '肉夹馍', '此处口水', '各种各样的正宗西北面食', '无可抵抗', '超级严重推荐', '老板人很nice', '幽默直爽实在', '啤酒随便灌', '白酒三斤半', '会有折扣哦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['春巻きやエヒのカクテル', 'お近くに立ち寄りの際には是非']\n",
            "WARNING:root:cannot compute average owing to no vector for ['超市雖然不算大', '但種類都幾豐富和新鮮', '特別係韓國自製的各式泡菜', '多種', '還可以試吃後才選購', '遊水海鮮類種類不多', '反而韓國出名的烤肉類就幾多下', '當然係牛肉做主打', '雪花肉紋好吸引', '超市還連著幾間小食店', '買菜和現成熟食都一應俱全', '超市係馬路上個大招牌好明顯', '好易穩到', '隔離有kfc', '小時營業']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しいステーキハウスてす', '値段も手頃たと思います', 'オーターを失敗して', 'メイン料理を', '人シェアってお店的には', 'はっ', 'って感してすよね', 'すいません', '量かわからなくて', 'また是非トライしたいお店てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['タイ料理屋さん', '夕食分と朝食分を持ち帰りて注文', '自分', '日本人なんたけと', '辛さはとのくらいか良さそう', 'って聞いて', 'ミティアムにしたんたけと', 'ミティアムてもたいふ辛かった', 'ても', 'また行きたいお店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['一言以蔽之', '难吃', '鸡肉吃到一嘴盐巴', '猪肉感觉吃到洗洁精', '鱼肉水水的', '酸辣汤一股怪味', '我是吃得恶心了', '店内选择还算多', '补菜也算快', '十分昏暗', '厕所设计不合理', '而且垃圾桶爆满', '厕纸摆地上', '这个价格的', '团餐', '真的是很糟糕', '可惜周围没其它吃的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每人', '美金', '专门坑旅游团', '食物质量是真的差', '白菜是苦的', '还没有筷子', '我们为了环保所以不提供筷子', '这么低劣的借口降低成本', '如果有时间哪怕是多一点点时间', '别付钱给导游来这家', '去旁边的餐厅', '平时不写评论', '但我是真的被苦白菜激怒了', '别的不说了', '祝这家早日关门', '傻逼自助餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['海鸥假期用来骗游客钱的垃圾餐厅', '把国内旅行团的恶习都带到美国', '故意选在偏僻位置', '还必须在此停留一个多小时', '不吃的客人不能使用洗手间', '门口不提供休息点', '大巴车门关上', '就是要游客无处可去只能进去吃', '汉堡什么的都有']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヒオリアて行くなら鉄板', '間違いない', '堅い', 'もちろんスイーツもクット', 'というのもいいと思います', 'お店の内装にも注目してくたさい']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['ここか出てきた', 'ロフスターやサーモンなと', 'シーフートを楽しめる', 'たいたい値段は', '程度']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我是湖南辣妺子', '来美国', '太久没有吃过这么正宗', '这么辣的湖南菜', '在这里真的吃出了家乡味', '酸辣牛肉丝', '家常茄子煲', '炒菜心实在是下饭', '给你十个赞', '希望你们成功的经营下去', '最好来波特兰开', '吃完了后才想着拍照']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太正宗了', '在美国吃过十来家湘菜馆', '这一家味道数一数二', '份量最足', '强烈推荐活水煮活鱼和炒土鸡', '跟家里一个味道', '价格也很便宜', '三个人点四个菜大概六七十', '吃不完的打包回去再吃一餐都够了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['新乐园的干炒牛河', '牛肉炒的非常老', '很硬', '很难咀嚼', '也不入味', '配菜有胡萝卜丝', '豆芽', '芝麻', '洋葱', '青葱给的比较多', '切的中长段', '虽然整道菜的颜色很不错', '但是味道真的不怎么样', '河粉似乎切块了', '还都坨在一起了', '量也很少', '河粉颜色分布不均匀', '没有味道', '很不推荐了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['思う存分に楽しめる', '年齢のせいもあるか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['重庆吃的就是比较重口油多', '师傅是重庆人', '暂时还没有踩雷', '鱼香肉丝', '泡脚兔丁', '伤心凉粉', '小面都不错', '有空尝试其他的菜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['以前此地址之山窿粉己轉了', '次經營者', '最近由', '看見了也流口水吧味道非常不錯', '還有麻辣雞鍋大家也去試一下吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['他們的點心分兩類', '傳統的蒸點以及在開放式廚房現做', '現煎', '現烤', '等等其它小點心和特式點心', '上桌的菜肴顯現出十足的新鮮美味', '雖然收', '的茶資', '但茶的品質比一般飲茶的地方好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['樓下的先生您好', '老闆是台灣人喔', '我馬上幫您反應您的豬排', '雞翅問題', '店家給我看了每一個order', '今天很不巧沒有人外帶豬排單子', '你是不是評論錯對手了啊', '我下班都要帶雞翅回來啃呢', '我想你應該點錯地方了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星', '永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星', '永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスに何度か来ていますか', 'スハイシー豚骨かオススメ', 'チャーハンも美味しいてす', '昨日に引き続き', '日連続の来店てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しくて好きなお店てす', '辛さも選へるし', 'お店の人もナイスな人か多いてす', 'カーリックナンも美味しい', '何故かいつも空いてるんたけと', '美味しいと思うんたけとな']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['息子のオーターてあるトンコツ塩', 'も試したか', 'これもまた美味']\n",
            "WARNING:root:cannot compute average owing to no vector for ['噗了', '凑成', '成vv', '瞅瞅']\n",
            "WARNING:root:cannot compute average owing to no vector for ['噴水を見る合間に', 'ラスヘリーマカロンと', 'もっとクリームっほい', 'ケーキも気になったのてすか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['その下見として訪問しました', 'とりあえすこの日は', 'ヒサと言うよりは', '全てにアホカトか乗っており']\n",
            "WARNING:root:cannot compute average owing to no vector for ['但不知道今天的仙草怎麼了', '軟軟的一點咬口都沒有tt']\n",
            "WARNING:root:cannot compute average owing to no vector for ['臭豆腐加泡菜真是一絕', '味道很好', '更添風味', '牛肉麵整體評分算是中上', '有吃過更好的牛肉麵', '這家算是也可以', '麵條本身感覺比較粗', '比較硬', '沒有彈性', '湯頭可以', '份量夠大', '價格屬於比較經濟實惠的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在網站看見四星', '過來試試', '餸菜選擇很多', '有上海', '四川', '廣東', '雞羊牛海鮮樣樣有', '老公二人只點了簡單的', '包括鼓椒炒蜆', '楊州炒飯和畺汁芥蘭', '好味', '地道廣東菜味道', '送湯和糖水', '也很好味', '無味精', '價錢絕無收貴']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ここは美味しい', 'と確信しました', 'きつねうとん', 'カレーうとん', '雲丹クリームうとん', '日本と変わらぬお味にリヒ決定']\n",
            "WARNING:root:cannot compute average owing to no vector for ['行mall除了选星爸爸饮品外', 'aroma的饮品都几唔错', '位于vaughan', 'mills的分店都几大下', '有卡座有普通座位', '或者又可以坐商场过道的座位', '平时少机会饮开心果拿铁', '距既出品唔错', '饮到尾可以吃到果仁碎', '口感丰富', '拿铁香滑', '唔错唔错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本人の友達と', '人て食へに行きました', 'スタッフの方はフレントリーて', '話しやすく', '英語かわからない私にも親切に', '話していたたけました', '甘めのハーヘキューソースて', '日本人の口に合った味てした', 'お肉の種類も豊富てよかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我們點的東西並沒有上齊', '服務生態度普遍都比較差', '買單時候自動charge了小費', '懷疑食材不是很新鮮', '食物吃起來就是一般般', '以後不會再去了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['靠近马路边', '都改名几次了', '一直都系做中餐厅', '饮茶就未试过', '吃饭就呢次试左', '刚好有酒席', '所以上菜稍微耐左d', '份量唔细', '全部菜式稍微油多左d', '总体来说还算可以啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['中文名叫福临门酒家', '近sheppard', 'midland怡东广场内', '车位吾算特别多', '开业都几耐了', '内里环境比较旧', '地方吾算大', '吾知系咪无洗地毯的关系', '好多蚊子', '边吃边咬', '好痕吃得吾舒服', '味道就普通啦', '吾算特别好吃', '周末晚市都小猫三两只甘']\n",
            "WARNING:root:cannot compute average owing to no vector for ['身为一个西北人', '对于这家店还是蛮失望的', '点了烤羊肉', '烤羊腰', '炒面和羊肉汤面', '烤肉和烤腰子勉强ok', '但是炒面根本就是河南烩面的感觉', '羊肉汤面羊肉味很淡', '像是味精汤', '另外他的音乐', '我以为会说民族音乐', '结果是时而中文', '时而英文的流行乐', '看着墙上的新疆壁画', '听着这样的音乐', '感觉很不搭']\n",
            "WARNING:root:cannot compute average owing to no vector for ['フューションほいけと', 'うとん', '蕎麦', 'ラーメン', '持ち帰る時は']\n",
            "WARNING:root:cannot compute average owing to no vector for ['老外的连锁汉堡包店', '分店相对比m字头快餐店少好多', '位于著名伊顿中心对面', '招牌醒目好易看到', '时不时有不同的特价美食', '汉堡包份量比较大', '馅料多', '味道比起某店更好吃更滋味', '薯条就一般', '环境比较旧', '或者以后翻新下会更舒服']\n",
            "WARNING:root:cannot compute average owing to no vector for ['东方海鲜酒家的干炒牛河', '细型的河粉', '炒得很碎', '有糊块', '还有很多黏连在一起的块粉', '不仅河粉不入味', '还有很大的糊味', '青葱虽然放的多', '但是也都炒蔫了', '估计是放入的时间过早', '也有可能是不够新鲜', '牛肉也比较普通', '尝起来不是很新鲜', '其他配菜就只有芽菜', '整体非常油腻', '不推荐', '但是老板很不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['但是因為我是第一次光顧', '例如中式牛柳等等什麼什麼的', '但是水準味道什麼什麼的都還好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['室内のインテリアか素晴らしく', '居心地のいい感してした', 'カツカレーを頼みました', '肉は柔らかく', '次回はお持ち帰りて頼みたいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本のチェーン店のお店牛角', 'お肉かコムのような感して', 'あんまり美味しくなかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスのスントゥフ屋ては', '間違いなく美味しいスントゥフ屋', 'お米もすこく美味しい', 'お茶漬けにもてきます', 'laカルヒの味付けも最高てす', '時間なのも良い']\n",
            "WARNING:root:cannot compute average owing to no vector for ['オススメされて行ってみました', 'シャリか小さいのて', 'いくらとハマチ', 'まくろも美味しかったてす', 'また行きたいと思います']\n",
            "WARNING:root:cannot compute average owing to no vector for ['這裡還能吃到韭菜盒子和小籠包', '炒飯', '炒麵', '合菜', '傳統麵點', '現做的需要等候但都很不錯吃', '牛肉炒麵和打滷麵都很好吃', '麵口感有嚼勁', '而是清淡的適合亞洲人口味喔', '喜歡亞洲食物的朋友一定要來']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真的鬼火冒', '每次点postmate都会出错', '点了霸王肘子', '送给我一个粉蒸肉', '你们弄不好就不要弄外卖了嘛', '真的气得想吐血', '燃火啊兄弟', '盘盘都这样', '啊啊啊啊啊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很道地四川菜', '我們帶了墨西哥客人一起吃晚餐', '我們四人點了乾煸四季豆', '牛肉', '牙籤羊肉', '水煮魚和辣子雞丁', '超推薦牙籤羊肉', '水煮魚非常香辣好吃', '真的是很值得一吃再吃', '讓我們吃白米飯時非常驚艷', '每道菜份量十足', '希望再訪']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店鋪有三層樓', '可能生意很好', '但後來點了可麗餅覺得還好而已', '我點了鹹食的可麗餅', '料餡很多', '但口味就一般', '沒到讓我覺得驚豔的感覺', '但店面在老蒙特利爾舊城區', '所以感覺蠻像在歐洲吃飯的', '不太加拿大', '來這邊可以點杯飲料聊天', '還不錯這樣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太咸了', '难吃死了', '而且', '菜单上明明写的是', '却算我', '块钱', '还没加上税的价', '服务态度一般', '还要小费', '食物并没有评价的那么好吃', '还不如去吃in', '以后也不去吃了', '又贵又难吃', '收银员一脸欠多还少的面孔', '看了都不想买他们家的东西了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ランチて来たけと', 'カレーも美味しかったし']\n",
            "WARNING:root:cannot compute average owing to no vector for ['個入りを', '人てシェア', '余裕て食へきれるサイス', '美味しかったーー', 'ヘーコンか乗ってるものもあって', 'カナタらしさを感しました', 'トーナツは基本同し味て']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很正宗的墨西哥餐', '调的酒也很有特色', '好漂亮', '味道很好', '适合女士喝', '要求少放冰', '他给的冰也真的很适量', '菜分量很足', '午餐菜单也非常划算', '生意很红火', '排了半个小时队才有位子坐', '环境不错', '服务也很好', '整体感觉很不错的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['お米をたへたくて', '量か多過きるのかたまに傷', 'てきれは', '人以上ての来店をお勧めします', '値段も許容範囲てしたよ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['菜色種類很多', '羊肉料理做得比較好吃', '雞肉料理普通', '沒有太多味道', '沙拉種類很多', '整體菜色感覺非常豐富', '可惜味道有一點點偏鹹', '如果喜歡重口味的人比較適合', '價格挺便宜的', '這是一個不錯的選擇', '整體感覺很乾淨', '服務也可以']\n",
            "WARNING:root:cannot compute average owing to no vector for ['当时一样都点了两串', '满满的一大盒', '年糕香肠和鱼糕饼真心好评', '剩下的什么鸡肉串干得要死咬不动', '还有什么鱿鱼的那个巨难吃', '反正我是hold不住', '对了刷的那个酱简直完美', '忘了什么名字', '一共两种酱', '没试过辣酱', '总的来说还是不错的小吃店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道非常平淡', '装潢不错', '也很干净', '但是食物一点滋味都没有', '完全不想再来第二次', '第一次觉得炸虾一点都不好吃', '茄子感觉不像是烤', '像是蒸的', '秋刀鱼肉已经烤到很乾', '炸鸡没有味道', '炒青菜没有味道', '整个的感觉就是干净', '但是没有味道']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しかったてーす', 'クリーンカレーを頼みました', '値段も安くて良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美しい日本料理と', '面白いお酒', 'とちらも美味しいてす', 'ハッヒーイヤーはオートフル', '寿司', 'お店は広々ゆったり', 'オーナは日本人なのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['菜色種類很多', '羊肉料理做得比較好吃', '雞肉料理普通', '沒有太多味道', '沙拉種類很多', '整體菜色感覺非常豐富', '可惜味道有一點點偏鹹', '如果喜歡重口味的人比較適合', '價格挺便宜的', '這是一個不錯的選擇', '整體感覺很乾淨', '服務也可以']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在国内都曾经兴起过', '不过貌似风潮已过', '而系多伦多竟然都开左', '店铺装修走韩风', '例牌台面都有木质长方蜡烛加热座', '食物方面选择不算多', '价钱都唔系平', '试了原味分米鸡', '套餐成个长盘上', '两盘鸡之间有白色酱汁配搭', '套餐还包炸薯条', '沙拉和', '碗白饭', '分米鸡口味偏甜', '唔算特别', '白饭非常细碗', '女生吃就刚刚好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['同事的屋企人系呢间面包店上班', '所以时不时同事拿面包比大家分享', '有日去完附近的medical', 'centre就离左呢度吃个蛋治', '好耐无吃过', '中规中矩啦', '蛋都算多', '面包店环境算有点旧', '但帮衬的人唔少', '切饼蛋糕款式都几多', '比较平']\n",
            "WARNING:root:cannot compute average owing to no vector for ['淨係店名以為就系一間麵包店', '其實唔算係曬咯', '入口處就看到有戶外客座使用', '唔使老外系度嘆咖啡同包點', '進店后除左睇到蛋糕柜', '仲有個小餐廳', '蛋糕櫃有靚靚蛋糕', '包點', '意式雪糕', '仲有芝士同自家制的歐式麵包', '堂食外賣都得', '價格比超市稍微貴dd']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听讲开左间银记肠粉', '原来系呢个唐人plaza内', '唔知同广州吃的差几远呢', '进店后看到一部大电视', '墙上挂住好多金属奖状', '仲有珠江的旧照tim', '好有亲切感', '主打肠粉必须要试', '招牌牛肉肠粉尚算滑溜', '除了烟韧度唔够和豉油味道稍逊外', '都算合格啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好好味啊', '腸粉有腸粉味', '蛋有蛋味', '哈哈', '好q正宗', '在加拿大吃到咁正宗的廣州腸粉', '比個讚', '而且開得夜', '可以食宵夜', '呢間西嘢d珍珠雞同奶茶唔錯', '都可以試下', '最緊要有廣州feel', 'd企檯應該全部係廣州人', '好似翻左廣州咁']\n",
            "WARNING:root:cannot compute average owing to no vector for ['早晨从温哥华流窜过来', '到这个广场', '大统华超市', '点才开门', '这家', '点多已经不少人了', '形态还是中餐形态', '味道不行', '内部稍有些腥味', '青菜本地挺贵只给了很小的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['唐园的干炒牛河也用的是细河粉', '配菜和传统不同', '有芹菜', '白菜', '胡萝卜丝', '青葱', '嘱咐了少酱少盐', '因此颜色比较淡', '口感偏湿', '河粉不是很好', '偏硬', '没有弹性', '牛肉片腌制的还不错', '比较软糯', '总体来说', '分量足', '河粉偏湿', '不过河粉本身口感不佳', '比较一般']\n",
            "WARNING:root:cannot compute average owing to no vector for ['非常糟的一次經歷', '點了餐', '說是一個小時', '但是來了以後還要在等一個多小時', '因為根本就沒做', '此外', '或許因為是外賣的原因吧', '服務態度簡直是差的不能理解', '還在訓斥顧客的', '所以', '我個人來說', '是不會想去了', '建議不要點外賣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真是跟国内的焖锅没法比啊', '一点都不辣', '不过最后方便面下进去倒是很入味', '挺好吃的', '其余的真没什么印象了', '去过两三次之后不会再去了', '而且价格还不便宜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好耐无离呢度吃野啦', '早左少少出门过来吃个早餐先', '仲系保持翻旧有的环境', '无变过', '坐低点餐', '价钱就比以前升价dd', '雪菜牛肉米粉味道稍微淡左d', '流心双蛋不是新鲜煎', '所以唔系好够热', '多士少少干', '埋单盛惠']\n",
            "WARNING:root:cannot compute average owing to no vector for ['开业前路过', '店内支持微信和支付宝付款', '非常方便留学生和国内游客', '饮品最快有得饮', '玫瑰柠檬梳打的柠檬味不算突出', '玫瑰糖味比较重', '盐酥鸡等了大概半小时先整好', '炸得一点都吾哝', '几脆口', '粉吾算重', '个人觉得吾够香', '味道如果稍微重翻dd更好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['既没有惊奇之处也没有明显缺点', '对于我来说去哪都差不多', '当然我也不是广东人', '不能算是优秀的评价吧', '但可能应该也有不少人和我一样']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位於eglinton', 'ave同don', 'mills附近', '應該是新開無耐的華人超市', '超市入口的喜慶恭賀盆栽還未設走', '望落去估計是豐泰超市的兄弟店', '樓底高', '地方大', '產品擺设整齊', '整潔', '人唔多', '逛得幾舒服', '停車場夠大', '住附近的華人夠曬方便', '希望以後都能保持甘乾淨舒服']\n",
            "WARNING:root:cannot compute average owing to no vector for ['入場するのに', '近くかかります', '頃て女の子は', 'てはなく', '友達と行きましょう', 'ヒールは', 'くらいたったかと']\n",
            "WARNING:root:cannot compute average owing to no vector for ['トリュフホテトもおススメ', 'とっても感しのいいお店て', '美味しかったー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真心不怎么样', '中午没有雪蟹腿', '菜都很一般', '寿司品质还行', '两小时的限制也让人不愉快', '前台不友好', '这里真心没有爱', '和很糟糕', '没礼貌的服务']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食物还可以', '但吃饭要排很久的队', '而且歧视中国人', '吃饭还要出示护照', '不是很好找', '在凯撒一楼赌场的里面', '牛排还可以', '但是螃蟹腿', '海鲜等食物一般', '没有想象中的好', '没有醋等调料', '吃起来可能会腻', '饮料需要找服务员单独要']\n",
            "WARNING:root:cannot compute average owing to no vector for ['排队等了很久终于吃上', '人一直很多', '点了前菜炸豆腐和春卷', '炸豆腐轻易不要尝试', '春卷里有薄荷', '不推荐', '五个人去点了一个padthai', '巨酸爽不推荐', '咖喱可以', '我朋友没吃够单点了一份饭', '我觉得可能是因为泰国香米吧', '服务态度及其一般', '但是一直排队', '不是特别理解']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次去都会排一小阵子的队', '不过熏肉真的值得等待', '每次朋友来都吃的非常开心', '推荐来了旅游的人都可以试试']\n",
            "WARNING:root:cannot compute average owing to no vector for ['友人に連れられてきました', '韓国料理か好きて', '韓国に遊ひによく行きますか', 'ここの韓国料理は', '美味しかったてす', 'カクテキも美味しかった', 'トッホキ美味しかった']\n",
            "WARNING:root:cannot compute average owing to no vector for ['整體感覺就像在韓國一樣', '非常喜歡韓國料理', '超級好吃', '牛肉超嫩', '人份的烤肉套餐', '個女生吃都沒問題', '物超所值', '非常貼心', '服務態度', '環境衛生', '食材新鮮']\n",
            "WARNING:root:cannot compute average owing to no vector for ['饺子和担担面都蛮不错的', '配的汤是真的棒', '南瓜饼也脆脆的好吃', 'delivery的时间正好', '分钟吧', '虽然我在摸鱼没听到电话', '送外卖的小哥还是态度很好', '而且居然是开车送来的', '十分的良心', '总之感觉很不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ku', 'さん', '料理の腕のあるこ主人さんと', '奥様の笑顔て', '絶品てした', 'ラスヘカスて', 'ホットしたい', '迷わす', 'kuへ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食材选取新鲜而又细致', '所有的菜品都让人赞不绝口', '服务周到', '每一道菜前后都会收拾桌面', '准备好下一道菜的餐具', '菜品大部分是按人头数早已备好', '最新鲜的放到我们的盘中', '芥末酱油这些都已经帮忙抹好了', '只需要张张嘴吃进去品尝就好', '真的太喜欢了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['と予約を入れて行きました', '遅刻をしたら', 'ても道を間違え大遅刻', '申し訳なかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['アメリカのマクトナルトては', '接客はここか一番良かった', '店内てたのむなら', 'このお店をお勧めします', 'かなりお勧めてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['サラタかすこく美味しかったてす', 'お味噌は好みしゃなかったてす', '店内か暑くて辛かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好吃', '珍珠也很q', '重要', '餐點價格合理', '像是salmon其實是生的', '餐廳位置蠻好找', '附近公共交通工具也多', '服務人員態度很親切', '但就是好像沒有想像中的那麼台式']\n",
            "WARNING:root:cannot compute average owing to no vector for ['时不时经过呢间越南餐厅', '有机会朋友请吃饭所以试了', '店内地方都不算少', '午饭时间都几多人吃野', '坐满', '菜单种类好似几多甘', '想吃檬就点左香茅牛扒檬配蔗虾', '蔗虾的虾胶味道ok', '牛扒稍微过熟了少少', '檬口感可以', '份量好足', '吃不完', '价钱合理', '上菜速度ok']\n",
            "WARNING:root:cannot compute average owing to no vector for ['今天跟女朋友一起去小橙都吃饭', '感觉味道真的很不错', '很正宗', '喜欢吃川菜的朋友可以尝一尝', '服务员的服务也很不错', '环境也是一个很中国风']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太垃圾的服务', '我来吃点心', '你让我点菜你', '你有毛病吗', '我吃东西还要你来教', '店里没有一座是点炒菜的', '菜能新鲜', 'sb老板娘', '不会再来了', '这家店非常脏', '点心也贵', '还越来越难吃', '我是客人', '你太度能好一点吗', '肚子饿', '没服务', '的服务费', '实在是垃圾店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太垃圾的服务', '我来吃点心', '你让我点菜你', '你有毛病吗', '我吃东西还要你来教', '店里没有一座是点炒菜的', '菜能新鲜', 'sb老板娘', '不会再来了', '这家店非常脏', '点心也贵', '还越来越难吃', '我是客人', '你太度能好一点吗', '肚子饿', '没服务', '的服务费', '实在是垃圾店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不過地理位置不錯', '人氣都唔少', '一到下班時間熟食區特別熱鬧', '肉檔同魚檔比較細', '應該話成個超市面積都唔算大', '但就好方便附近居住的華人購物', '超市的用餐區系超市出面', '座位唔算多']\n",
            "WARNING:root:cannot compute average owing to no vector for ['耐无吃就觉得好好吃', '不同安士不同价钱', '选了白包', '安士汉堡牛扒', '可选配料好多', '加了适合夏天口味的芒果酱', '番茄', '酸青瓜', '生菜等等', '新鲜烤好汉堡扒放入一起吃', '一淡吃落去feel到满满牛肉汁', '加了配菜吾油腻又味道丰富', '一个combo十几元']\n",
            "WARNING:root:cannot compute average owing to no vector for ['吃过midland分店', '呢度系唐人街附近系dundas', 'st的夹层店铺', '窄长的小店基本上都坐满晒人', '无midland店甘大甘舒服', '个店员在忙的感觉', '落单后稍微等了数分钟先上菜', '招牌的重庆碗杂面分量足', '因为唔系好饿', '两个女生分享刚好', '配料够多', '不过汤咸左d']\n",
            "WARNING:root:cannot compute average owing to no vector for ['因为朋友的妹妹曾经系呢度做过野', '所以朋友约我离呢度饮茶', '听她讲曾经装修过', '所以环境相比旧时好左d', '周末好多人等位', '座位有少少逼', '不过台好够大', '不怕放多几笼点心就无位', '卤水猪脚仔配海蜇味道都算可以', '凤爪份量有d少', '叉烧饭d叉烧确实唔错', '好香', '松软又有口感', '价钱算中等吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我点了土豆丝', '上来的菜那个颜色', '浅咖啡色的', '带着鱼腥味', '应该是用炸鱼的油炒的菜吧', '都舍不得用新油吗', '干巴巴的米饭用个小破铝罐子装', '都瘪了好几个坑了', '出来在出租车上司机中国人', '说本地中国人都不来这家', '难吃的很', '说你们肯定是网上看来的推荐这家']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于超多唐人居住集中地内', '隔离有超市', '烧腊店', '其他食肆等等', '所以入到去都会听到熟悉的语言', '不过服务员是外国人', '效率有点低', '下单我都要重复讲', '呢间麦当劳好多老人聚在一起聊天', '并有儿童游乐区', '和drive', 'thur服务']\n",
            "WARNING:root:cannot compute average owing to no vector for ['超大盘的干炒牛河', '牛肉超大片', '数量也多', '配菜只有青葱和洋葱', '却味道十足', '个人觉得不像是港式或者广式牛河', 'wai', 'wai家的牛河更像是美式', '用了很多洋葱', '味道比较独特', '最值得一提的是河粉', '大片软糯', '不粘连口感q弹', '干湿也正好', '目前为止河粉最棒']\n",
            "WARNING:root:cannot compute average owing to no vector for ['去这家coco买饮料', '以前都是看着他们做的', '都有放红茶', '这次我看到他们没放', '我就问你们没放红茶是吗', '他们说放了', '只是放得少', '我还以为我看错了', '结果一喝就是没味道', '员工太不走心了好吗', '气得一匹', '再也不来这家买奶茶了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['午餐时间来的', '人非常多', '听说pho比较好吃', '但我没有吃出来有什么特别的', '哈哈', '反正周围也没有什么pho', '生意应该不差']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听親戚講前身好似叫耀永', '宜家叫富臨門', '係victoria', '門口系商場停車場果邊', '非馬路面', '餐館地方吾大', '魚缸養住不少游水海鮮', '象拔蚌', '皇帝蟹', '龍蝦等等', '皇帝蟹兩吃', '新鮮特別鮮甜', '肉飽滿', '雖然有d貴', '但味道真系幾吾錯']\n",
            "WARNING:root:cannot compute average owing to no vector for ['貌似新开无耐', '午饭时间人五多', '餐厅系转角位', '餐厅地方唔算大', '简约带点中国风的设计', '餐牌上选择唔算多', '招牌的红烧肉味道有姜醋蛋的感觉', '微微醋酸香', '肥肉唔算腻', '唔讲得话好好吃', '反而卤水鸭肾唔错', '够霖又入味', '生煎包普通啦', '唔够香唔够脆']\n",
            "WARNING:root:cannot compute average owing to no vector for ['点了直接进垃圾桶', '饭是超级软烂的', '分不清是小孩吃的饭糊还是饭', '配菜你要是不想给就索性不要给', '点的咖喱鸡', '鸡肉一共小小的', '剩下全是土豆胡萝卜', '想问一下咖喱是不是根本没放', '饮料全色素', '糖浆兑出来的', '好无语']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ストリッフエリアからは車て', '分くらいのロケーションて', 'なお隣接する立体駐車場は', 'トルてす', 'クーホンなともあるのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['人気店の為', '頃に']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很好吃的三明治店的', '原本太太想來這邊吃', '但是來這邊吃一次就感動感動', '蠻不錯的味道', '這邊', '個小時開的', '所以玩賭博到很晚也可以吃到', '我們的話', '吃早餐', '吃完就還有精神', '如果今天玩賭博的話', '可能會贏吧', '哈哈哈']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['お酒のおつまみと', 'こうしさんとお話てきて', 'また', '寄りますねー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['寿司隆', 'を発見', 'つになりましたー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['開店して間もないらしい', 'お酒もいろいろな種類かあり', 'とにかくすへてかリースナフル']\n",
            "WARNING:root:cannot compute average owing to no vector for ['油泼臊子面还有涮三样', '刚去吃第一次的时候惊为天人', '但多吃几次就觉得也就一般般了', '面的油有点多', '吃到最后感觉就是泡在油里', '不过味道还是蛮好的', '以后有机会去陕西吃最正宗的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['出てくるまてに', '分かかりましたか', 'とてもおいしかったてす', 'また食へたい', '店員さんに日本人の方かいました', '現金のみてすのて注意か必要てす', 'また', '米トルも使えますか', 'レートは損しますのて', 'こ注意を']\n",
            "WARNING:root:cannot compute average owing to no vector for ['この手の店にはありかちたか', '量かかなり多いのて注意', '日本人なら', 'つ頼んて', '近くに来たら寄ってもいいかも', '程度']\n",
            "WARNING:root:cannot compute average owing to no vector for ['値段も良心的て味も美味しい', 'おススメ料理の中から', 'その', 'つは', 'ちょっと微妙たったけと', '混んてた割にはすく入れて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['有名なステーキチェーンてす', 'ステーキのサイスかほほ', 'oz以上て大きいてす', '無理を言って', '人て', 'ワイン一本とステーキ', 'つとサイトの野菜てチッフ込みて', 'てした', '気持ち高いてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['まあまあ美味しかった', '安いのに', 'サラタは味付けか美味しかった']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于bloor', 'st的holt', '地下一边男装部', '一边彩妆', '和一线名牌包包区', '二楼是女装', '餐厅在夹层', '想看想买最新想知的潮流动向', '耐唔耐过来逛下hr', '总会看中不少']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我一直百思不得其解', '每个菜都是极其难吃', '味道完全不对', '客观的说', '厨师水平应该是相当的低', '点一个京酱肉丝', '做出来是咸的', '大概味道就是酱油炒肉丝', '您最起码也放点甜面酱啊', '服务环境什么的都不说', '就是味道太差', '我很负责的说', '他家倒闭只是时间问题']\n",
            "WARNING:root:cannot compute average owing to no vector for ['之前听朋友讲华丽宫出品可以', '行完街就打算离呢度吃晚饭试下', '原址是龙腾金阁', '改成华丽宫后人气高企', '早茶很长人龙帮衬', '晚饭生意都貌似唔错', '餐厅推荐的羊腩煲有点失望', '味道偏淡', '羊肉整得唔够入味', '核桃牛柳粒都算可以', '温哥华蟹蒸饭不过不失']\n",
            "WARNING:root:cannot compute average owing to no vector for ['相比起以前', '多左云吞面專門店', '所以感觉上无其他店甘出色', '传统云吞面', '细蓉', '配韭黄', '距呢度仍然系葱花', '汤底就不过不失', '面就唔够竹升面甘脆口', '乒乓球甘大的', '云吞', '系亮点', '鱼球', '有鱼味', '算系号', '价格就肯定跟物价一起上涨']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很棒的咖啡厅', '咖啡', '甜点', '炸鸡waffle', '都很好吃', '擦脸的分量很大', '今天点的炸雞waffle', '有沙拉', '三隻雞腿', '好烤得非常棒的鬆餅', '每一個的味道都處理的很好', '量也夠大', '價錢感覺很合理', '超赞的早午餐', '服务态度也很好', '气氛很好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['亲戚朋友要远游前', '约埋一起吃饭', '就亲戚屋企附近来了华景轩', '门口在角落位', '停车场足够大', '餐厅唔大', '一眼睇晒', '埋边有包厢', '菜式方面特价琵琶鸭唔错', '蜡下眼有少少似烧鸭', '只鸭一d都唔肥', '唔会太大只', '皮脆肉唔痴牙', '特价离讲性价比几高', '其他小菜出品都可以', '工作日晚人唔多', '倾计吃饭都唔嘈']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店面更改是不是厨师也换了', '味道极差', '巨咸难以接受', '不多bb', '能别来就别来', '入雷坑了', '难吃的一批', '我记得点了个大蒜牛肉', '不知道是不是把盐打翻了', '牛肉上大把盐颗粒', '挤完也这吊样', '服了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于太古广场', '楼民族村内的papa台式美食店', '在民族村中间的小巷内', '长形的店铺', '餐牌系收银的台面上', '比较多是主食', '小吃也有', '下单后大概等', '分钟左右就可以食了', '大肠面线', '几正宗', '少少酸的面线', '好滑', '大肠软霖入味', '下次要试试距d便当先']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好耐无吃港式早餐了', '系附近办完事后时间尚早', '吃翻个早餐叹下先', '无意记得呢间茶餐厅', '门口对住油站', '餐厅系茶餐厅出品为主', '地方唔大', '数张小台', '人多两下就坐满', '点左个沙爹牛肉米粉', '份量几多', '牛肉和沙爹牛肉汁够味', '米粉唔太硬', '耐无吃就觉得几ok个味道']\n",
            "WARNING:root:cannot compute average owing to no vector for ['中午和朋友来这家店吃饭', '心情不错', '会再来光顾', '点了一个舒芙蕾蛋糕', '就是等的有点久', '不过味道还是值得的', '棒棒哒', '饮料的名字真的绝了', '初恋的味道', '相当好看太诱人了', '还没来得及拍就被我朋友喝了', '总体而言真的不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次到這裏來都很早', '很寧靜舒服', '舖面也很清潔', '有停車場但沒有單車停泊的位置', '内有小孩遊樂園區', '設有用餐的坐椅', '家長們可边看着小孩玩耍嘆咖啡', '享受片刻的寧靜', '但沒有看到小孩用餐的椅子', '總括這裏花費不多', '小孩子有吃有玩', '家長們可以鬆一鬆是個不錯的選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['同朋友去湖边影日落', '在两条路的交界角落位', '离巴士站唔远', '全落地玻璃设计', '可以边叹边望住店外的风景', '地理位置唔错', '所以不少人中意系度做下功课', '或带电脑离做自己野']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['今天的菜色失望', '廚師沒用心', '全是蛋', '蛋蛋大歺', '果汁飲料都失去以前的用心', '味噌太鹹了', '以前還蠻養生', '有酥皮濃湯', '現在食物一般般', '沒有特色', '以前有水果冰茶', '有特色', '現在是一般的紅茶', '以前我喜歡來這裏吃', '現在食物退步了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧', '老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧', '老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['すこい人気て', '予約していなかったのて', '分くらい待ちました', 'サラタとステーキ', 'を頼みました', 'とちらもとても美味しかったてす', 'ついてくるハケットも', '接客もよく']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前回は', '時くらいに来店したら', '今回は', 'わーいわーい', '笑っ', 'たた', '午前中は待ち時間か少ないてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['確かに', 'フイヤヘース', 'ロフスターハン', 'オイスターともに美味しかった', 'たた', '客席数か少なすきる', 'ウエイトレスの効率か悪いからか', '時間近く待たされるのは辛い', '貴重な海外て', '時間待つ価値かあるかは', '悩ましい', '行列嫌いならおすすめてきない']\n",
            "WARNING:root:cannot compute average owing to no vector for ['カフェラテをオーター', 'あまりの美味しさに感激しました', 'こちらにきてまた', 'ヶ月ほとてすか', '美味しいラテに出会えなく', '探し求めてました', '本当にオススメてす', 'ラテたけのつもりか', 'おやつもオーターしちゃいました']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我不会再来这个地方', '一共三个服务生', '门口有', '个人等着进去', '我等了二十分钟', '没有一个服务生过来服务', '沟通了两次也没有人过来招待', '即便是很忙', '也可以让顾客坐进去等吧', '难道要所有人吃完我们才能进去', '草泥马的傻逼地方']\n",
            "WARNING:root:cannot compute average owing to no vector for ['アメリカて', '番好きな韓国料理屋さんてす', '接客もきちんとしていて', 'フレントリーて優しいてす', 'メニューも充実しているのて', 'お客さんは']\n",
            "WARNING:root:cannot compute average owing to no vector for ['首先不说菜如何', '小笼包还好', '牛筋太难吃了', '更重要的是等了将半个多小时', '抄手一直没有上来', '告诉我们因为忘了', '所以抄手没送上来', '而且店员的服务态度很差劲', '本来是来旅游玩的', '心情被搞的贼差', '再也不会去这种烂店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['スーフは美味しかったし', '正に日本の味たって感し', '日本風かいいな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['集饮食', '购物', '医疗和娱乐活动', '停车场有室内室外', '每逢公众假期都好多人黎', '环境普普通通啦', '不过就够方便', '可惜近呢几年相对无甘繁华了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['フレントリーて心優しい店員さん']\n",
            "WARNING:root:cannot compute average owing to no vector for ['嘱咐了少酱少盐', '河粉是大的面片', '比较有嚼劲', '配菜是豆芽', '芝麻', '特别是这个青葱', '非常新鲜', '为牛河提味提色', '芝麻也是比较独特之处', '之前吃别的家都没有', '酱的味道稍微偏甜', '应该是港式干炒牛河', '偏甜口', '牛肉是碎肉', '丰富了口感']\n",
            "WARNING:root:cannot compute average owing to no vector for ['大大大好きな場所てす', '嫌な顔一つされませんてした']\n",
            "WARNING:root:cannot compute average owing to no vector for ['予約をしていくのかおすすめかも', 'そこてお試して行くのも良いかも', '店内も素敵たったし', '店員さんの対応も素晴らしい', '又是非行きたいな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食事かとってもおいしかったてす', 'ホスヒタリティー満点てした', 'スティーフナイスカイ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这几天拉斯维加斯', '大峡谷走一圈', '鼻子干到不行', '急切想喝汤', '在yelp上搜了一下', '这家店竟然有椰青土鸡小火锅', '太适合我这种中国胃了', '从strip开车过来', '大概', '分钟', '很方便停车', '奔着汤过来的', '顺带着点了烧烤', '也挺好吃的', '烤海鲜类和肉类的方式不同', '很用心', '强推']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しかったのはもちろん', '接客か最高てした', 'スタッフの方々の雰囲気か良くて', '気さくて', 'とても居心地か良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次去都是超级满意', '食品的质量', '上菜的速度', '服务生的态度都是最爱的原因', '最近还看到了', '他们有开始有新的早餐的菜单', '不停的有新的惊喜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于kennedy夹hwy', '的西南角', '以前系新旺角', '宜家系帝豪名宴', '痴住华人商场', '大厅好大', '会有少少嘈', '环境唔算华丽', '出品味道一般般', '价钱中等', '还有延续点心车的出现', '呢样我中意', '服务还算ok', '加水或收碟都几快', '服务员唔会扮听唔到看唔到', '近酒楼的车位周末肯定爆d', '需要停远少少']\n",
            "WARNING:root:cannot compute average owing to no vector for ['没有任何问题或者失望', '但是今天我来吃饭', '加起来税前', '税后', 'yelp上说可以', '我问服务员能打折吗', '她说', '不行', '我真就偶然看到这个折扣', '对服务还挺失望的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['川北涼粉口感同味道都幾好', '對我嚟講都做到恰好', '只係略嫌唔夠麻味', '四川菜此終味道比較濃和比較油膩', '如果想食清淡嘢就唔好嚟喇', '最後還有']\n",
            "WARNING:root:cannot compute average owing to no vector for ['偶然發現新的麵館', '味道正宗做法講究', '月陽樓', '當時只給上層社會人士提供服務', '年時回族人馬保子以', '一清', '二白', '蘿蔔', '三綠', '香菜蒜苗', '四紅', '辣子', '五黃', '麵條黃亮', '統一了蘭州牛肉麵的標準', '並使用煮羊肝', '牛肝的湯來煮麵以提升其風味']\n",
            "WARNING:root:cannot compute average owing to no vector for ['朝食に', '人て利用', '広い', 'フートコート的な作りて', '最後退店時にまとめてお支払い', 'また朝早いとやってない店舗', '料理', 'とかもある', 'いろいろ選へて良い']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于yorkdale', 'mall商场内', '靠近rh的对面', 'iq', '呢个店名好特别', '主打健康的素食沙律为主的餐厅', '当然唔少得老外最爱的咖啡饮品', 'iq的沙律卖相几诱人甘', '健康为主从来都受到不少人喜爱', '下次下午茶时段过来试试', '看下口味如何']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位於密市square', 'one負一層的美食廣場開左啦', '地方無想像中甘大', '装修的围板系涂鸦墙', '又是拍照打卡点', '扶手电梯旁有现开业的餐厅店名', '環境系ok架', '幾有氣氛', '每家小店都幾好特色', '附設堂食的座位唔算多', '週末過假期人多就可能會有d逼']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我确定还会回来这里', '所以我推荐大家来这里尝试', '您一定会得到一个满意的中国餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['昨天和女兒首次到這裏來', '看見有中文字的菜單', '感覺好親切', '叫了餃子山西刀削麵和蘭州拉麵', '味道很不錯', '牛肉鬆軟濃郁', '麵是師傅自己做的', '坐在椅子上都可看到', '熱茶一元用茶包泡的好大杯', '茉莉花茶好清香', '收費合理']\n",
            "WARNING:root:cannot compute average owing to no vector for ['价格涨的飞快', '每样菜都上涨了', '生意好就是这样拽', '东西也没见多给', '这老板还真是会赚钱', '本就是面饭之类的材料', '上次吃了个番茄面更夸张', '连鸡蛋都几乎没见过', '一点鸡蛋星儿而已', '真是无语了']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['少し遅めのランチに行けは', '新しく美味しい食事', 'アメリカにしては']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とれも美味しく', '店員さんの接客も親切てした', '次回ラスヘカスに行ったときは', '試してみようと思ってます']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['干锅茶树菇里全是洋葱和豆芽', '只有表面浮着几根茶树菇', '还超级老', '一看一度认为是梅干菜梗', '角度刁钻', '泪目', '未来半年都不会爱茶树菇了', '慎点', '慎点', '慎点啊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于victoria', '感觉出品同服务ok', '呢次黎系参加婚宴', '头盘的乳猪件几香脆', '唔会太肥腻', '带少少乳猪肉', '口感唔错', '带子炒时蔬的带子大粒弹牙', '姜葱双龙的龙虾就唔够香', '蛋白瑶柱菜粒炒饭味道稍微淡左d', '总体离讲都算唔错啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しいかったー', 'おすすめてす', '英語か話せなくても', '笑顔て対応してくれたのて', '気落ちせすに夕食を楽しめました', '日本ては馴染みかない', 'チッフもチェックの時の伝票に', '自分ていくらにするか', '決めやすいのて安心てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['最悪のサーヒス', '知らないと二重払いになる', '知らすに払うように仕向けている', 'チッフのことを言うと']\n",
            "WARNING:root:cannot compute average owing to no vector for ['おうとんかおそはか食へたい', 'と思って探し見つけたお店てした', 'お値段もリースナフルて大満足', '今度は夜お邪魔したいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['寒い冬にとうてすか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヒルの最上階', 'にあるレストラン', '外は一面カラス張りて', '超高層から見える景色は圧巻', 'ホリュームは多いのて', 'すくに満腹になっちゃいます']\n",
            "WARNING:root:cannot compute average owing to no vector for ['他們家的乾料特別棒', '尤其是老闆賊帥', '人也幽默有趣', '我下次要帶更多的朋友來', '比那個tempemarket', 'place的好太多了', '服務員我很喜歡']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店員さんも優しく', '店内も綺麗に清掃されており', '美味しかったのて良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['シシケハフハウス', 'とあるか', '店内はとても広く', '活気にあふれている', 'うれしい']\n",
            "WARNING:root:cannot compute average owing to no vector for ['女服務員漂亮親切', '舒適美好的用餐環境', '下次還一定會常來這裏', '食物料理每道都很好吃', '完全可以放心食品的衛生', '吃得放心就更開心了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['都有好幾年无幫襯呢間分店', '晚上過了飯市高峰期唔使等位', '中意距靠邊既座位基本上都系卡位', '岩晒撐台腳', '菜單上選擇比以前增加唔少', '流行的魚湯米線都有', '鮑貝鷄絲粥', '粥好杰身', 'ok綿', '沙爹牛柳炒腸粉', '如果再香d更好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本食とは思えない店構え', '多分もう行かない']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听说换了大厨', '菜的照片看着也还不错', '所以大过年的决定去试一试', '点了一个烤鱼和毛氏红烧肉', '鱼上来时没有热汽', '干吧吧的', '一尝这个鱼非常的不新鲜', '还是凉的', '红烧肉也是凉的', '干的', '总之', '不会再去了', '也不会推荐朋友去']\n",
            "WARNING:root:cannot compute average owing to no vector for ['仅为吐槽', '今天等了快', '分钟告诉我龙虾没了刚从超市买', '还他妈超市原价卖给我', '你们到底是怎么在香槟活到今天的', '就这种服务态度', '成本不够就别他妈瞎打折', '打完折隔这么久才通知顾客', '还超市原价卖给我', '美其名曰', '免费加工']\n",
            "WARNING:root:cannot compute average owing to no vector for ['凝り固まった首', 'ありかとうこさいましたm']\n",
            "WARNING:root:cannot compute average owing to no vector for ['凝り固まった首', 'ありかとうこさいましたm']\n",
            "WARNING:root:cannot compute average owing to no vector for ['麻婆豆腐和辣子鸡是我的最爱', '豆腐很嫩', '酱料甜辣得当', '重庆辣子鸡炸的很酥', '火候刚好', '也不腻', '这家菜量也大', '感觉这俩菜完胜匹兹堡其他餐厅的', '不知为何还不是很火', '店里的厨师应该是四川本地人', '做的各种川菜都很正宗', '不过有次点过三杯鸡', '感觉就比较一般']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前面评价都不高', '我吃了居然感觉不错', '中餐馆一般让人失望', '干净程度不如韩餐', '韩餐不如日餐', '这家真的还有模有样', '炒五花肉都不错', '炒饭中上', '只是韩国服务员英语听不懂', '写汉字也不认识', '菜单第一页没有编号的就头痛了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道很像中国国内的腌菜炒饭', '可是现在没了', '好可惜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店員很親切', '這家店有果汁特調', '可以選你喜歡的水果', '他們會打成美味果汁', '我點了', 'nemo這個餐', '餐點包含了', '烤麵包', '烤馬鈴薯以及水果', '煎蛋裡包了鮭魚', '非常美味', '還有烤馬鈴薯也很好吃', '是間很值得再光顧的餐廳']\n",
            "WARNING:root:cannot compute average owing to no vector for ['鲍鱼捞饭', '第一口就知道是罐头鲍鱼', '酱汁陀在一起都拌不开', '这也太难吃了', '黑金流沙包还行吧', '给小费的时候', '服务员嫌少', '追问道', '今天你有什么问题吗', '这话问的', '我没什么问题', '我就是觉得不好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['呢個plaza內唯一的韓國超市', '其他都系食肆多', '超市唔大', '有點舊', '蔬菜水果選擇都唔少', '但就无大型的肉檔同海鮮q部', '熟食部好細', '生活用品唔多', '但足夠日常補給', '價錢相對比起唐人超市有點貴', '也无galleria甘多選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['pkkoj', 'lkkoj', 'pl', 'kiool']\n",
            "WARNING:root:cannot compute average owing to no vector for ['加完服務費', '這是一個四人套餐', '服務非常好', '裝潢很華麗', '停車方便', '食材算新鮮', '只是全部都是清蒸的', '其實算是非常簡單的料理', '沒有更多的特色', '想嘗試一次可以去試看看', '沒有必要去第二次', '吃完感覺好像沒吃東西']\n",
            "WARNING:root:cannot compute average owing to no vector for ['長形店鋪', '一邊系落單區', '一邊系客座區', '也有落地玻璃墻的高凳位', '因為係市中心金融區', '好多寫字樓人士幫襯', '店員幾nice', '服務親切好笑容', '相比起其他食肆稍微晚少少關門']\n",
            "WARNING:root:cannot compute average owing to no vector for ['bbbbbbbb', 'bbbbbbb', 'bbbbbbbb', 'ggggggg', 'hgghhg', 'hcbugf', 'jhfgjk', 'ghjihbnjgh', 'gfjbi', 'gchki', 'gfnifj', 'hfnikfdyk', 'gdjk']\n",
            "WARNING:root:cannot compute average owing to no vector for ['选择了这家tea', 'station结果我后悔了', '结果好难喝啊', '而且还卖', '一杯', '喝了两口就喝不下去了', '简直就是坑人钱']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とても美味しかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在我们吃饭的过程当中', '甚至等了超过二十分钟好几次', '后来问了问才知道', '可能漏单了', '不过漏的也太多了', '后面直接不给上了', '上菜时间这么久', '后来点的菜根本就吃不动了', '我也就来这一次了以后不会再来了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスの空港近くにあり', '便利て美味しいラーメン屋', '嬉しいホイントは', '米国のラーメンにしては安く', 'て提供', 'たた', 'からあけは特に美味しくてお勧め', '多くの人か喜ふのに', 'と感しる']\n",
            "WARNING:root:cannot compute average owing to no vector for ['這次是帶我的姐姐連續', '天吃他們的下午茶', '甜點製作用心', 'pub一樣好吃', '如果能坐到靠近garden', '位置', '不但花朵香氣四溢', '甚至景觀設計上也讓人感覺很舒適']\n",
            "WARNING:root:cannot compute average owing to no vector for ['個人的にはお勧め', '餃子も美味しい', '場所も中心地のモールから歩いて', '分距離て便利', '改善してほしい点は', 'ヒールのクラスか無く', 'フラスチックな事', 'なのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['見ていたのに何も反応せす', '拭いていたら', 'といてくれと言われました', '不愉快てした']\n",
            "WARNING:root:cannot compute average owing to no vector for ['拉斯維加斯最好吃的豬排飯', '乾淨', '新鮮', '味增湯很像在日本吃的味道', '每次想吃豬排一定來這裡', '更吸引人的地方', '飲料也很多好喝的種類', '晚餐時光約三五好友一起吃飯', '喝點生啤酒或是清酒配上日本小食', '都是不錯的選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['去the', 'alley的时候经过呢间甜品店', '之前都唔留意到距', '人唔多比较清静', '店铺唔大', '数张桌椅供客人堂食使用', '收银隔离是雪糕柜', '台面有简易的菜单同彩图的甜品相', '不过晚上时间得翻雪糕卖', '见相内的格仔饼卖相唔错', '可惜无得食', '唯有下次再离试下啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本語は通しませんか味', '雰囲気', 'ヒールはdosqes', 'ambarかオススメ', 'チキンやフリトーも美味しいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不味過き', 'スーフ', '具材', '全て最悪', '粘土を食へているような感覚', '味噌ラーメンなのに', '具材のチキンは猫の餌のような味', '他のメニューは美味しいのかな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家餐厅的火锅超级美味', '肉很新鲜', '麻辣火锅', '药膳和番茄锅味道很好吃', '这个肉真是新鲜', '这家餐厅的火锅超级美味', '肉很新鲜', '麻辣火锅', '药膳和番茄锅味道很好吃', '这个肉真是新鲜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['做为义美冠军的老客户', '听说他家要延长营业时间', '我真的超', '日起', '每日早', '点到晚', '周一公休', '同时还会全天供应烧饼', '油条', '豆浆', '每晚', '点后增加新品清粥和小菜', '以后随时可以吃到他家的美食啦', '完美完美']\n",
            "WARNING:root:cannot compute average owing to no vector for ['紅燒牛肉麵很好吃', '蘿蔔糕也不錯', '鍋貼及小籠包的肉餡稍微甜了一點', '燒餅不錯', '可是油條炸的太油了', '還有其他的台灣小吃', '蚵仔煎', '滷肉飯', '油豆腐粉絲', '大腸麵線', '如果你有機會來賭城', '不妨來這裡試一試', '這裡的服務態度很好', '老闆非常客氣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在這也想說可以嘗試他們的豆漿', '給我一碗肉燥飯', '沒什麼味道', '感覺水水的', '肉圓裡面也不新鮮', '皮又有點硬', '餐廳裡面也一堆國民黨的東東', '真的吃到待不下去']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とても美味いシェラート', 'カフェてす', 'トロント辺りては']\n",
            "WARNING:root:cannot compute average owing to no vector for ['奥は床か上けてあって', '奥の席ても通りの景観を楽しめる', '料理の味も素晴らしかった', 'また行きたいと思っています']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['fℓℓ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于丰泰超市隔离', '邻近都有几间食店', '算开左几耐下的茶餐厅', '呢间同香港某烧鹅店无关啦', '店铺简简单单的布置', '唔算大', '左边一排都系卡座', '选择方面都ok多', '会耐唔耐换少少新款美食', '海南鸡饭出品一般', 'd饭少少硬', '鸡腿肉无骨', '七彩海王炒新竹米叫做ok啦', '落d辣椒酱吃会好吃d']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家吃的还算不错', '分单刷卡每人', '非要刷每人', '搞得人不想再来']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好吃的早茶千篇一律', '难吃的早茶千奇百怪', '慕名而来', '败兴而去', '别说跟广州的炳胜比', '跟加州的本土早茶比', '也没有超越到哪里去', '流沙包还是不错的', '但是虾饺疑似复蒸', '澄粉皮一夹就破', '甚至还会黏在蒸笼上', '门口玻璃上标签那么多', '却连个虾饺都做不好', '也是令人震惊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不太能理解', '消費低於', '元不能刷卡', '一旦要刷卡就要charge', '變相要求客戶消費到', '或是使用現金以便業者逃漏稅', '這樣還要charge', '讓消費者感覺被受騙', '米粉湯裡面的主菜也是少之又少', '只會去一次了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['估不到系度都吃到', '位于scarborough', 'midland分店', '闲日晚上去非常少人', '接近包场甘', '坐得几舒服', '辣度', '麻度都可选', '少麻少辣唔错', '肥肠面的肥肠够霖够入味', '面的软硬度适中', '仲有得免费加面']\n",
            "WARNING:root:cannot compute average owing to no vector for ['トロント市内のフォー屋さん', 'もやしや香菜か一緒に出てくるか', '特に説明はない', '生ヒールはない', '瓶ヒールはそのまま出てくる', 'たいていそうたか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['士嘉堡中心連接巴士总站', 'station和轻轨', '交通方便', '隔离就系civic', 'centre政府办公楼', '商场改造后又靓又新左', '虽然改造工程还在进行中', '不过多了好多大牌店铺', '真系可以逛大半日', '而且还有电影院在商场内', '真系唔行街都可以去看电影', '唔会闷亲']\n",
            "WARNING:root:cannot compute average owing to no vector for ['最近因為天氣轉冷', '拿上手熱熱的', '非常適合暖一暖身體', '我覺得比我平常喝的會甜了一點', '但是冬天嘛', '喝一些甜的也無妨喔']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食品正', '紅燒牛肉麵', '麵同牛肉都超好吃', '不過湯可以辣一點就更完美', '其他食品都不錯只是上菜時間', '如果先上䓤油餅等小食', '再上麵同飯', '比較合理一點', '其實如果落單前問一下', '是否須要等上左小食先至上飯麵', '視付客人須要就更貼心']\n",
            "WARNING:root:cannot compute average owing to no vector for ['辣的很过瘾哦', '都好好次', '水煮鱼', '和酸菜鱼都是鱼片都是我的最爱', '我不喜欢有刺的鱼', '不会吃的很多', '正正好好', '汤也是很新鲜很好喝']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不錯吃', '菜色也有多樣選擇', '而且不會太過甜', '上次點餐不曉得這有buffet', '想點這個用英文', '還跟老闆娘溝通半天', '最後她說用中文講就通了', '烏龍竟忘了這叫buffet', '不過今天去第二次', '就駕輕就熟囉', '順便跟老闆哈啦哈啦', '說我一定會幫他yelp一下']\n",
            "WARNING:root:cannot compute average owing to no vector for ['看落普通甘', '竟然出品又唔错窝', '相对精致d', '份量适中', '味道还算可以', '比较中意距整的龙虾', '不会太油', '吃到龙虾鲜味', '肉质弹牙又甜', '蒸鱔味道可以但有点油', '夏天天气可以坐户外', '看住日落吃晚饭', '别有一番风味', '价钱都算合理']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Demo of word averaging doc vector...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-0.26582885,  0.22404371,  0.1287943 , -0.53785056,  0.07666932,\n",
              "       -0.25357029,  0.54859519,  0.48824978, -0.16573307,  0.08325347,\n",
              "       -0.67627025,  0.17856796, -0.25076663,  1.58109665,  0.30031291,\n",
              "       -0.27396011,  0.01270146, -0.4202247 ,  0.08845488,  0.43136281,\n",
              "        0.01242834,  0.04251913, -1.2551403 , -0.18725334,  0.10683066,\n",
              "       -0.86247623,  0.45531714, -0.01915023,  0.27224302,  0.45840284,\n",
              "       -0.25301138,  0.11330594,  0.47666591, -0.24454993, -0.51720333,\n",
              "        0.08282014,  0.06324197,  0.68052262, -0.41023502, -0.15548965,\n",
              "        0.7444976 , -0.60998619, -0.40074015,  0.27874401,  0.12731276,\n",
              "        0.56095165, -0.43628484,  0.36843908, -0.37784827,  0.32394493,\n",
              "        0.32103467, -0.00942253,  0.62965661, -0.97230816,  0.09360031,\n",
              "       -0.62158096,  0.04257327,  0.7877357 ,  0.35178643,  0.53417367,\n",
              "        0.02722833,  0.11257032, -0.50214124,  0.27306372, -0.59637451,\n",
              "        0.09255053,  0.33958894, -0.80236363, -0.01565541, -0.33717549,\n",
              "       -0.02671836,  0.26149768, -0.85895199, -0.52932525, -0.32351112,\n",
              "        0.29073223,  0.06522175, -0.34931979, -0.97549409, -0.13633515,\n",
              "        0.08227906, -0.08470681,  0.34392008, -0.5022732 ,  0.40724409,\n",
              "       -0.73980176, -0.53802574, -0.19414061, -0.27970925, -0.17715199,\n",
              "        0.04312287, -0.41246593, -0.17098075,  0.30343544,  0.67813623,\n",
              "        0.44069856, -0.36417913,  0.17384519,  0.20004793, -0.04225012,\n",
              "       -0.62992752,  0.3337647 ,  0.47255024,  0.35561869, -0.43325552,\n",
              "       -0.59114623, -0.48214605,  0.3652983 ,  0.57932889,  0.65395135,\n",
              "       -0.596735  , -0.53610241, -0.12526472, -0.13844277,  0.13793983,\n",
              "        0.60143417, -0.39693129, -0.53256297,  0.18877228,  0.18879922,\n",
              "       -0.01945457,  0.33158097, -1.00129366,  0.41400853, -0.31941688,\n",
              "        0.14605883,  0.58580744,  0.16741316, -0.28029349, -0.65880102,\n",
              "       -0.00394167, -0.26673833,  0.48893726, -0.09355756, -0.12025096,\n",
              "       -0.70807773,  1.16156328, -0.02377944, -0.24220154,  0.79753166,\n",
              "       -0.82035589,  0.24041361,  0.45327419, -0.39688173,  0.09937946,\n",
              "       -0.56234682, -0.33412737, -0.69769013,  0.17311463,  0.01982403,\n",
              "        0.02764639,  0.02388966,  0.49830163,  0.00663418, -0.62297088,\n",
              "       -0.24969451, -0.01650686,  0.77824879, -0.37680456, -0.40386289,\n",
              "        0.11049893,  0.63192719, -0.05635716,  0.37007844,  0.19520906,\n",
              "       -0.53557992, -0.29423898, -0.30757385,  0.1366463 ,  0.17433132,\n",
              "        0.2286853 ,  0.4022018 , -0.05936392,  0.21557379, -0.36756164,\n",
              "       -0.44321522,  0.41759732,  0.63045555,  0.11475223,  0.67874521,\n",
              "       -0.00468549,  0.56116879,  0.48096356,  0.10657101,  0.06308687,\n",
              "       -0.09709854, -0.89556682, -0.37360954,  0.36468285,  0.30544332,\n",
              "       -0.26040348, -0.52801108,  0.52997464, -0.4499507 ,  1.02799165,\n",
              "       -0.5783627 ,  0.04980203,  0.85129577, -0.21303998, -0.77840143])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wgG1gkaovWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "152bb37a-99fa-4c59-fbb1-81f827aa42b7"
      },
      "source": [
        "print('Shape of word-mean doc2vec...')\n",
        "display(doc_vec.shape)\n",
        "#print('Save word-mean doc2vec as csv file...')\n",
        "#np.savetxt(os.path.join(dir_path,'doc_vec.csv'), doc_vec, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of word-mean doc2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(275197, 200)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFOBdDkyV-bR",
        "outputId": "24da9a76-9fea-4a04-ff21-f7295137de98"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 275197 entries, 0 to 275196\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   stars   275197 non-null  float64\n",
            " 1   text    275197 non-null  object \n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 4.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxUdC31qpRoL"
      },
      "source": [
        "np.savetxt(os.path.join('./word_embedding/','doc_vec.csv'), doc_vec, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfn3RLZpFX8"
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "\n",
        "\tdef __init__(self, word_model):\n",
        "\n",
        "\t\tself.word_model = word_model\n",
        "\t\tself.word_idf_weight = None\n",
        "\t\tself.vector_size = word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\t\"\"\"\n",
        "\t\tFit in a list of docs, which had been preprocessed and tokenized,\n",
        "\t\tsuch as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
        "\t\tThen build up a tfidf model to compute each word's idf as its weight.\n",
        "\t\tNoted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
        "\t\t:param\n",
        "\t\t\tpre_processed_docs: list of docs, which are tokenized\n",
        "\t\t:return:\n",
        "\t\t\tself\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\ttext_docs = []\n",
        "\t\tfor doc in docs:\n",
        "\t\t\ttext_docs.append(\" \".join(doc))\n",
        "\n",
        "\t\ttfidf = TfidfVectorizer()\n",
        "\t\ttfidf.fit(text_docs)  # must be list of text string\n",
        "\n",
        "\t\t# if a word was never seen - it must be at least as infrequent\n",
        "\t\t# as any of the known words - so the default idf is the max of\n",
        "\t\t# known idf's\n",
        "\t\tmax_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
        "\t\tself.word_idf_weight = defaultdict(lambda: max_idf,\n",
        "\t\t\t\t\t\t\t\t\t\t   [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
        "\t\treturn self\n",
        "\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdB2N2bZpC1j"
      },
      "source": [
        "tfidf_vec_tr = TfidfEmbeddingVectorizer(word_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbQxL5wHpNvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59ff74a-38ec-4658-c840-3fdb3da35fab"
      },
      "source": [
        "\n",
        "tfidf_vec_tr.fit(all_docs.doc_words)  # fit tfidf model first\n",
        "tfidf_doc_vec = tfidf_vec_tr.transform(all_docs.doc_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:cannot compute average owing to no vector for ['お店の接客レヘルは最低てす', 'それと', 'とにかく遅いてす', 'オーターを聞きに来るまて', '飲み物か出て来るまて', 'ヒサか来るまて', '時間てした', '他のお店に行きましょう']\n",
            "WARNING:root:cannot compute average owing to no vector for ['クロワッサンも美味しいけと', '種類あるキーシュかおススメ', 'おススメ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['係la就中意吃', '雖然唔吃辣', '但係佢唔辣既都好吃', '特別佢個綠豆湯', '超解渴', '仲有個震騰騰既蒸蛋', '好過蘿蔔臣', '佢話我太短喔', '要夠長先得喔']\n",
            "WARNING:root:cannot compute average owing to no vector for ['非常nice的面馆咧', '店里的小姐姐很nice', '面就更不用说啦', '光看图片就超级有食欲了', '味道更加好哦', '美味美味美味呀', '值得一试哦', '总之真的很好吃就对了', '店面很大', '装修很有味道', '个人非常喜欢到颜值高的店', '享用美食', '嘻嘻嘻']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道很正', '面很足', '配菜少了点', '绿豆汤不错', '但不喜欢里面的糖的比例', '肥肠味道一般', '牛肉比较淡', '小菜还可以', '徵儿说干拌牛肉不该加那么多辣椒', '然后猪嘴肉太淡了', '徵儿说应该加酱油而不是辣酱', '我指出应该是辣椒面而不是辣椒酱', '徵儿说吃到最后完全没有感觉']\n",
            "WARNING:root:cannot compute average owing to no vector for ['没想到能在vegas吃到豌杂面', '味道正宗', '吃货一本满足', '好吃到哭', '而且面的选择也多', '即使不吃辣也有其它面可以选', '还有抄手', '不过我们这次没有尝试', '下次会再来试试', '而且停车也不成问题', '位置很好找', '我们是中午来的', '也没有等位', '总体很满意']\n",
            "WARNING:root:cannot compute average owing to no vector for ['从面到附送的小菜', '绿豆汤', '全都超级棒', '在三几顿西餐自助后来这吃碗面', '有种灵魂瞬间回归的感觉', '墙裂', '分推', '来vegas玩', '喜欢川味的小朋友', '请一定一定打卡', '滋味小面', '感觉下次再来vegas', '有一半是冲着这个面来的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['挺不错的', '味道都挺好', '食物卖相也不错', '唯一遗憾蒸蛋没啦', '跑来中国城吃饭', '周围也蛮好停车的', '点了一个面一个红油抄手', '一个小菜', '又送了一个木耳', '酸酸辣辣的', '很多地方的红油抄手都会偏甜', '就感觉很不正宗', '但是他们家的完全不会', '所以觉得很好哦', '然后面里的炒黄豆很好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['時に遅めのランチて訪問', 'この時間ても', '分くらい待ったから本当に人気店', 'コースて注文', '生カキも食へられたし', 'お肉も美味しいし', 'コスハ良し', '味よし', '店員さんも感しよかったし', 'おススメ', 'ラスヘカス旅行時は行く価値あり']\n",
            "WARNING:root:cannot compute average owing to no vector for ['点的是兰州拉面', '红油抄手和干炒牛河', '兰州拉面就是机器制作的挂面类的', '不是拉面', '红油抄手基本上没有馅', '全都是皮', '干炒牛河相对来说比较甜', '总体来说又贵又不好吃', '建议的话可以往郊区走一点', '可以吃到便宜又正宗的中餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヘカスの大通りに面している', 'メキシカン料理', '味は普通に美味しい', '店内のテコレーションかお洒落']\n",
            "WARNING:root:cannot compute average owing to no vector for ['如果是台灣的朋友看到這篇留言', '不要猶豫了一定要去', '完全的台灣味', '吃了會想哭', '一百分的服務和餐點', '份量完全飽', '但是烹調不馬虎非常用心製作', '這是我們在美國吃到最好吃的東西', '我永遠不會忘記的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['家在陕西超赞', '超好吃', '严重推荐', '大盘鸡', '羊肉串', '孜然羊肉', '肉夹馍', '此处口水', '各种各样的正宗西北面食', '无可抵抗', '超级严重推荐', '老板人很nice', '幽默直爽实在', '啤酒随便灌', '白酒三斤半', '会有折扣哦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['春巻きやエヒのカクテル', 'お近くに立ち寄りの際には是非']\n",
            "WARNING:root:cannot compute average owing to no vector for ['超市雖然不算大', '但種類都幾豐富和新鮮', '特別係韓國自製的各式泡菜', '多種', '還可以試吃後才選購', '遊水海鮮類種類不多', '反而韓國出名的烤肉類就幾多下', '當然係牛肉做主打', '雪花肉紋好吸引', '超市還連著幾間小食店', '買菜和現成熟食都一應俱全', '超市係馬路上個大招牌好明顯', '好易穩到', '隔離有kfc', '小時營業']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しいステーキハウスてす', '値段も手頃たと思います', 'オーターを失敗して', 'メイン料理を', '人シェアってお店的には', 'はっ', 'って感してすよね', 'すいません', '量かわからなくて', 'また是非トライしたいお店てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['タイ料理屋さん', '夕食分と朝食分を持ち帰りて注文', '自分', '日本人なんたけと', '辛さはとのくらいか良さそう', 'って聞いて', 'ミティアムにしたんたけと', 'ミティアムてもたいふ辛かった', 'ても', 'また行きたいお店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['一言以蔽之', '难吃', '鸡肉吃到一嘴盐巴', '猪肉感觉吃到洗洁精', '鱼肉水水的', '酸辣汤一股怪味', '我是吃得恶心了', '店内选择还算多', '补菜也算快', '十分昏暗', '厕所设计不合理', '而且垃圾桶爆满', '厕纸摆地上', '这个价格的', '团餐', '真的是很糟糕', '可惜周围没其它吃的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每人', '美金', '专门坑旅游团', '食物质量是真的差', '白菜是苦的', '还没有筷子', '我们为了环保所以不提供筷子', '这么低劣的借口降低成本', '如果有时间哪怕是多一点点时间', '别付钱给导游来这家', '去旁边的餐厅', '平时不写评论', '但我是真的被苦白菜激怒了', '别的不说了', '祝这家早日关门', '傻逼自助餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['海鸥假期用来骗游客钱的垃圾餐厅', '把国内旅行团的恶习都带到美国', '故意选在偏僻位置', '还必须在此停留一个多小时', '不吃的客人不能使用洗手间', '门口不提供休息点', '大巴车门关上', '就是要游客无处可去只能进去吃', '汉堡什么的都有']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヒオリアて行くなら鉄板', '間違いない', '堅い', 'もちろんスイーツもクット', 'というのもいいと思います', 'お店の内装にも注目してくたさい']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['ここか出てきた', 'ロフスターやサーモンなと', 'シーフートを楽しめる', 'たいたい値段は', '程度']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我是湖南辣妺子', '来美国', '太久没有吃过这么正宗', '这么辣的湖南菜', '在这里真的吃出了家乡味', '酸辣牛肉丝', '家常茄子煲', '炒菜心实在是下饭', '给你十个赞', '希望你们成功的经营下去', '最好来波特兰开', '吃完了后才想着拍照']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太正宗了', '在美国吃过十来家湘菜馆', '这一家味道数一数二', '份量最足', '强烈推荐活水煮活鱼和炒土鸡', '跟家里一个味道', '价格也很便宜', '三个人点四个菜大概六七十', '吃不完的打包回去再吃一餐都够了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['新乐园的干炒牛河', '牛肉炒的非常老', '很硬', '很难咀嚼', '也不入味', '配菜有胡萝卜丝', '豆芽', '芝麻', '洋葱', '青葱给的比较多', '切的中长段', '虽然整道菜的颜色很不错', '但是味道真的不怎么样', '河粉似乎切块了', '还都坨在一起了', '量也很少', '河粉颜色分布不均匀', '没有味道', '很不推荐了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['思う存分に楽しめる', '年齢のせいもあるか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['重庆吃的就是比较重口油多', '师傅是重庆人', '暂时还没有踩雷', '鱼香肉丝', '泡脚兔丁', '伤心凉粉', '小面都不错', '有空尝试其他的菜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['以前此地址之山窿粉己轉了', '次經營者', '最近由', '看見了也流口水吧味道非常不錯', '還有麻辣雞鍋大家也去試一下吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['他們的點心分兩類', '傳統的蒸點以及在開放式廚房現做', '現煎', '現烤', '等等其它小點心和特式點心', '上桌的菜肴顯現出十足的新鮮美味', '雖然收', '的茶資', '但茶的品質比一般飲茶的地方好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['樓下的先生您好', '老闆是台灣人喔', '我馬上幫您反應您的豬排', '雞翅問題', '店家給我看了每一個order', '今天很不巧沒有人外帶豬排單子', '你是不是評論錯對手了啊', '我下班都要帶雞翅回來啃呢', '我想你應該點錯地方了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星', '永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星', '永康食物很不錯', '點了個牛肉麵', '炒飯都好好吃', '下次還會再回來', '環境也不錯', '服務好評', '我給', '顆星']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスに何度か来ていますか', 'スハイシー豚骨かオススメ', 'チャーハンも美味しいてす', '昨日に引き続き', '日連続の来店てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しくて好きなお店てす', '辛さも選へるし', 'お店の人もナイスな人か多いてす', 'カーリックナンも美味しい', '何故かいつも空いてるんたけと', '美味しいと思うんたけとな']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['息子のオーターてあるトンコツ塩', 'も試したか', 'これもまた美味']\n",
            "WARNING:root:cannot compute average owing to no vector for ['噗了', '凑成', '成vv', '瞅瞅']\n",
            "WARNING:root:cannot compute average owing to no vector for ['噴水を見る合間に', 'ラスヘリーマカロンと', 'もっとクリームっほい', 'ケーキも気になったのてすか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['その下見として訪問しました', 'とりあえすこの日は', 'ヒサと言うよりは', '全てにアホカトか乗っており']\n",
            "WARNING:root:cannot compute average owing to no vector for ['但不知道今天的仙草怎麼了', '軟軟的一點咬口都沒有tt']\n",
            "WARNING:root:cannot compute average owing to no vector for ['臭豆腐加泡菜真是一絕', '味道很好', '更添風味', '牛肉麵整體評分算是中上', '有吃過更好的牛肉麵', '這家算是也可以', '麵條本身感覺比較粗', '比較硬', '沒有彈性', '湯頭可以', '份量夠大', '價格屬於比較經濟實惠的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在網站看見四星', '過來試試', '餸菜選擇很多', '有上海', '四川', '廣東', '雞羊牛海鮮樣樣有', '老公二人只點了簡單的', '包括鼓椒炒蜆', '楊州炒飯和畺汁芥蘭', '好味', '地道廣東菜味道', '送湯和糖水', '也很好味', '無味精', '價錢絕無收貴']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ここは美味しい', 'と確信しました', 'きつねうとん', 'カレーうとん', '雲丹クリームうとん', '日本と変わらぬお味にリヒ決定']\n",
            "WARNING:root:cannot compute average owing to no vector for ['行mall除了选星爸爸饮品外', 'aroma的饮品都几唔错', '位于vaughan', 'mills的分店都几大下', '有卡座有普通座位', '或者又可以坐商场过道的座位', '平时少机会饮开心果拿铁', '距既出品唔错', '饮到尾可以吃到果仁碎', '口感丰富', '拿铁香滑', '唔错唔错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本人の友達と', '人て食へに行きました', 'スタッフの方はフレントリーて', '話しやすく', '英語かわからない私にも親切に', '話していたたけました', '甘めのハーヘキューソースて', '日本人の口に合った味てした', 'お肉の種類も豊富てよかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我們點的東西並沒有上齊', '服務生態度普遍都比較差', '買單時候自動charge了小費', '懷疑食材不是很新鮮', '食物吃起來就是一般般', '以後不會再去了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['靠近马路边', '都改名几次了', '一直都系做中餐厅', '饮茶就未试过', '吃饭就呢次试左', '刚好有酒席', '所以上菜稍微耐左d', '份量唔细', '全部菜式稍微油多左d', '总体来说还算可以啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['中文名叫福临门酒家', '近sheppard', 'midland怡东广场内', '车位吾算特别多', '开业都几耐了', '内里环境比较旧', '地方吾算大', '吾知系咪无洗地毯的关系', '好多蚊子', '边吃边咬', '好痕吃得吾舒服', '味道就普通啦', '吾算特别好吃', '周末晚市都小猫三两只甘']\n",
            "WARNING:root:cannot compute average owing to no vector for ['身为一个西北人', '对于这家店还是蛮失望的', '点了烤羊肉', '烤羊腰', '炒面和羊肉汤面', '烤肉和烤腰子勉强ok', '但是炒面根本就是河南烩面的感觉', '羊肉汤面羊肉味很淡', '像是味精汤', '另外他的音乐', '我以为会说民族音乐', '结果是时而中文', '时而英文的流行乐', '看着墙上的新疆壁画', '听着这样的音乐', '感觉很不搭']\n",
            "WARNING:root:cannot compute average owing to no vector for ['フューションほいけと', 'うとん', '蕎麦', 'ラーメン', '持ち帰る時は']\n",
            "WARNING:root:cannot compute average owing to no vector for ['老外的连锁汉堡包店', '分店相对比m字头快餐店少好多', '位于著名伊顿中心对面', '招牌醒目好易看到', '时不时有不同的特价美食', '汉堡包份量比较大', '馅料多', '味道比起某店更好吃更滋味', '薯条就一般', '环境比较旧', '或者以后翻新下会更舒服']\n",
            "WARNING:root:cannot compute average owing to no vector for ['东方海鲜酒家的干炒牛河', '细型的河粉', '炒得很碎', '有糊块', '还有很多黏连在一起的块粉', '不仅河粉不入味', '还有很大的糊味', '青葱虽然放的多', '但是也都炒蔫了', '估计是放入的时间过早', '也有可能是不够新鲜', '牛肉也比较普通', '尝起来不是很新鲜', '其他配菜就只有芽菜', '整体非常油腻', '不推荐', '但是老板很不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['但是因為我是第一次光顧', '例如中式牛柳等等什麼什麼的', '但是水準味道什麼什麼的都還好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['室内のインテリアか素晴らしく', '居心地のいい感してした', 'カツカレーを頼みました', '肉は柔らかく', '次回はお持ち帰りて頼みたいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本のチェーン店のお店牛角', 'お肉かコムのような感して', 'あんまり美味しくなかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスのスントゥフ屋ては', '間違いなく美味しいスントゥフ屋', 'お米もすこく美味しい', 'お茶漬けにもてきます', 'laカルヒの味付けも最高てす', '時間なのも良い']\n",
            "WARNING:root:cannot compute average owing to no vector for ['オススメされて行ってみました', 'シャリか小さいのて', 'いくらとハマチ', 'まくろも美味しかったてす', 'また行きたいと思います']\n",
            "WARNING:root:cannot compute average owing to no vector for ['這裡還能吃到韭菜盒子和小籠包', '炒飯', '炒麵', '合菜', '傳統麵點', '現做的需要等候但都很不錯吃', '牛肉炒麵和打滷麵都很好吃', '麵口感有嚼勁', '而是清淡的適合亞洲人口味喔', '喜歡亞洲食物的朋友一定要來']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真的鬼火冒', '每次点postmate都会出错', '点了霸王肘子', '送给我一个粉蒸肉', '你们弄不好就不要弄外卖了嘛', '真的气得想吐血', '燃火啊兄弟', '盘盘都这样', '啊啊啊啊啊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很道地四川菜', '我們帶了墨西哥客人一起吃晚餐', '我們四人點了乾煸四季豆', '牛肉', '牙籤羊肉', '水煮魚和辣子雞丁', '超推薦牙籤羊肉', '水煮魚非常香辣好吃', '真的是很值得一吃再吃', '讓我們吃白米飯時非常驚艷', '每道菜份量十足', '希望再訪']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店鋪有三層樓', '可能生意很好', '但後來點了可麗餅覺得還好而已', '我點了鹹食的可麗餅', '料餡很多', '但口味就一般', '沒到讓我覺得驚豔的感覺', '但店面在老蒙特利爾舊城區', '所以感覺蠻像在歐洲吃飯的', '不太加拿大', '來這邊可以點杯飲料聊天', '還不錯這樣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太咸了', '难吃死了', '而且', '菜单上明明写的是', '却算我', '块钱', '还没加上税的价', '服务态度一般', '还要小费', '食物并没有评价的那么好吃', '还不如去吃in', '以后也不去吃了', '又贵又难吃', '收银员一脸欠多还少的面孔', '看了都不想买他们家的东西了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ランチて来たけと', 'カレーも美味しかったし']\n",
            "WARNING:root:cannot compute average owing to no vector for ['個入りを', '人てシェア', '余裕て食へきれるサイス', '美味しかったーー', 'ヘーコンか乗ってるものもあって', 'カナタらしさを感しました', 'トーナツは基本同し味て']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很正宗的墨西哥餐', '调的酒也很有特色', '好漂亮', '味道很好', '适合女士喝', '要求少放冰', '他给的冰也真的很适量', '菜分量很足', '午餐菜单也非常划算', '生意很红火', '排了半个小时队才有位子坐', '环境不错', '服务也很好', '整体感觉很不错的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['お米をたへたくて', '量か多過きるのかたまに傷', 'てきれは', '人以上ての来店をお勧めします', '値段も許容範囲てしたよ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['菜色種類很多', '羊肉料理做得比較好吃', '雞肉料理普通', '沒有太多味道', '沙拉種類很多', '整體菜色感覺非常豐富', '可惜味道有一點點偏鹹', '如果喜歡重口味的人比較適合', '價格挺便宜的', '這是一個不錯的選擇', '整體感覺很乾淨', '服務也可以']\n",
            "WARNING:root:cannot compute average owing to no vector for ['当时一样都点了两串', '满满的一大盒', '年糕香肠和鱼糕饼真心好评', '剩下的什么鸡肉串干得要死咬不动', '还有什么鱿鱼的那个巨难吃', '反正我是hold不住', '对了刷的那个酱简直完美', '忘了什么名字', '一共两种酱', '没试过辣酱', '总的来说还是不错的小吃店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道非常平淡', '装潢不错', '也很干净', '但是食物一点滋味都没有', '完全不想再来第二次', '第一次觉得炸虾一点都不好吃', '茄子感觉不像是烤', '像是蒸的', '秋刀鱼肉已经烤到很乾', '炸鸡没有味道', '炒青菜没有味道', '整个的感觉就是干净', '但是没有味道']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しかったてーす', 'クリーンカレーを頼みました', '値段も安くて良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美しい日本料理と', '面白いお酒', 'とちらも美味しいてす', 'ハッヒーイヤーはオートフル', '寿司', 'お店は広々ゆったり', 'オーナは日本人なのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['菜色種類很多', '羊肉料理做得比較好吃', '雞肉料理普通', '沒有太多味道', '沙拉種類很多', '整體菜色感覺非常豐富', '可惜味道有一點點偏鹹', '如果喜歡重口味的人比較適合', '價格挺便宜的', '這是一個不錯的選擇', '整體感覺很乾淨', '服務也可以']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在国内都曾经兴起过', '不过貌似风潮已过', '而系多伦多竟然都开左', '店铺装修走韩风', '例牌台面都有木质长方蜡烛加热座', '食物方面选择不算多', '价钱都唔系平', '试了原味分米鸡', '套餐成个长盘上', '两盘鸡之间有白色酱汁配搭', '套餐还包炸薯条', '沙拉和', '碗白饭', '分米鸡口味偏甜', '唔算特别', '白饭非常细碗', '女生吃就刚刚好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['同事的屋企人系呢间面包店上班', '所以时不时同事拿面包比大家分享', '有日去完附近的medical', 'centre就离左呢度吃个蛋治', '好耐无吃过', '中规中矩啦', '蛋都算多', '面包店环境算有点旧', '但帮衬的人唔少', '切饼蛋糕款式都几多', '比较平']\n",
            "WARNING:root:cannot compute average owing to no vector for ['淨係店名以為就系一間麵包店', '其實唔算係曬咯', '入口處就看到有戶外客座使用', '唔使老外系度嘆咖啡同包點', '進店后除左睇到蛋糕柜', '仲有個小餐廳', '蛋糕櫃有靚靚蛋糕', '包點', '意式雪糕', '仲有芝士同自家制的歐式麵包', '堂食外賣都得', '價格比超市稍微貴dd']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听讲开左间银记肠粉', '原来系呢个唐人plaza内', '唔知同广州吃的差几远呢', '进店后看到一部大电视', '墙上挂住好多金属奖状', '仲有珠江的旧照tim', '好有亲切感', '主打肠粉必须要试', '招牌牛肉肠粉尚算滑溜', '除了烟韧度唔够和豉油味道稍逊外', '都算合格啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好好味啊', '腸粉有腸粉味', '蛋有蛋味', '哈哈', '好q正宗', '在加拿大吃到咁正宗的廣州腸粉', '比個讚', '而且開得夜', '可以食宵夜', '呢間西嘢d珍珠雞同奶茶唔錯', '都可以試下', '最緊要有廣州feel', 'd企檯應該全部係廣州人', '好似翻左廣州咁']\n",
            "WARNING:root:cannot compute average owing to no vector for ['早晨从温哥华流窜过来', '到这个广场', '大统华超市', '点才开门', '这家', '点多已经不少人了', '形态还是中餐形态', '味道不行', '内部稍有些腥味', '青菜本地挺贵只给了很小的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['唐园的干炒牛河也用的是细河粉', '配菜和传统不同', '有芹菜', '白菜', '胡萝卜丝', '青葱', '嘱咐了少酱少盐', '因此颜色比较淡', '口感偏湿', '河粉不是很好', '偏硬', '没有弹性', '牛肉片腌制的还不错', '比较软糯', '总体来说', '分量足', '河粉偏湿', '不过河粉本身口感不佳', '比较一般']\n",
            "WARNING:root:cannot compute average owing to no vector for ['非常糟的一次經歷', '點了餐', '說是一個小時', '但是來了以後還要在等一個多小時', '因為根本就沒做', '此外', '或許因為是外賣的原因吧', '服務態度簡直是差的不能理解', '還在訓斥顧客的', '所以', '我個人來說', '是不會想去了', '建議不要點外賣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真是跟国内的焖锅没法比啊', '一点都不辣', '不过最后方便面下进去倒是很入味', '挺好吃的', '其余的真没什么印象了', '去过两三次之后不会再去了', '而且价格还不便宜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好耐无离呢度吃野啦', '早左少少出门过来吃个早餐先', '仲系保持翻旧有的环境', '无变过', '坐低点餐', '价钱就比以前升价dd', '雪菜牛肉米粉味道稍微淡左d', '流心双蛋不是新鲜煎', '所以唔系好够热', '多士少少干', '埋单盛惠']\n",
            "WARNING:root:cannot compute average owing to no vector for ['开业前路过', '店内支持微信和支付宝付款', '非常方便留学生和国内游客', '饮品最快有得饮', '玫瑰柠檬梳打的柠檬味不算突出', '玫瑰糖味比较重', '盐酥鸡等了大概半小时先整好', '炸得一点都吾哝', '几脆口', '粉吾算重', '个人觉得吾够香', '味道如果稍微重翻dd更好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['既没有惊奇之处也没有明显缺点', '对于我来说去哪都差不多', '当然我也不是广东人', '不能算是优秀的评价吧', '但可能应该也有不少人和我一样']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位於eglinton', 'ave同don', 'mills附近', '應該是新開無耐的華人超市', '超市入口的喜慶恭賀盆栽還未設走', '望落去估計是豐泰超市的兄弟店', '樓底高', '地方大', '產品擺设整齊', '整潔', '人唔多', '逛得幾舒服', '停車場夠大', '住附近的華人夠曬方便', '希望以後都能保持甘乾淨舒服']\n",
            "WARNING:root:cannot compute average owing to no vector for ['入場するのに', '近くかかります', '頃て女の子は', 'てはなく', '友達と行きましょう', 'ヒールは', 'くらいたったかと']\n",
            "WARNING:root:cannot compute average owing to no vector for ['トリュフホテトもおススメ', 'とっても感しのいいお店て', '美味しかったー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['真心不怎么样', '中午没有雪蟹腿', '菜都很一般', '寿司品质还行', '两小时的限制也让人不愉快', '前台不友好', '这里真心没有爱', '和很糟糕', '没礼貌的服务']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食物还可以', '但吃饭要排很久的队', '而且歧视中国人', '吃饭还要出示护照', '不是很好找', '在凯撒一楼赌场的里面', '牛排还可以', '但是螃蟹腿', '海鲜等食物一般', '没有想象中的好', '没有醋等调料', '吃起来可能会腻', '饮料需要找服务员单独要']\n",
            "WARNING:root:cannot compute average owing to no vector for ['排队等了很久终于吃上', '人一直很多', '点了前菜炸豆腐和春卷', '炸豆腐轻易不要尝试', '春卷里有薄荷', '不推荐', '五个人去点了一个padthai', '巨酸爽不推荐', '咖喱可以', '我朋友没吃够单点了一份饭', '我觉得可能是因为泰国香米吧', '服务态度及其一般', '但是一直排队', '不是特别理解']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次去都会排一小阵子的队', '不过熏肉真的值得等待', '每次朋友来都吃的非常开心', '推荐来了旅游的人都可以试试']\n",
            "WARNING:root:cannot compute average owing to no vector for ['友人に連れられてきました', '韓国料理か好きて', '韓国に遊ひによく行きますか', 'ここの韓国料理は', '美味しかったてす', 'カクテキも美味しかった', 'トッホキ美味しかった']\n",
            "WARNING:root:cannot compute average owing to no vector for ['整體感覺就像在韓國一樣', '非常喜歡韓國料理', '超級好吃', '牛肉超嫩', '人份的烤肉套餐', '個女生吃都沒問題', '物超所值', '非常貼心', '服務態度', '環境衛生', '食材新鮮']\n",
            "WARNING:root:cannot compute average owing to no vector for ['饺子和担担面都蛮不错的', '配的汤是真的棒', '南瓜饼也脆脆的好吃', 'delivery的时间正好', '分钟吧', '虽然我在摸鱼没听到电话', '送外卖的小哥还是态度很好', '而且居然是开车送来的', '十分的良心', '总之感觉很不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ku', 'さん', '料理の腕のあるこ主人さんと', '奥様の笑顔て', '絶品てした', 'ラスヘカスて', 'ホットしたい', '迷わす', 'kuへ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食材选取新鲜而又细致', '所有的菜品都让人赞不绝口', '服务周到', '每一道菜前后都会收拾桌面', '准备好下一道菜的餐具', '菜品大部分是按人头数早已备好', '最新鲜的放到我们的盘中', '芥末酱油这些都已经帮忙抹好了', '只需要张张嘴吃进去品尝就好', '真的太喜欢了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['と予約を入れて行きました', '遅刻をしたら', 'ても道を間違え大遅刻', '申し訳なかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['アメリカのマクトナルトては', '接客はここか一番良かった', '店内てたのむなら', 'このお店をお勧めします', 'かなりお勧めてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['サラタかすこく美味しかったてす', 'お味噌は好みしゃなかったてす', '店内か暑くて辛かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好吃', '珍珠也很q', '重要', '餐點價格合理', '像是salmon其實是生的', '餐廳位置蠻好找', '附近公共交通工具也多', '服務人員態度很親切', '但就是好像沒有想像中的那麼台式']\n",
            "WARNING:root:cannot compute average owing to no vector for ['时不时经过呢间越南餐厅', '有机会朋友请吃饭所以试了', '店内地方都不算少', '午饭时间都几多人吃野', '坐满', '菜单种类好似几多甘', '想吃檬就点左香茅牛扒檬配蔗虾', '蔗虾的虾胶味道ok', '牛扒稍微过熟了少少', '檬口感可以', '份量好足', '吃不完', '价钱合理', '上菜速度ok']\n",
            "WARNING:root:cannot compute average owing to no vector for ['今天跟女朋友一起去小橙都吃饭', '感觉味道真的很不错', '很正宗', '喜欢吃川菜的朋友可以尝一尝', '服务员的服务也很不错', '环境也是一个很中国风']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太垃圾的服务', '我来吃点心', '你让我点菜你', '你有毛病吗', '我吃东西还要你来教', '店里没有一座是点炒菜的', '菜能新鲜', 'sb老板娘', '不会再来了', '这家店非常脏', '点心也贵', '还越来越难吃', '我是客人', '你太度能好一点吗', '肚子饿', '没服务', '的服务费', '实在是垃圾店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['太垃圾的服务', '我来吃点心', '你让我点菜你', '你有毛病吗', '我吃东西还要你来教', '店里没有一座是点炒菜的', '菜能新鲜', 'sb老板娘', '不会再来了', '这家店非常脏', '点心也贵', '还越来越难吃', '我是客人', '你太度能好一点吗', '肚子饿', '没服务', '的服务费', '实在是垃圾店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不過地理位置不錯', '人氣都唔少', '一到下班時間熟食區特別熱鬧', '肉檔同魚檔比較細', '應該話成個超市面積都唔算大', '但就好方便附近居住的華人購物', '超市的用餐區系超市出面', '座位唔算多']\n",
            "WARNING:root:cannot compute average owing to no vector for ['耐无吃就觉得好好吃', '不同安士不同价钱', '选了白包', '安士汉堡牛扒', '可选配料好多', '加了适合夏天口味的芒果酱', '番茄', '酸青瓜', '生菜等等', '新鲜烤好汉堡扒放入一起吃', '一淡吃落去feel到满满牛肉汁', '加了配菜吾油腻又味道丰富', '一个combo十几元']\n",
            "WARNING:root:cannot compute average owing to no vector for ['吃过midland分店', '呢度系唐人街附近系dundas', 'st的夹层店铺', '窄长的小店基本上都坐满晒人', '无midland店甘大甘舒服', '个店员在忙的感觉', '落单后稍微等了数分钟先上菜', '招牌的重庆碗杂面分量足', '因为唔系好饿', '两个女生分享刚好', '配料够多', '不过汤咸左d']\n",
            "WARNING:root:cannot compute average owing to no vector for ['因为朋友的妹妹曾经系呢度做过野', '所以朋友约我离呢度饮茶', '听她讲曾经装修过', '所以环境相比旧时好左d', '周末好多人等位', '座位有少少逼', '不过台好够大', '不怕放多几笼点心就无位', '卤水猪脚仔配海蜇味道都算可以', '凤爪份量有d少', '叉烧饭d叉烧确实唔错', '好香', '松软又有口感', '价钱算中等吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我点了土豆丝', '上来的菜那个颜色', '浅咖啡色的', '带着鱼腥味', '应该是用炸鱼的油炒的菜吧', '都舍不得用新油吗', '干巴巴的米饭用个小破铝罐子装', '都瘪了好几个坑了', '出来在出租车上司机中国人', '说本地中国人都不来这家', '难吃的很', '说你们肯定是网上看来的推荐这家']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于超多唐人居住集中地内', '隔离有超市', '烧腊店', '其他食肆等等', '所以入到去都会听到熟悉的语言', '不过服务员是外国人', '效率有点低', '下单我都要重复讲', '呢间麦当劳好多老人聚在一起聊天', '并有儿童游乐区', '和drive', 'thur服务']\n",
            "WARNING:root:cannot compute average owing to no vector for ['超大盘的干炒牛河', '牛肉超大片', '数量也多', '配菜只有青葱和洋葱', '却味道十足', '个人觉得不像是港式或者广式牛河', 'wai', 'wai家的牛河更像是美式', '用了很多洋葱', '味道比较独特', '最值得一提的是河粉', '大片软糯', '不粘连口感q弹', '干湿也正好', '目前为止河粉最棒']\n",
            "WARNING:root:cannot compute average owing to no vector for ['去这家coco买饮料', '以前都是看着他们做的', '都有放红茶', '这次我看到他们没放', '我就问你们没放红茶是吗', '他们说放了', '只是放得少', '我还以为我看错了', '结果一喝就是没味道', '员工太不走心了好吗', '气得一匹', '再也不来这家买奶茶了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['午餐时间来的', '人非常多', '听说pho比较好吃', '但我没有吃出来有什么特别的', '哈哈', '反正周围也没有什么pho', '生意应该不差']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听親戚講前身好似叫耀永', '宜家叫富臨門', '係victoria', '門口系商場停車場果邊', '非馬路面', '餐館地方吾大', '魚缸養住不少游水海鮮', '象拔蚌', '皇帝蟹', '龍蝦等等', '皇帝蟹兩吃', '新鮮特別鮮甜', '肉飽滿', '雖然有d貴', '但味道真系幾吾錯']\n",
            "WARNING:root:cannot compute average owing to no vector for ['貌似新开无耐', '午饭时间人五多', '餐厅系转角位', '餐厅地方唔算大', '简约带点中国风的设计', '餐牌上选择唔算多', '招牌的红烧肉味道有姜醋蛋的感觉', '微微醋酸香', '肥肉唔算腻', '唔讲得话好好吃', '反而卤水鸭肾唔错', '够霖又入味', '生煎包普通啦', '唔够香唔够脆']\n",
            "WARNING:root:cannot compute average owing to no vector for ['点了直接进垃圾桶', '饭是超级软烂的', '分不清是小孩吃的饭糊还是饭', '配菜你要是不想给就索性不要给', '点的咖喱鸡', '鸡肉一共小小的', '剩下全是土豆胡萝卜', '想问一下咖喱是不是根本没放', '饮料全色素', '糖浆兑出来的', '好无语']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ストリッフエリアからは車て', '分くらいのロケーションて', 'なお隣接する立体駐車場は', 'トルてす', 'クーホンなともあるのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['人気店の為', '頃に']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很好吃的三明治店的', '原本太太想來這邊吃', '但是來這邊吃一次就感動感動', '蠻不錯的味道', '這邊', '個小時開的', '所以玩賭博到很晚也可以吃到', '我們的話', '吃早餐', '吃完就還有精神', '如果今天玩賭博的話', '可能會贏吧', '哈哈哈']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['お酒のおつまみと', 'こうしさんとお話てきて', 'また', '寄りますねー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['寿司隆', 'を発見', 'つになりましたー']\n",
            "WARNING:root:cannot compute average owing to no vector for ['開店して間もないらしい', 'お酒もいろいろな種類かあり', 'とにかくすへてかリースナフル']\n",
            "WARNING:root:cannot compute average owing to no vector for ['油泼臊子面还有涮三样', '刚去吃第一次的时候惊为天人', '但多吃几次就觉得也就一般般了', '面的油有点多', '吃到最后感觉就是泡在油里', '不过味道还是蛮好的', '以后有机会去陕西吃最正宗的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['出てくるまてに', '分かかりましたか', 'とてもおいしかったてす', 'また食へたい', '店員さんに日本人の方かいました', '現金のみてすのて注意か必要てす', 'また', '米トルも使えますか', 'レートは損しますのて', 'こ注意を']\n",
            "WARNING:root:cannot compute average owing to no vector for ['この手の店にはありかちたか', '量かかなり多いのて注意', '日本人なら', 'つ頼んて', '近くに来たら寄ってもいいかも', '程度']\n",
            "WARNING:root:cannot compute average owing to no vector for ['値段も良心的て味も美味しい', 'おススメ料理の中から', 'その', 'つは', 'ちょっと微妙たったけと', '混んてた割にはすく入れて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['有名なステーキチェーンてす', 'ステーキのサイスかほほ', 'oz以上て大きいてす', '無理を言って', '人て', 'ワイン一本とステーキ', 'つとサイトの野菜てチッフ込みて', 'てした', '気持ち高いてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['まあまあ美味しかった', '安いのに', 'サラタは味付けか美味しかった']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于bloor', 'st的holt', '地下一边男装部', '一边彩妆', '和一线名牌包包区', '二楼是女装', '餐厅在夹层', '想看想买最新想知的潮流动向', '耐唔耐过来逛下hr', '总会看中不少']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我一直百思不得其解', '每个菜都是极其难吃', '味道完全不对', '客观的说', '厨师水平应该是相当的低', '点一个京酱肉丝', '做出来是咸的', '大概味道就是酱油炒肉丝', '您最起码也放点甜面酱啊', '服务环境什么的都不说', '就是味道太差', '我很负责的说', '他家倒闭只是时间问题']\n",
            "WARNING:root:cannot compute average owing to no vector for ['之前听朋友讲华丽宫出品可以', '行完街就打算离呢度吃晚饭试下', '原址是龙腾金阁', '改成华丽宫后人气高企', '早茶很长人龙帮衬', '晚饭生意都貌似唔错', '餐厅推荐的羊腩煲有点失望', '味道偏淡', '羊肉整得唔够入味', '核桃牛柳粒都算可以', '温哥华蟹蒸饭不过不失']\n",
            "WARNING:root:cannot compute average owing to no vector for ['相比起以前', '多左云吞面專門店', '所以感觉上无其他店甘出色', '传统云吞面', '细蓉', '配韭黄', '距呢度仍然系葱花', '汤底就不过不失', '面就唔够竹升面甘脆口', '乒乓球甘大的', '云吞', '系亮点', '鱼球', '有鱼味', '算系号', '价格就肯定跟物价一起上涨']\n",
            "WARNING:root:cannot compute average owing to no vector for ['很棒的咖啡厅', '咖啡', '甜点', '炸鸡waffle', '都很好吃', '擦脸的分量很大', '今天点的炸雞waffle', '有沙拉', '三隻雞腿', '好烤得非常棒的鬆餅', '每一個的味道都處理的很好', '量也夠大', '價錢感覺很合理', '超赞的早午餐', '服务态度也很好', '气氛很好']\n",
            "WARNING:root:cannot compute average owing to no vector for ['亲戚朋友要远游前', '约埋一起吃饭', '就亲戚屋企附近来了华景轩', '门口在角落位', '停车场足够大', '餐厅唔大', '一眼睇晒', '埋边有包厢', '菜式方面特价琵琶鸭唔错', '蜡下眼有少少似烧鸭', '只鸭一d都唔肥', '唔会太大只', '皮脆肉唔痴牙', '特价离讲性价比几高', '其他小菜出品都可以', '工作日晚人唔多', '倾计吃饭都唔嘈']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店面更改是不是厨师也换了', '味道极差', '巨咸难以接受', '不多bb', '能别来就别来', '入雷坑了', '难吃的一批', '我记得点了个大蒜牛肉', '不知道是不是把盐打翻了', '牛肉上大把盐颗粒', '挤完也这吊样', '服了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于太古广场', '楼民族村内的papa台式美食店', '在民族村中间的小巷内', '长形的店铺', '餐牌系收银的台面上', '比较多是主食', '小吃也有', '下单后大概等', '分钟左右就可以食了', '大肠面线', '几正宗', '少少酸的面线', '好滑', '大肠软霖入味', '下次要试试距d便当先']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好耐无吃港式早餐了', '系附近办完事后时间尚早', '吃翻个早餐叹下先', '无意记得呢间茶餐厅', '门口对住油站', '餐厅系茶餐厅出品为主', '地方唔大', '数张小台', '人多两下就坐满', '点左个沙爹牛肉米粉', '份量几多', '牛肉和沙爹牛肉汁够味', '米粉唔太硬', '耐无吃就觉得几ok个味道']\n",
            "WARNING:root:cannot compute average owing to no vector for ['中午和朋友来这家店吃饭', '心情不错', '会再来光顾', '点了一个舒芙蕾蛋糕', '就是等的有点久', '不过味道还是值得的', '棒棒哒', '饮料的名字真的绝了', '初恋的味道', '相当好看太诱人了', '还没来得及拍就被我朋友喝了', '总体而言真的不错']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次到這裏來都很早', '很寧靜舒服', '舖面也很清潔', '有停車場但沒有單車停泊的位置', '内有小孩遊樂園區', '設有用餐的坐椅', '家長們可边看着小孩玩耍嘆咖啡', '享受片刻的寧靜', '但沒有看到小孩用餐的椅子', '總括這裏花費不多', '小孩子有吃有玩', '家長們可以鬆一鬆是個不錯的選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['同朋友去湖边影日落', '在两条路的交界角落位', '离巴士站唔远', '全落地玻璃设计', '可以边叹边望住店外的风景', '地理位置唔错', '所以不少人中意系度做下功课', '或带电脑离做自己野']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['今天的菜色失望', '廚師沒用心', '全是蛋', '蛋蛋大歺', '果汁飲料都失去以前的用心', '味噌太鹹了', '以前還蠻養生', '有酥皮濃湯', '現在食物一般般', '沒有特色', '以前有水果冰茶', '有特色', '現在是一般的紅茶', '以前我喜歡來這裏吃', '現在食物退步了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧', '老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧', '老板很热情', '说我评价的字数少我也是醉了', '好吧那我继续码几个字吧']\n",
            "WARNING:root:cannot compute average owing to no vector for ['すこい人気て', '予約していなかったのて', '分くらい待ちました', 'サラタとステーキ', 'を頼みました', 'とちらもとても美味しかったてす', 'ついてくるハケットも', '接客もよく']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前回は', '時くらいに来店したら', '今回は', 'わーいわーい', '笑っ', 'たた', '午前中は待ち時間か少ないてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['確かに', 'フイヤヘース', 'ロフスターハン', 'オイスターともに美味しかった', 'たた', '客席数か少なすきる', 'ウエイトレスの効率か悪いからか', '時間近く待たされるのは辛い', '貴重な海外て', '時間待つ価値かあるかは', '悩ましい', '行列嫌いならおすすめてきない']\n",
            "WARNING:root:cannot compute average owing to no vector for ['カフェラテをオーター', 'あまりの美味しさに感激しました', 'こちらにきてまた', 'ヶ月ほとてすか', '美味しいラテに出会えなく', '探し求めてました', '本当にオススメてす', 'ラテたけのつもりか', 'おやつもオーターしちゃいました']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我不会再来这个地方', '一共三个服务生', '门口有', '个人等着进去', '我等了二十分钟', '没有一个服务生过来服务', '沟通了两次也没有人过来招待', '即便是很忙', '也可以让顾客坐进去等吧', '难道要所有人吃完我们才能进去', '草泥马的傻逼地方']\n",
            "WARNING:root:cannot compute average owing to no vector for ['アメリカて', '番好きな韓国料理屋さんてす', '接客もきちんとしていて', 'フレントリーて優しいてす', 'メニューも充実しているのて', 'お客さんは']\n",
            "WARNING:root:cannot compute average owing to no vector for ['首先不说菜如何', '小笼包还好', '牛筋太难吃了', '更重要的是等了将半个多小时', '抄手一直没有上来', '告诉我们因为忘了', '所以抄手没送上来', '而且店员的服务态度很差劲', '本来是来旅游玩的', '心情被搞的贼差', '再也不会去这种烂店']\n",
            "WARNING:root:cannot compute average owing to no vector for ['スーフは美味しかったし', '正に日本の味たって感し', '日本風かいいな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['集饮食', '购物', '医疗和娱乐活动', '停车场有室内室外', '每逢公众假期都好多人黎', '环境普普通通啦', '不过就够方便', '可惜近呢几年相对无甘繁华了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['フレントリーて心優しい店員さん']\n",
            "WARNING:root:cannot compute average owing to no vector for ['嘱咐了少酱少盐', '河粉是大的面片', '比较有嚼劲', '配菜是豆芽', '芝麻', '特别是这个青葱', '非常新鲜', '为牛河提味提色', '芝麻也是比较独特之处', '之前吃别的家都没有', '酱的味道稍微偏甜', '应该是港式干炒牛河', '偏甜口', '牛肉是碎肉', '丰富了口感']\n",
            "WARNING:root:cannot compute average owing to no vector for ['大大大好きな場所てす', '嫌な顔一つされませんてした']\n",
            "WARNING:root:cannot compute average owing to no vector for ['予約をしていくのかおすすめかも', 'そこてお試して行くのも良いかも', '店内も素敵たったし', '店員さんの対応も素晴らしい', '又是非行きたいな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食事かとってもおいしかったてす', 'ホスヒタリティー満点てした', 'スティーフナイスカイ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这几天拉斯维加斯', '大峡谷走一圈', '鼻子干到不行', '急切想喝汤', '在yelp上搜了一下', '这家店竟然有椰青土鸡小火锅', '太适合我这种中国胃了', '从strip开车过来', '大概', '分钟', '很方便停车', '奔着汤过来的', '顺带着点了烧烤', '也挺好吃的', '烤海鲜类和肉类的方式不同', '很用心', '强推']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しかったのはもちろん', '接客か最高てした', 'スタッフの方々の雰囲気か良くて', '気さくて', 'とても居心地か良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['每次去都是超级满意', '食品的质量', '上菜的速度', '服务生的态度都是最爱的原因', '最近还看到了', '他们有开始有新的早餐的菜单', '不停的有新的惊喜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于kennedy夹hwy', '的西南角', '以前系新旺角', '宜家系帝豪名宴', '痴住华人商场', '大厅好大', '会有少少嘈', '环境唔算华丽', '出品味道一般般', '价钱中等', '还有延续点心车的出现', '呢样我中意', '服务还算ok', '加水或收碟都几快', '服务员唔会扮听唔到看唔到', '近酒楼的车位周末肯定爆d', '需要停远少少']\n",
            "WARNING:root:cannot compute average owing to no vector for ['没有任何问题或者失望', '但是今天我来吃饭', '加起来税前', '税后', 'yelp上说可以', '我问服务员能打折吗', '她说', '不行', '我真就偶然看到这个折扣', '对服务还挺失望的']\n",
            "WARNING:root:cannot compute average owing to no vector for ['川北涼粉口感同味道都幾好', '對我嚟講都做到恰好', '只係略嫌唔夠麻味', '四川菜此終味道比較濃和比較油膩', '如果想食清淡嘢就唔好嚟喇', '最後還有']\n",
            "WARNING:root:cannot compute average owing to no vector for ['偶然發現新的麵館', '味道正宗做法講究', '月陽樓', '當時只給上層社會人士提供服務', '年時回族人馬保子以', '一清', '二白', '蘿蔔', '三綠', '香菜蒜苗', '四紅', '辣子', '五黃', '麵條黃亮', '統一了蘭州牛肉麵的標準', '並使用煮羊肝', '牛肝的湯來煮麵以提升其風味']\n",
            "WARNING:root:cannot compute average owing to no vector for ['朝食に', '人て利用', '広い', 'フートコート的な作りて', '最後退店時にまとめてお支払い', 'また朝早いとやってない店舗', '料理', 'とかもある', 'いろいろ選へて良い']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于yorkdale', 'mall商场内', '靠近rh的对面', 'iq', '呢个店名好特别', '主打健康的素食沙律为主的餐厅', '当然唔少得老外最爱的咖啡饮品', 'iq的沙律卖相几诱人甘', '健康为主从来都受到不少人喜爱', '下次下午茶时段过来试试', '看下口味如何']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位於密市square', 'one負一層的美食廣場開左啦', '地方無想像中甘大', '装修的围板系涂鸦墙', '又是拍照打卡点', '扶手电梯旁有现开业的餐厅店名', '環境系ok架', '幾有氣氛', '每家小店都幾好特色', '附設堂食的座位唔算多', '週末過假期人多就可能會有d逼']\n",
            "WARNING:root:cannot compute average owing to no vector for ['我确定还会回来这里', '所以我推荐大家来这里尝试', '您一定会得到一个满意的中国餐']\n",
            "WARNING:root:cannot compute average owing to no vector for ['昨天和女兒首次到這裏來', '看見有中文字的菜單', '感覺好親切', '叫了餃子山西刀削麵和蘭州拉麵', '味道很不錯', '牛肉鬆軟濃郁', '麵是師傅自己做的', '坐在椅子上都可看到', '熱茶一元用茶包泡的好大杯', '茉莉花茶好清香', '收費合理']\n",
            "WARNING:root:cannot compute average owing to no vector for ['价格涨的飞快', '每样菜都上涨了', '生意好就是这样拽', '东西也没见多给', '这老板还真是会赚钱', '本就是面饭之类的材料', '上次吃了个番茄面更夸张', '连鸡蛋都几乎没见过', '一点鸡蛋星儿而已', '真是无语了']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['少し遅めのランチに行けは', '新しく美味しい食事', 'アメリカにしては']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とれも美味しく', '店員さんの接客も親切てした', '次回ラスヘカスに行ったときは', '試してみようと思ってます']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['干锅茶树菇里全是洋葱和豆芽', '只有表面浮着几根茶树菇', '还超级老', '一看一度认为是梅干菜梗', '角度刁钻', '泪目', '未来半年都不会爱茶树菇了', '慎点', '慎点', '慎点啊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于victoria', '感觉出品同服务ok', '呢次黎系参加婚宴', '头盘的乳猪件几香脆', '唔会太肥腻', '带少少乳猪肉', '口感唔错', '带子炒时蔬的带子大粒弹牙', '姜葱双龙的龙虾就唔够香', '蛋白瑶柱菜粒炒饭味道稍微淡左d', '总体离讲都算唔错啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['美味しいかったー', 'おすすめてす', '英語か話せなくても', '笑顔て対応してくれたのて', '気落ちせすに夕食を楽しめました', '日本ては馴染みかない', 'チッフもチェックの時の伝票に', '自分ていくらにするか', '決めやすいのて安心てす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['最悪のサーヒス', '知らないと二重払いになる', '知らすに払うように仕向けている', 'チッフのことを言うと']\n",
            "WARNING:root:cannot compute average owing to no vector for ['おうとんかおそはか食へたい', 'と思って探し見つけたお店てした', 'お値段もリースナフルて大満足', '今度は夜お邪魔したいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['寒い冬にとうてすか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ヒルの最上階', 'にあるレストラン', '外は一面カラス張りて', '超高層から見える景色は圧巻', 'ホリュームは多いのて', 'すくに満腹になっちゃいます']\n",
            "WARNING:root:cannot compute average owing to no vector for ['他們家的乾料特別棒', '尤其是老闆賊帥', '人也幽默有趣', '我下次要帶更多的朋友來', '比那個tempemarket', 'place的好太多了', '服務員我很喜歡']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店員さんも優しく', '店内も綺麗に清掃されており', '美味しかったのて良かったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['シシケハフハウス', 'とあるか', '店内はとても広く', '活気にあふれている', 'うれしい']\n",
            "WARNING:root:cannot compute average owing to no vector for ['女服務員漂亮親切', '舒適美好的用餐環境', '下次還一定會常來這裏', '食物料理每道都很好吃', '完全可以放心食品的衛生', '吃得放心就更開心了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['都有好幾年无幫襯呢間分店', '晚上過了飯市高峰期唔使等位', '中意距靠邊既座位基本上都系卡位', '岩晒撐台腳', '菜單上選擇比以前增加唔少', '流行的魚湯米線都有', '鮑貝鷄絲粥', '粥好杰身', 'ok綿', '沙爹牛柳炒腸粉', '如果再香d更好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本食とは思えない店構え', '多分もう行かない']\n",
            "WARNING:root:cannot compute average owing to no vector for ['听说换了大厨', '菜的照片看着也还不错', '所以大过年的决定去试一试', '点了一个烤鱼和毛氏红烧肉', '鱼上来时没有热汽', '干吧吧的', '一尝这个鱼非常的不新鲜', '还是凉的', '红烧肉也是凉的', '干的', '总之', '不会再去了', '也不会推荐朋友去']\n",
            "WARNING:root:cannot compute average owing to no vector for ['仅为吐槽', '今天等了快', '分钟告诉我龙虾没了刚从超市买', '还他妈超市原价卖给我', '你们到底是怎么在香槟活到今天的', '就这种服务态度', '成本不够就别他妈瞎打折', '打完折隔这么久才通知顾客', '还超市原价卖给我', '美其名曰', '免费加工']\n",
            "WARNING:root:cannot compute average owing to no vector for ['凝り固まった首', 'ありかとうこさいましたm']\n",
            "WARNING:root:cannot compute average owing to no vector for ['凝り固まった首', 'ありかとうこさいましたm']\n",
            "WARNING:root:cannot compute average owing to no vector for ['麻婆豆腐和辣子鸡是我的最爱', '豆腐很嫩', '酱料甜辣得当', '重庆辣子鸡炸的很酥', '火候刚好', '也不腻', '这家菜量也大', '感觉这俩菜完胜匹兹堡其他餐厅的', '不知为何还不是很火', '店里的厨师应该是四川本地人', '做的各种川菜都很正宗', '不过有次点过三杯鸡', '感觉就比较一般']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前面评价都不高', '我吃了居然感觉不错', '中餐馆一般让人失望', '干净程度不如韩餐', '韩餐不如日餐', '这家真的还有模有样', '炒五花肉都不错', '炒饭中上', '只是韩国服务员英语听不懂', '写汉字也不认识', '菜单第一页没有编号的就头痛了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['味道很像中国国内的腌菜炒饭', '可是现在没了', '好可惜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['店員很親切', '這家店有果汁特調', '可以選你喜歡的水果', '他們會打成美味果汁', '我點了', 'nemo這個餐', '餐點包含了', '烤麵包', '烤馬鈴薯以及水果', '煎蛋裡包了鮭魚', '非常美味', '還有烤馬鈴薯也很好吃', '是間很值得再光顧的餐廳']\n",
            "WARNING:root:cannot compute average owing to no vector for ['鲍鱼捞饭', '第一口就知道是罐头鲍鱼', '酱汁陀在一起都拌不开', '这也太难吃了', '黑金流沙包还行吧', '给小费的时候', '服务员嫌少', '追问道', '今天你有什么问题吗', '这话问的', '我没什么问题', '我就是觉得不好吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['呢個plaza內唯一的韓國超市', '其他都系食肆多', '超市唔大', '有點舊', '蔬菜水果選擇都唔少', '但就无大型的肉檔同海鮮q部', '熟食部好細', '生活用品唔多', '但足夠日常補給', '價錢相對比起唐人超市有點貴', '也无galleria甘多選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['pkkoj', 'lkkoj', 'pl', 'kiool']\n",
            "WARNING:root:cannot compute average owing to no vector for ['加完服務費', '這是一個四人套餐', '服務非常好', '裝潢很華麗', '停車方便', '食材算新鮮', '只是全部都是清蒸的', '其實算是非常簡單的料理', '沒有更多的特色', '想嘗試一次可以去試看看', '沒有必要去第二次', '吃完感覺好像沒吃東西']\n",
            "WARNING:root:cannot compute average owing to no vector for ['長形店鋪', '一邊系落單區', '一邊系客座區', '也有落地玻璃墻的高凳位', '因為係市中心金融區', '好多寫字樓人士幫襯', '店員幾nice', '服務親切好笑容', '相比起其他食肆稍微晚少少關門']\n",
            "WARNING:root:cannot compute average owing to no vector for ['bbbbbbbb', 'bbbbbbb', 'bbbbbbbb', 'ggggggg', 'hgghhg', 'hcbugf', 'jhfgjk', 'ghjihbnjgh', 'gfjbi', 'gchki', 'gfnifj', 'hfnikfdyk', 'gdjk']\n",
            "WARNING:root:cannot compute average owing to no vector for ['选择了这家tea', 'station结果我后悔了', '结果好难喝啊', '而且还卖', '一杯', '喝了两口就喝不下去了', '简直就是坑人钱']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とても美味しかったてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在我们吃饭的过程当中', '甚至等了超过二十分钟好几次', '后来问了问才知道', '可能漏单了', '不过漏的也太多了', '后面直接不给上了', '上菜时间这么久', '后来点的菜根本就吃不动了', '我也就来这一次了以后不会再来了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['ラスヘカスの空港近くにあり', '便利て美味しいラーメン屋', '嬉しいホイントは', '米国のラーメンにしては安く', 'て提供', 'たた', 'からあけは特に美味しくてお勧め', '多くの人か喜ふのに', 'と感しる']\n",
            "WARNING:root:cannot compute average owing to no vector for ['這次是帶我的姐姐連續', '天吃他們的下午茶', '甜點製作用心', 'pub一樣好吃', '如果能坐到靠近garden', '位置', '不但花朵香氣四溢', '甚至景觀設計上也讓人感覺很舒適']\n",
            "WARNING:root:cannot compute average owing to no vector for ['個人的にはお勧め', '餃子も美味しい', '場所も中心地のモールから歩いて', '分距離て便利', '改善してほしい点は', 'ヒールのクラスか無く', 'フラスチックな事', 'なのて']\n",
            "WARNING:root:cannot compute average owing to no vector for ['見ていたのに何も反応せす', '拭いていたら', 'といてくれと言われました', '不愉快てした']\n",
            "WARNING:root:cannot compute average owing to no vector for ['拉斯維加斯最好吃的豬排飯', '乾淨', '新鮮', '味增湯很像在日本吃的味道', '每次想吃豬排一定來這裡', '更吸引人的地方', '飲料也很多好喝的種類', '晚餐時光約三五好友一起吃飯', '喝點生啤酒或是清酒配上日本小食', '都是不錯的選擇']\n",
            "WARNING:root:cannot compute average owing to no vector for ['去the', 'alley的时候经过呢间甜品店', '之前都唔留意到距', '人唔多比较清静', '店铺唔大', '数张桌椅供客人堂食使用', '收银隔离是雪糕柜', '台面有简易的菜单同彩图的甜品相', '不过晚上时间得翻雪糕卖', '见相内的格仔饼卖相唔错', '可惜无得食', '唯有下次再离试下啦']\n",
            "WARNING:root:cannot compute average owing to no vector for ['日本語は通しませんか味', '雰囲気', 'ヒールはdosqes', 'ambarかオススメ', 'チキンやフリトーも美味しいてす']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不味過き', 'スーフ', '具材', '全て最悪', '粘土を食へているような感覚', '味噌ラーメンなのに', '具材のチキンは猫の餌のような味', '他のメニューは美味しいのかな']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家餐厅的火锅超级美味', '肉很新鲜', '麻辣火锅', '药膳和番茄锅味道很好吃', '这个肉真是新鲜', '这家餐厅的火锅超级美味', '肉很新鲜', '麻辣火锅', '药膳和番茄锅味道很好吃', '这个肉真是新鲜']\n",
            "WARNING:root:cannot compute average owing to no vector for ['做为义美冠军的老客户', '听说他家要延长营业时间', '我真的超', '日起', '每日早', '点到晚', '周一公休', '同时还会全天供应烧饼', '油条', '豆浆', '每晚', '点后增加新品清粥和小菜', '以后随时可以吃到他家的美食啦', '完美完美']\n",
            "WARNING:root:cannot compute average owing to no vector for ['紅燒牛肉麵很好吃', '蘿蔔糕也不錯', '鍋貼及小籠包的肉餡稍微甜了一點', '燒餅不錯', '可是油條炸的太油了', '還有其他的台灣小吃', '蚵仔煎', '滷肉飯', '油豆腐粉絲', '大腸麵線', '如果你有機會來賭城', '不妨來這裡試一試', '這裡的服務態度很好', '老闆非常客氣']\n",
            "WARNING:root:cannot compute average owing to no vector for ['在這也想說可以嘗試他們的豆漿', '給我一碗肉燥飯', '沒什麼味道', '感覺水水的', '肉圓裡面也不新鮮', '皮又有點硬', '餐廳裡面也一堆國民黨的東東', '真的吃到待不下去']\n",
            "WARNING:root:cannot compute average owing to no vector for ['とても美味いシェラート', 'カフェてす', 'トロント辺りては']\n",
            "WARNING:root:cannot compute average owing to no vector for ['奥は床か上けてあって', '奥の席ても通りの景観を楽しめる', '料理の味も素晴らしかった', 'また行きたいと思っています']\n",
            "WARNING:root:cannot compute average owing to no vector for []\n",
            "WARNING:root:cannot compute average owing to no vector for ['fℓℓ']\n",
            "WARNING:root:cannot compute average owing to no vector for ['位于丰泰超市隔离', '邻近都有几间食店', '算开左几耐下的茶餐厅', '呢间同香港某烧鹅店无关啦', '店铺简简单单的布置', '唔算大', '左边一排都系卡座', '选择方面都ok多', '会耐唔耐换少少新款美食', '海南鸡饭出品一般', 'd饭少少硬', '鸡腿肉无骨', '七彩海王炒新竹米叫做ok啦', '落d辣椒酱吃会好吃d']\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家吃的还算不错', '分单刷卡每人', '非要刷每人', '搞得人不想再来']\n",
            "WARNING:root:cannot compute average owing to no vector for ['好吃的早茶千篇一律', '难吃的早茶千奇百怪', '慕名而来', '败兴而去', '别说跟广州的炳胜比', '跟加州的本土早茶比', '也没有超越到哪里去', '流沙包还是不错的', '但是虾饺疑似复蒸', '澄粉皮一夹就破', '甚至还会黏在蒸笼上', '门口玻璃上标签那么多', '却连个虾饺都做不好', '也是令人震惊']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不太能理解', '消費低於', '元不能刷卡', '一旦要刷卡就要charge', '變相要求客戶消費到', '或是使用現金以便業者逃漏稅', '這樣還要charge', '讓消費者感覺被受騙', '米粉湯裡面的主菜也是少之又少', '只會去一次了']\n",
            "WARNING:root:cannot compute average owing to no vector for ['估不到系度都吃到', '位于scarborough', 'midland分店', '闲日晚上去非常少人', '接近包场甘', '坐得几舒服', '辣度', '麻度都可选', '少麻少辣唔错', '肥肠面的肥肠够霖够入味', '面的软硬度适中', '仲有得免费加面']\n",
            "WARNING:root:cannot compute average owing to no vector for ['トロント市内のフォー屋さん', 'もやしや香菜か一緒に出てくるか', '特に説明はない', '生ヒールはない', '瓶ヒールはそのまま出てくる', 'たいていそうたか']\n",
            "WARNING:root:cannot compute average owing to no vector for ['士嘉堡中心連接巴士总站', 'station和轻轨', '交通方便', '隔离就系civic', 'centre政府办公楼', '商场改造后又靓又新左', '虽然改造工程还在进行中', '不过多了好多大牌店铺', '真系可以逛大半日', '而且还有电影院在商场内', '真系唔行街都可以去看电影', '唔会闷亲']\n",
            "WARNING:root:cannot compute average owing to no vector for ['最近因為天氣轉冷', '拿上手熱熱的', '非常適合暖一暖身體', '我覺得比我平常喝的會甜了一點', '但是冬天嘛', '喝一些甜的也無妨喔']\n",
            "WARNING:root:cannot compute average owing to no vector for ['食品正', '紅燒牛肉麵', '麵同牛肉都超好吃', '不過湯可以辣一點就更完美', '其他食品都不錯只是上菜時間', '如果先上䓤油餅等小食', '再上麵同飯', '比較合理一點', '其實如果落單前問一下', '是否須要等上左小食先至上飯麵', '視付客人須要就更貼心']\n",
            "WARNING:root:cannot compute average owing to no vector for ['辣的很过瘾哦', '都好好次', '水煮鱼', '和酸菜鱼都是鱼片都是我的最爱', '我不喜欢有刺的鱼', '不会吃的很多', '正正好好', '汤也是很新鲜很好喝']\n",
            "WARNING:root:cannot compute average owing to no vector for ['不錯吃', '菜色也有多樣選擇', '而且不會太過甜', '上次點餐不曉得這有buffet', '想點這個用英文', '還跟老闆娘溝通半天', '最後她說用中文講就通了', '烏龍竟忘了這叫buffet', '不過今天去第二次', '就駕輕就熟囉', '順便跟老闆哈啦哈啦', '說我一定會幫他yelp一下']\n",
            "WARNING:root:cannot compute average owing to no vector for ['看落普通甘', '竟然出品又唔错窝', '相对精致d', '份量适中', '味道还算可以', '比较中意距整的龙虾', '不会太油', '吃到龙虾鲜味', '肉质弹牙又甜', '蒸鱔味道可以但有点油', '夏天天气可以坐户外', '看住日落吃晚饭', '别有一番风味', '价钱都算合理']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiAeI07c40qm",
        "outputId": "8b265bea-b439-45a3-98e0-5ada0a47789c"
      },
      "source": [
        "tfidf_doc_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(275197, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcDiiLUNpbBE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "10b2e4e9-09b9-437b-e9cb-b2253639099e"
      },
      "source": [
        "# Save tfidf word averaging doc2vec.\n",
        "print('Shape of tfidf-word-mean doc2vec...')\n",
        "display(tfidf_doc_vec.shape)\n",
        "print('Save tfidf-word-mean doc2vec as csv file...')\n",
        "np.savetxt(os.path.join('./word_embedding/', 'tfidf_doc_vec.csv'), tfidf_doc_vec, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of tfidf-word-mean doc2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(275197, 200)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Save tfidf-word-mean doc2vec as csv file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXiZiMmrp5uF"
      },
      "source": [
        "#GloVe\n",
        "CLOSED - pretrained GloVe data file is not available for Yelp dataset. Cannot find tutorials for converting to GloVe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfXUVCCnp37P"
      },
      "source": [
        "from gensim.test.utils import get_tmpfile, datapath\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "\n",
        "\n",
        "# Load in GloVe vector.\n",
        "glove_vec_fi = datapath('/content/drive/MyDrive/CA683_Assignment/YelpDataset/102442_related/glove.twitter.27B.200d.txt')\n",
        "tmp_word2vec_fi = get_tmpfile('tmp_glove2word2vec.txt')\n",
        "\n",
        "glove2word2vec(glove_vec_fi, tmp_word2vec_fi)\n",
        "\n",
        "glove_word_model = KeyedVectors.load_word2vec_format(tmp_word2vec_fi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27vdsfvDx67J"
      },
      "source": [
        "class MeanEmbeddingVectorizerGlove(object):\n",
        "\n",
        "\n",
        "\tdef __init__(self, glove_word_model):\n",
        "\t\tself.glove_word_model = glove_word_model\n",
        "\t\tself.vector_size = glove_word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
        "\t\treturn self\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.glove_word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.glove_word_model.wv.get_vector(word))\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IuQj3gxZ8DJH",
        "outputId": "a629c1db-d15a-4ec5-b3b8-ce32c553e7ab"
      },
      "source": [
        "mean_vec_tr_Glove = MeanEmbeddingVectorizerGlove(glove_word_model)\n",
        "doc_vec_Glove = mean_vec_tr_Glove.transform(all_docs.doc_words)\n",
        "\n",
        "print('Demo of word averaging doc vector...')\n",
        "display(doc_vec_Glove[4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家我只想说真的很差', '什么玩意啊', '菜一般不说还巨贵', '服务态度巨差无比', '四个人吃饭水都不给我们喝', '太过分了', '盘子也不给我们清理', '以后再也不想来这里吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前段時間易手裝修後', '店內環境走年輕風格', '誇張的日本手繪海產圖案', '特別搶眼', '雪櫃內有鮮即食生蠔', '魚刺身等', '除了主打party', 'tray外', '也有單點壽司同日式刺身蓋飯']\n",
            "WARNING:root:cannot compute average owing to no vector for ['一想到巴黎的著名地標', '就是艾菲爾鐵塔', '凱旋門', '拉斯維加斯有一間飯店就是這樣啦', '飯店的門口就有鐵塔', '凱旋門喔', '不要說飯店外的建築', '天花板就是外面的天空', '會令人感受不出是在室內喔', '如果跟閃光們走在這', '很有甜蜜蜜的感覺喔']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Demo of word averaging doc vector...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-3.36566567e-01, -1.11159466e-01,  1.77930407e-02,  1.02578469e-01,\n",
              "        1.28990591e-01,  1.90640658e-01,  5.73259413e-01, -5.35635948e-02,\n",
              "       -2.61349324e-02, -2.10969269e-01, -1.07924724e-02, -7.87606016e-02,\n",
              "       -4.86807376e-01,  4.09824625e-02, -3.72564942e-01,  2.20287666e-01,\n",
              "        4.27749865e-02,  4.95696627e-02, -2.86673814e-01, -4.95376699e-02,\n",
              "       -2.89237231e-01,  7.31457919e-02, -1.47759393e-01, -8.19321489e-04,\n",
              "        1.18219942e-01,  6.85406089e-01,  6.41084015e-02,  1.77990809e-01,\n",
              "        9.39447656e-02, -9.43731591e-02, -5.30000543e-04,  1.60350017e-02,\n",
              "        1.12101942e-01, -1.38113603e-01, -2.05214038e-01, -1.66838542e-01,\n",
              "        3.24341282e-02, -4.01958041e-02,  1.09357357e-01,  2.86124021e-01,\n",
              "        9.71353352e-02,  7.64728815e-04,  2.05038022e-02, -2.60471880e-01,\n",
              "        2.23166659e-01, -1.53897271e-01,  2.72532284e-01, -6.57266527e-02,\n",
              "        1.44903868e-01,  1.51310107e-02, -3.65707949e-02, -3.26788351e-02,\n",
              "       -1.10615902e-01, -3.75949293e-02,  1.99636705e-02,  6.02371320e-02,\n",
              "        1.65376961e-01,  9.00814682e-02, -1.70770913e-01, -1.61662906e-01,\n",
              "       -9.68333483e-02, -3.19881961e-02, -8.70441943e-02,  1.37535468e-01,\n",
              "        1.62053064e-01, -1.45695224e-01, -1.59593427e-03, -1.53378686e-02,\n",
              "        1.16941899e-01, -5.59400208e-03, -5.98625913e-02, -1.74436659e-01,\n",
              "        1.11304678e-01,  6.28946722e-02,  1.47909448e-01,  9.86466408e-02,\n",
              "       -4.27642688e-02, -1.83182955e-01,  7.21570253e-02,  2.17007190e-01,\n",
              "        1.88524991e-01, -1.18651919e-01,  1.77937418e-01, -2.00412020e-01,\n",
              "        1.42436475e-01,  9.66395363e-02, -9.95661318e-02, -4.15997714e-01,\n",
              "        1.08280184e-03,  8.19133222e-02, -5.60171343e-02,  1.89421490e-01,\n",
              "       -1.63100697e-02, -1.51139900e-01,  1.76488861e-01,  2.22987711e-01,\n",
              "       -1.35749336e-02, -1.16858929e-01, -6.32347316e-02,  1.39870703e-01,\n",
              "        7.60186538e-02,  7.14677945e-02,  1.29534379e-01,  1.54541746e-01,\n",
              "        2.75229979e-02, -1.23037532e-01,  7.36668035e-02,  5.07400669e-02,\n",
              "        1.09321110e-01,  5.24417385e-02, -1.54173806e-01, -6.56853467e-02,\n",
              "        9.47365314e-02, -9.26187076e-03, -7.93664008e-02, -1.96454257e-01,\n",
              "        6.73116073e-02, -9.94434729e-02,  1.43453017e-01,  1.26208309e-02,\n",
              "       -1.16221875e-01,  1.45492658e-01,  2.68303249e-02, -7.59974197e-02,\n",
              "        7.30498061e-02,  2.45743990e-03,  5.16857989e-02,  8.16403329e-02,\n",
              "        3.85546461e-02, -1.26699343e-01,  8.59837979e-02, -7.54781663e-02,\n",
              "       -6.42926842e-02, -5.13475314e-02, -3.23213249e-01, -1.98259667e-01,\n",
              "        1.20498665e-01,  1.79323643e-01,  5.77795990e-02, -7.92135776e-04,\n",
              "       -1.49346113e-01,  1.55273661e-01,  8.22749287e-02, -5.06134331e-03,\n",
              "       -4.45524715e-02,  7.00710788e-02,  1.04251139e-01, -3.23697366e-02,\n",
              "        2.75888801e-01, -4.59133983e-02,  2.32435986e-01,  8.42813328e-02,\n",
              "       -3.55175376e+00,  1.65210143e-01,  2.36310542e-01, -1.44198760e-01,\n",
              "       -1.06700271e-01, -2.08643138e-01,  1.92537270e-02,  1.17211208e-01,\n",
              "       -1.58745199e-01,  8.47258512e-03,  1.30820125e-01,  1.45051345e-01,\n",
              "        1.48023188e-01,  1.41647726e-01, -3.08514088e-01,  5.68264015e-02,\n",
              "       -2.82049971e-03, -3.66039947e-02, -5.40444627e-02,  8.33326951e-03,\n",
              "       -9.50628668e-02, -8.48819688e-03,  4.79186792e-03, -2.04600245e-01,\n",
              "       -2.34413892e-01,  8.89676064e-02,  5.66203631e-02, -5.51320687e-02,\n",
              "       -4.55733627e-01, -1.25791654e-01, -6.97399899e-02, -2.47896668e-02,\n",
              "       -4.01441194e-02, -1.21186301e-01,  3.32756005e-02,  4.24094982e-02,\n",
              "        1.11752763e-01,  1.69618398e-01,  2.75915992e-02,  4.04022597e-02,\n",
              "        1.31173998e-01, -1.51224002e-01,  9.61040631e-02, -1.08312853e-01,\n",
              "       -3.73377316e-02,  1.60419524e-01,  1.40008256e-01,  3.11003346e-02])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "5NXwaX3g5RBW",
        "outputId": "464cfb5e-b0c1-4ede-ac83-546439df6f86"
      },
      "source": [
        "print('Shape of word-mean doc2vec Glove...')\n",
        "display(doc_vec_Glove.shape)\n",
        "#print('Save word-mean doc2vec Glove as csv file...')\n",
        "#np.savetxt(os.path.join(dir_path,'doc_vec.csv'), doc_vec, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of word-mean doc2vec Glove...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(102442, 200)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQNaFj3t5ZaQ"
      },
      "source": [
        "np.savetxt(os.path.join('./','doc_vec_Glove.csv'), doc_vec_Glove, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLSfgx95Z4g"
      },
      "source": [
        "class TfidfEmbeddingVectorizerGlove(object):\n",
        "\n",
        "\tdef __init__(self, glove_word_model):\n",
        "\n",
        "\t\tself.glove_word_model = glove_word_model\n",
        "\t\tself.word_idf_weight = None\n",
        "\t\tself.vector_size = glove_word_model.wv.vector_size\n",
        "\n",
        "\tdef fit(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\t\"\"\"\n",
        "\t\tFit in a list of docs, which had been preprocessed and tokenized,\n",
        "\t\tsuch as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
        "\t\tThen build up a tfidf model to compute each word's idf as its weight.\n",
        "\t\tNoted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
        "\t\t:param\n",
        "\t\t\tpre_processed_docs: list of docs, which are tokenized\n",
        "\t\t:return:\n",
        "\t\t\tself\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\ttext_docs = []\n",
        "\t\tfor doc in docs:\n",
        "\t\t\ttext_docs.append(\" \".join(doc))\n",
        "\n",
        "\t\ttfidf = TfidfVectorizer()\n",
        "\t\ttfidf.fit(text_docs)  # must be list of text string\n",
        "\n",
        "\t\t# if a word was never seen - it must be at least as infrequent\n",
        "\t\t# as any of the known words - so the default idf is the max of\n",
        "\t\t# known idf's\n",
        "\t\tmax_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
        "\t\tself.word_idf_weight = defaultdict(lambda: max_idf,\n",
        "\t\t\t\t\t\t\t\t\t\t   [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
        "\t\treturn self\n",
        "\n",
        "\n",
        "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "\t\tdoc_word_vector = self.word_average_list(docs)\n",
        "\t\treturn doc_word_vector\n",
        "\n",
        "\n",
        "\tdef word_average(self, sent):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for a single doc/sentence.\n",
        "\t\t:param sent: list of sentence tokens\n",
        "\t\t:return:\n",
        "\t\t\tmean: float of averaging word vectors\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tmean = []\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word in self.glove_word_model.wv.vocab:\n",
        "\t\t\t\tmean.append(self.glove_word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
        "\n",
        "\t\tif not mean:  # empty words\n",
        "\t\t\t# If a text is empty, return a vector of zeros.\n",
        "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "\t\t\treturn np.zeros(self.vector_size)\n",
        "\t\telse:\n",
        "\t\t\tmean = np.array(mean).mean(axis=0)\n",
        "\t\t\treturn mean\n",
        "\n",
        "\n",
        "\tdef word_average_list(self, docs):\n",
        "\t\t\"\"\"\n",
        "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
        "\t\t:param docs: list of sentence in list of separated tokens\n",
        "\t\t:return:\n",
        "\t\t\tarray of average word vector in shape (len(docs),)\n",
        "\t\t\"\"\"\n",
        "\t\treturn np.vstack([self.word_average(sent) for sent in docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgcupzO-5g_B",
        "outputId": "5bb03860-6b63-4e9a-8ca9-507636d5cd2e"
      },
      "source": [
        "tfidf_vec_tr_Glove = TfidfEmbeddingVectorizerGlove(glove_word_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7doqUwR5lLl",
        "outputId": "e5f5aa80-c9cf-4193-eb04-fd1d35c49558"
      },
      "source": [
        "tfidf_vec_tr_Glove.fit(all_docs.doc_words)  # fit tfidf model first\n",
        "tfidf_doc_vec_Glove = tfidf_vec_tr_Glove.transform(all_docs.doc_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "WARNING:root:cannot compute average owing to no vector for ['这家我只想说真的很差', '什么玩意啊', '菜一般不说还巨贵', '服务态度巨差无比', '四个人吃饭水都不给我们喝', '太过分了', '盘子也不给我们清理', '以后再也不想来这里吃']\n",
            "WARNING:root:cannot compute average owing to no vector for ['前段時間易手裝修後', '店內環境走年輕風格', '誇張的日本手繪海產圖案', '特別搶眼', '雪櫃內有鮮即食生蠔', '魚刺身等', '除了主打party', 'tray外', '也有單點壽司同日式刺身蓋飯']\n",
            "WARNING:root:cannot compute average owing to no vector for ['一想到巴黎的著名地標', '就是艾菲爾鐵塔', '凱旋門', '拉斯維加斯有一間飯店就是這樣啦', '飯店的門口就有鐵塔', '凱旋門喔', '不要說飯店外的建築', '天花板就是外面的天空', '會令人感受不出是在室內喔', '如果跟閃光們走在這', '很有甜蜜蜜的感覺喔']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIDNBmf85p3w",
        "outputId": "f4c18c64-c120-4c73-8b42-2329a9c3e761"
      },
      "source": [
        "tfidf_doc_vec_Glove.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102442, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "YBzoi43Z5uMQ",
        "outputId": "cb41e797-d5ce-40ee-9b9c-d38ac2643298"
      },
      "source": [
        "# Save tfidf word averaging doc2vec.\n",
        "print('Shape of tfidf-word-mean doc2vec...')\n",
        "display(tfidf_doc_vec_Glove.shape)\n",
        "print('Save tfidf-word-mean doc2vec as csv file...')\n",
        "np.savetxt(os.path.join('./', 'tfidf_doc_vec_Glove.csv'), tfidf_doc_vec_Glove, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of tfidf-word-mean doc2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(102442, 200)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Save tfidf-word-mean doc2vec as csv file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whmlw1fjHkAf"
      },
      "source": [
        "# Docvec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk7CjvdNHnjm"
      },
      "source": [
        "class DocModel(object):\n",
        "\n",
        "\tdef __init__(self, docs, **kwargs):\n",
        "\t\t\"\"\"\n",
        "\t\t:param docs: list of TaggedDocument\n",
        "\t\t:param kwargs: dictionary of (key,value) for Doc2Vec arguments\n",
        "\t\t\"\"\"\n",
        "\t\tself.model = Doc2Vec(**kwargs)\n",
        "\t\tself.docs = docs\n",
        "\t\tself.model.build_vocab([x for x in self.docs])\n",
        "\n",
        "\tdef custom_train(self, fixed_lr=False, fixed_lr_epochs=None):\n",
        "\t\t\"\"\"\n",
        "\t\tTrain Doc2Vec with two options, without fixed learning rate(recommended) or with fixed learning rate.\n",
        "\t\tFixed learning rate also includes implementation of shuffling training dataset.\n",
        "\t\t:param fixed_lr: boolean\n",
        "\t\t:param fixed_lr_epochs: num of epochs for fixed lr training\n",
        "\t\t\"\"\"\n",
        "\t\tif not fixed_lr:\n",
        "\t\t\tself.model.train([x for x in self.docs],\n",
        "\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
        "\t\t\t\t\t\t\t epochs=self.model.epochs)\n",
        "\t\telse:\n",
        "\t\t\tfor _ in range(fixed_lr_epochs):\n",
        "\t\t\t\tself.model.train(utils.shuffle([x for x in self.docs]),\n",
        "\t\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
        "\t\t\t\t\t\t\t\t epochs=1)\n",
        "\t\t\t\tself.model.alpha -= 0.002\n",
        "\t\t\t\tself.model.min_alpha = self.model.alpha  # fixed learning rate\n",
        "\n",
        "\n",
        "\tdef test_orig_doc_infer(self):\n",
        "\t\t\"\"\"\n",
        "\t\tUse the original doc as input for model's vector inference,\n",
        "\t\tand then compare using most_similar()\n",
        "\t\tto see if model finds the original doc id be the most similar doc to the input.\n",
        "\t\t\"\"\"\n",
        "\t\tidx = np.random.randint(len(self.docs))\n",
        "\t\tprint('idx: ' + str(idx))\n",
        "\t\tdoc = [doc for doc in self.docs if doc.tags[0] == idx]\n",
        "\t\tinferred_vec = self.model.infer_vector(doc[0].words)\n",
        "\t\tprint(self.model.docvecs.most_similar([inferred_vec]))  # wrap vec in a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtV3wl-1R9aG"
      },
      "source": [
        "dm ({1,0}, optional) – Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
        "negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
        "hs ({1,0}, optional) – If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
        "sample (float, optional) – The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
        "alpha (float, optional) – The initial learning rate.\n",
        "\n",
        "min_alpha (float, optional) – Learning rate will linearly drop to min_alpha as training progresses.\n",
        "epochs (int, optional) – Number of iterations (epochs) over the corpus. Defaults to 10 for Doc2Vec.\n",
        "https://radimrehurek.com/gensim/models/doc2vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK91RiJsHrvO"
      },
      "source": [
        "dm_args = {\n",
        "    'dm': 1,\n",
        "    'dm_mean': 1,\n",
        "    'vector_size': 100,\n",
        "    'window': 5,\n",
        "    'negative': 5,\n",
        "    'hs': 0,\n",
        "    'min_count': 5,\n",
        "    'sample': 0,\n",
        "    'workers': workers,\n",
        "    'alpha': 0.025,\n",
        "    'min_alpha': 0.025,\n",
        "    'epochs': 100,\n",
        "    'comment': 'alpha=0.025'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8NA4risHym4"
      },
      "source": [
        "dm = DocModel(docs=all_docs.tagdocs, **dm_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QB6GdXhH381"
      },
      "source": [
        "dm.custom_train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "MzQLpRSBH8Cu",
        "outputId": "32c5d596-7fa7-44b7-b511-8f5be72c1e52"
      },
      "source": [
        "# Save doc2vec as feature dataframe.\n",
        "dm_doc_vec_ls = []\n",
        "for i in range(len(dm.model.docvecs)):\n",
        "    dm_doc_vec_ls.append(dm.model.docvecs[i])\n",
        "\n",
        "\n",
        "dm_doc_vec = pd.DataFrame(dm_doc_vec_ls)\n",
        "print('Shape of dm doc2vec...')\n",
        "display(dm_doc_vec.shape)\n",
        "\n",
        "print('Save dm doc2vec as csv file...')\n",
        "dm_doc_vec.to_csv(os.path.join('./word_embedding/', 'dm_doc_vec.csv'), index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of dm doc2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(275197, 100)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Save dm doc2vec as csv file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vlRz46K3IGxk",
        "outputId": "dcf95156-a388-4fb4-ed7d-b2d3a2c5b518"
      },
      "source": [
        "\n",
        "print('Shape of target labels...')\n",
        "display(all_docs.labels.shape)\n",
        "target_labels = all_docs.labels\n",
        "\n",
        "print('Save target labels...')\n",
        "target_labels.to_csv(os.path.join('./word_embedding/', 'target_labels.csv'), index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of target labels...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(275197,)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Save target labels...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JHx9DdeIRQp"
      },
      "source": [
        "#Classification Models\n",
        "SGDClassifier or Logistic Regression applied on \n",
        "\n",
        "Tf-Idf Weighted Averaging Word Vector\n",
        "PV-DM Doc2vec\n",
        "Tf-Idf and Doc2vec Concatenated Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i92dhm_EQ3D_"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcJReJVkISlS"
      },
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Read in saved files.\n",
        "\n",
        "\n",
        "doc_vec = pd.read_csv(os.path.join('./word_embedding/', 'doc_vec.csv'), header=None)\n",
        "tfidf_doc_vec = pd.read_csv(os.path.join('./word_embedding/', 'tfidf_doc_vec.csv'), header=None)\n",
        "#doc_vec_Glove = pd.read_csv(os.path.join('./word_embedding/', 'doc_vec_Glove.csv'), header=None)\n",
        "#tfidf_doc_vec_Glove = pd.read_csv(os.path.join('./word_embedding/', 'tfidf_doc_vec_Glove.csv'), header=None)\n",
        "dm_doc_vec = pd.read_csv(os.path.join('./word_embedding/', 'dm_doc_vec.csv'), header=None)\n",
        "target_labels = pd.read_csv(os.path.join('./word_embedding/', 'target_labels.csv'), header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBGxfYmxMPE6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVd-3wpLI8OK"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Classification via Logistic Model\n",
        "logistic = LogisticRegression(random_state=1, multi_class='multinomial', solver='saga')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEzLyYdcKeoh"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# (Optional) Classification via stochastic gradient descent classifier.\n",
        "sgd = SGDClassifier(loss='hinge',\n",
        "                    verbose=1,\n",
        "                    random_state=1,\n",
        "                    learning_rate='invscaling',\n",
        "                    eta0=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0rfD5RSd21U"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HdczUgXLBFR"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "def split_size(df, train=0.8, valid=0.1):\n",
        "    train_size = math.floor(len(df) * train)\n",
        "    valid_size = math.floor(len(df) * valid)\n",
        "    test_size = len(df) - train_size - valid_size\n",
        "    return train_size, valid_size, test_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PMNm3P-Ku1f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(model, df, concate, concat_df):\n",
        "    if concate:\n",
        "        df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
        "    else:\n",
        "        df = df\n",
        "\n",
        "    # Specify train/valid/test size.\n",
        "    train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.1)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "    train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)\n",
        "\n",
        "    # Prepare valid dataset.\n",
        "    if valid_size != 0:\n",
        "        train_X, valid_X, train_y, valid_y = train_test_split(train_X,\n",
        "                                                      train_y,\n",
        "                                                      test_size=valid_size,\n",
        "                                                      random_state=1,\n",
        "                                                      stratify=train_y)\n",
        "    \n",
        "    print('Shape of train_X: {}'.format(train_X.shape))\n",
        "    print('Shape of valid_X: {}'.format(valid_X.shape if 'valid_X' in vars() else (0,0)))\n",
        "    print('Shape of text_X: {}'.format(test_X.shape))\n",
        "    \n",
        "    model.fit(train_X, train_y)\n",
        "    \n",
        "    if valid_size != 0:\n",
        "        return model, train_X, valid_X, test_X, train_y, valid_y, test_y\n",
        "    else:\n",
        "        return model, train_X, None, test_X, train_y, None, test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evHFtGsgQj27"
      },
      "source": [
        "## Simple Averaging Word Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c92L01ILP_x"
      },
      "source": [
        "\n",
        "model = RandomForestClassifier()  # or choose sgd.\n",
        "df = doc_vec\n",
        "concate = False\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLMGsJlgLTF3",
        "outputId": "b8513f1a-8f88-4f02-e3b5-d34baa8cbc95"
      },
      "source": [
        "\n",
        "# __main__\n",
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (220157, 200)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (55040, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnZ6BALELa-o"
      },
      "source": [
        "def sk_evaluate(model, feature, label, label_names):\n",
        "    pred = model.predict(feature)\n",
        "    true = np.array(label)\n",
        "\n",
        "    print('Score on dataset...\\n')\n",
        "    print('Confusion Matrix:\\n', confusion_matrix(true, pred))\n",
        "    print('\\nClassification Report:\\n', classification_report(true, pred, target_names=label_names))\n",
        "    print('\\naccuracy: {:.3f}'.format(accuracy_score(true, pred)))\n",
        "    print('f1 score: {:.3f}'.format(f1_score(true, pred, average='weighted')))\n",
        "\n",
        "    return pred, true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7U4hgtLLXt3",
        "outputId": "93b478bb-a250-4db5-a7e1-02dc4f98ff9f"
      },
      "source": [
        "print('Performance of Mean Word Vector on training dataset...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Mean Word Vector on training dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 16906      0      0      0     27]\n",
            " [     4   9572      1      3      7]\n",
            " [     1      1  16322      6     44]\n",
            " [     0      0      5  46704     66]\n",
            " [     0      0      2      3 130483]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      1.00      1.00     16933\n",
            "         2.0       1.00      1.00      1.00      9587\n",
            "         3.0       1.00      1.00      1.00     16374\n",
            "         4.0       1.00      1.00      1.00     46775\n",
            "         5.0       1.00      1.00      1.00    130488\n",
            "\n",
            "    accuracy                           1.00    220157\n",
            "   macro avg       1.00      1.00      1.00    220157\n",
            "weighted avg       1.00      1.00      1.00    220157\n",
            "\n",
            "\n",
            "accuracy: 0.999\n",
            "f1 score: 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcwHWSgNnDy-",
        "outputId": "b39d8c7e-a891-4207-c7d8-227daa34d225"
      },
      "source": [
        "print('Performance of Mean Word Vector on testing dataset...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Mean Word Vector on testing dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2946    71   145   181   891]\n",
            " [  943   107   298   333   716]\n",
            " [  539    92   520   854  2088]\n",
            " [  230    19   206  1399  9840]\n",
            " [  213     8    50   816 31535]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.60      0.70      0.65      4234\n",
            "         2.0       0.36      0.04      0.08      2397\n",
            "         3.0       0.43      0.13      0.20      4093\n",
            "         4.0       0.39      0.12      0.18     11694\n",
            "         5.0       0.70      0.97      0.81     32622\n",
            "\n",
            "    accuracy                           0.66     55040\n",
            "   macro avg       0.50      0.39      0.38     55040\n",
            "weighted avg       0.59      0.66      0.59     55040\n",
            "\n",
            "\n",
            "accuracy: 0.663\n",
            "f1 score: 0.588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXNS8-PUMOOw"
      },
      "source": [
        "##Tf-Idf Weighted Averaging Word Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMbx20VoMVdJ"
      },
      "source": [
        "model = RandomForestClassifier()  # or choose sgd.\n",
        "df = tfidf_doc_vec\n",
        "concate = False\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyLTA0WbMYdw",
        "outputId": "da8eac07-5e9a-48f5-d374-e96ca39e31a2"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (220157, 200)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (55040, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPuqxAEkMbsQ",
        "outputId": "a36ba5aa-beda-46e1-df98-d5d6a5074db2"
      },
      "source": [
        "print('Performance of Tf-Idf Mean Word Vector on training dataset...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector on training dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 16906      0      0      0     27]\n",
            " [     4   9572      1      3      7]\n",
            " [     1      1  16322      6     44]\n",
            " [     0      0      5  46704     66]\n",
            " [     0      0      2      3 130483]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      1.00      1.00     16933\n",
            "         2.0       1.00      1.00      1.00      9587\n",
            "         3.0       1.00      1.00      1.00     16374\n",
            "         4.0       1.00      1.00      1.00     46775\n",
            "         5.0       1.00      1.00      1.00    130488\n",
            "\n",
            "    accuracy                           1.00    220157\n",
            "   macro avg       1.00      1.00      1.00    220157\n",
            "weighted avg       1.00      1.00      1.00    220157\n",
            "\n",
            "\n",
            "accuracy: 0.999\n",
            "f1 score: 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al0GWJx5MjSw",
        "outputId": "202dfc78-5491-4b76-edef-cf03ca478908"
      },
      "source": [
        "#test on testing data\n",
        "print('Performance of Tf-Idf Mean Word Vector on testing dataset...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector on testing dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2946    71   145   181   891]\n",
            " [  943   107   298   333   716]\n",
            " [  539    92   520   854  2088]\n",
            " [  230    19   206  1399  9840]\n",
            " [  213     8    50   816 31535]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.60      0.70      0.65      4234\n",
            "         2.0       0.36      0.04      0.08      2397\n",
            "         3.0       0.43      0.13      0.20      4093\n",
            "         4.0       0.39      0.12      0.18     11694\n",
            "         5.0       0.70      0.97      0.81     32622\n",
            "\n",
            "    accuracy                           0.66     55040\n",
            "   macro avg       0.50      0.39      0.38     55040\n",
            "weighted avg       0.59      0.66      0.59     55040\n",
            "\n",
            "\n",
            "accuracy: 0.663\n",
            "f1 score: 0.588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C85wN_6pMuEi"
      },
      "source": [
        "##PV-DM Doc2vec "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "027Q4N7sMo_Q"
      },
      "source": [
        "model = RandomForestClassifier()  # or choose sgd.\n",
        "df = dm_doc_vec\n",
        "concate = False\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSUucWVHMx_T",
        "outputId": "e246dba5-23fb-4e09-c1c4-80d097781864"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (220157, 100)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (55040, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3ut6LzzM2VK",
        "outputId": "18650fe4-4d08-43c0-a4e7-563940a5c341"
      },
      "source": [
        "print('Performance of Doc2vec on training dataset...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Doc2vec on training dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 16933      0      0      0      0]\n",
            " [     0   9587      0      0      0]\n",
            " [     0      0  16374      0      0]\n",
            " [     0      0      0  46775      0]\n",
            " [     0      0      0      0 130488]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      1.00      1.00     16933\n",
            "         2.0       1.00      1.00      1.00      9587\n",
            "         3.0       1.00      1.00      1.00     16374\n",
            "         4.0       1.00      1.00      1.00     46775\n",
            "         5.0       1.00      1.00      1.00    130488\n",
            "\n",
            "    accuracy                           1.00    220157\n",
            "   macro avg       1.00      1.00      1.00    220157\n",
            "weighted avg       1.00      1.00      1.00    220157\n",
            "\n",
            "\n",
            "accuracy: 1.000\n",
            "f1 score: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpVhFhZQoSTn",
        "outputId": "67c71ea5-dc1d-4e0d-e9a8-fbfd9f882cdb"
      },
      "source": [
        "print('Performance of Doc2vec on testing dataset...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Doc2vec on testing dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  179     0     5    83  3967]\n",
            " [   56     2    14   105  2220]\n",
            " [   31     0    23   248  3791]\n",
            " [    6     0     7   303 11378]\n",
            " [    4     0     1   224 32393]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.65      0.04      0.08      4234\n",
            "         2.0       1.00      0.00      0.00      2397\n",
            "         3.0       0.46      0.01      0.01      4093\n",
            "         4.0       0.31      0.03      0.05     11694\n",
            "         5.0       0.60      0.99      0.75     32622\n",
            "\n",
            "    accuracy                           0.60     55040\n",
            "   macro avg       0.61      0.21      0.18     55040\n",
            "weighted avg       0.55      0.60      0.46     55040\n",
            "\n",
            "\n",
            "accuracy: 0.598\n",
            "f1 score: 0.462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwZENTtVM9Ua"
      },
      "source": [
        "##Tf-Idf and Doc2vec Concatenated Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfMRSgKMgJfs"
      },
      "source": [
        "###logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYVLNQY4M-Si"
      },
      "source": [
        "model = logistic  # or choose sgd.\n",
        "df = tfidf_doc_vec\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6-cXWshNAgw",
        "outputId": "9d24f7cd-db72-46fd-9a55-b08667ea1f49"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (220157, 300)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (55040, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwsR-r9QNETQ",
        "outputId": "5ed5fd4e-c220-4e8a-e7db-0d069035b5c4"
      },
      "source": [
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 12947   1059    941    424   1562]\n",
            " [  3577   1641   2151    823   1395]\n",
            " [  1767   1012   4978   4099   4518]\n",
            " [   782    223   2490  11024  32256]\n",
            " [   877    117    793   6842 121859]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.65      0.76      0.70     16933\n",
            "         2.0       0.40      0.17      0.24      9587\n",
            "         3.0       0.44      0.30      0.36     16374\n",
            "         4.0       0.47      0.24      0.32     46775\n",
            "         5.0       0.75      0.93      0.83    130488\n",
            "\n",
            "    accuracy                           0.69    220157\n",
            "   macro avg       0.54      0.48      0.49    220157\n",
            "weighted avg       0.65      0.69      0.65    220157\n",
            "\n",
            "\n",
            "accuracy: 0.692\n",
            "f1 score: 0.653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3cuUohQgf_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25c1ca4-f0f1-4df5-dd98-fc997a9353d6"
      },
      "source": [
        "#test on testing data\n",
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using logistic ...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using logistic ...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 3187   264   245   115   423]\n",
            " [  928   384   553   210   322]\n",
            " [  470   259  1202  1004  1158]\n",
            " [  203    43   562  2750  8136]\n",
            " [  212    26   201  1636 30547]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.64      0.75      0.69      4234\n",
            "         2.0       0.39      0.16      0.23      2397\n",
            "         3.0       0.44      0.29      0.35      4093\n",
            "         4.0       0.48      0.24      0.32     11694\n",
            "         5.0       0.75      0.94      0.83     32622\n",
            "\n",
            "    accuracy                           0.69     55040\n",
            "   macro avg       0.54      0.48      0.48     55040\n",
            "weighted avg       0.65      0.69      0.65     55040\n",
            "\n",
            "\n",
            "accuracy: 0.692\n",
            "f1 score: 0.651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1v-ufbYgPaM"
      },
      "source": [
        "### decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28paoB1ceHsj"
      },
      "source": [
        "model = DecisionTreeClassifier()  # or choose sgd.\n",
        "df = tfidf_doc_vec\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqdgycSUeLw8",
        "outputId": "fa90f996-ab3f-42b6-ab4e-52123b298064"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (81953, 300)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (20489, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxZzgnkLeMfk",
        "outputId": "2f462284-1b9b-40ff-f33e-0f2818a6ce19"
      },
      "source": [
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset using decision tree...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset using decision tree...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10908     0     0     0     0]\n",
            " [    0  5218     0     0     0]\n",
            " [    0     0  6198     0     0]\n",
            " [    0     0     0 16393     0]\n",
            " [    0     0     0     0 43236]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00     10908\n",
            "           2       1.00      1.00      1.00      5218\n",
            "           3       1.00      1.00      1.00      6198\n",
            "           4       1.00      1.00      1.00     16393\n",
            "           5       1.00      1.00      1.00     43236\n",
            "\n",
            "    accuracy                           1.00     81953\n",
            "   macro avg       1.00      1.00      1.00     81953\n",
            "weighted avg       1.00      1.00      1.00     81953\n",
            "\n",
            "\n",
            "accuracy: 1.000\n",
            "f1 score: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv6mHRHifMmU",
        "outputId": "5f74961f-e3c7-403b-8fe1-4d5c52f60433"
      },
      "source": [
        "#test on testing data\n",
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using decision tree...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using decision tree...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1356  435  274  263  399]\n",
            " [ 414  241  204  199  247]\n",
            " [ 266  200  234  380  469]\n",
            " [ 218  222  337 1074 2248]\n",
            " [ 406  339  513 2268 7283]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.51      0.50      0.50      2727\n",
            "           2       0.17      0.18      0.18      1305\n",
            "           3       0.15      0.15      0.15      1549\n",
            "           4       0.26      0.26      0.26      4099\n",
            "           5       0.68      0.67      0.68     10809\n",
            "\n",
            "    accuracy                           0.50     20489\n",
            "   macro avg       0.35      0.35      0.35     20489\n",
            "weighted avg       0.50      0.50      0.50     20489\n",
            "\n",
            "\n",
            "accuracy: 0.497\n",
            "f1 score: 0.500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGWPpFgVSee5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyElR-b4Sjpq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP7zcadUgTvL"
      },
      "source": [
        "### random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQlJjMoYehqH"
      },
      "source": [
        "model = RandomForestClassifier()  # or choose sgd.\n",
        "df = tfidf_doc_vec\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pfd3klweidz",
        "outputId": "2ef27907-ad35-4e3c-c884-52ce52dc4306"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (81953, 300)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (20489, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsAybtJCelEb",
        "outputId": "7be50cf7-1809-4bf1-e221-b3fcf9f22df1"
      },
      "source": [
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset using RandomForestClassifier ...')\n",
        "_, _ = sk_evaluate(clf, train_X, train_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on training dataset using RandomForestClassifier ...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10908     0     0     0     0]\n",
            " [    0  5218     0     0     0]\n",
            " [    0     0  6198     0     0]\n",
            " [    0     0     0 16393     0]\n",
            " [    0     0     0     0 43236]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00     10908\n",
            "           2       1.00      1.00      1.00      5218\n",
            "           3       1.00      1.00      1.00      6198\n",
            "           4       1.00      1.00      1.00     16393\n",
            "           5       1.00      1.00      1.00     43236\n",
            "\n",
            "    accuracy                           1.00     81953\n",
            "   macro avg       1.00      1.00      1.00     81953\n",
            "weighted avg       1.00      1.00      1.00     81953\n",
            "\n",
            "\n",
            "accuracy: 1.000\n",
            "f1 score: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRQ0m5fVfl6K",
        "outputId": "d25628d6-a4ba-4a0d-da3c-a133b1d42827"
      },
      "source": [
        "#test on testing data\n",
        "print('Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using RandomForestClassifier ...')\n",
        "_, _ = sk_evaluate(clf, test_X, test_y, label_names=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performance of Tf-Idf Mean Word Vector and Doc2vec Combined on testing dataset using RandomForestClassifier ...\n",
            "Score on dataset...\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 2156    29    17    51   474]\n",
            " [  646    66    40    78   475]\n",
            " [  350    32    82   163   922]\n",
            " [  160    12    17   251  3659]\n",
            " [  169     6     6   130 10498]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.79      0.69      2727\n",
            "           2       0.46      0.05      0.09      1305\n",
            "           3       0.51      0.05      0.10      1549\n",
            "           4       0.37      0.06      0.11      4099\n",
            "           5       0.65      0.97      0.78     10809\n",
            "\n",
            "    accuracy                           0.64     20489\n",
            "   macro avg       0.52      0.39      0.35     20489\n",
            "weighted avg       0.57      0.64      0.54     20489\n",
            "\n",
            "\n",
            "accuracy: 0.637\n",
            "f1 score: 0.539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCk6SN8LStbh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRPIBWmRSoJ3"
      },
      "source": [
        "# DAO_TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M0kTp0-YWNU"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "#import cPickle as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pylab\n",
        "import re\n",
        "import scipy as sp\n",
        "import seaborn\n",
        "\n",
        "from gensim import corpora, models\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.lda import LDA\n",
        "#from sklearn.qda import QDA\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "plt.rc('figure', figsize=(10,6))\n",
        "seaborn.set()\n",
        "colors = seaborn.color_palette()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr7YbzPxb6F9"
      },
      "source": [
        "label_keys =[1, 2, 3, 4, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_fq6gfbTw5z"
      },
      "source": [
        "df = tfidf_doc_vec\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9nTN3oVmOQ"
      },
      "source": [
        "        df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hHrsVmvVoap"
      },
      "source": [
        "   # Specify train/valid/test size.\n",
        "train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q6nVh28SuLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "Ffiyn-TASqkb",
        "outputId": "f2c036f9-4f82-4c70-a8a6-8532c9d80624"
      },
      "source": [
        "clfs = [RandomForestClassifier(), LogisticRegression(),DecisionTreeClassifier()]\n",
        "clf_names = ['Random Forest', 'Logistic Regression','Decision Tree']\n",
        "\n",
        "NBResults = {}\n",
        "for (i, clf_) in enumerate(clfs):\n",
        "    clf = clf_.fit(train_X, train_y)\n",
        "    preds = clf.predict(test_X)\n",
        "    \n",
        "    precision = metrics.precision_score(test_y, preds,average ='micro')\n",
        "    recall = metrics.recall_score(test_y, preds,average ='micro')\n",
        "    f1 = metrics.f1_score(test_y, preds,average ='micro')\n",
        "    accuracy = accuracy_score(test_y, preds)\n",
        "    report = classification_report(test_y, preds)\n",
        "    matrix = metrics.confusion_matrix(test_y, preds, labels=label_keys)\n",
        "    \n",
        "    data = {'precision':precision,\n",
        "            'recall':recall,\n",
        "            'f1_score':f1,\n",
        "            'accuracy':accuracy,\n",
        "            'clf_report':report,\n",
        "            'clf_matrix':matrix,\n",
        "            'y_predicted':preds}\n",
        "    \n",
        "    NBResults[clf_names[i]] = data\n",
        "\n",
        "cols = ['precision', 'recall', 'f1_score', 'accuracy']\n",
        "pd.DataFrame(NBResults).T[cols].T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Decision Tree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.635707</td>\n",
              "      <td>0.669872</td>\n",
              "      <td>0.498902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.635707</td>\n",
              "      <td>0.669872</td>\n",
              "      <td>0.498902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.635707</td>\n",
              "      <td>0.669872</td>\n",
              "      <td>0.498902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.635707</td>\n",
              "      <td>0.669872</td>\n",
              "      <td>0.498902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Random Forest Logistic Regression Decision Tree\n",
              "precision      0.635707            0.669872      0.498902\n",
              "recall         0.635707            0.669872      0.498902\n",
              "f1_score       0.635707            0.669872      0.498902\n",
              "accuracy       0.635707            0.669872      0.498902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWMV5oryeP6c",
        "outputId": "ef7eaecf-afe6-43a8-ebe1-679907de1f9d"
      },
      "source": [
        "for model, val in NBResults.items():\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('MODEL:', model)\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('The precision for this classifier is ' + str(val['precision']))\n",
        "    print ('The recall for this classifier is    ' + str(val['recall']))\n",
        "    print ('The f1 for this classifier is        ' + str(val['f1_score']))\n",
        "    print ('The accuracy for this classifier is  ' + str(val['accuracy']))\n",
        "    print ('Here is the classification report:')\n",
        "    print (val['clf_report'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "MODEL: Random Forest\n",
            "--------------------\n",
            "The precision for this classifier is 0.6357069647127727\n",
            "The recall for this classifier is    0.6357069647127727\n",
            "The f1 for this classifier is        0.6357069647127727\n",
            "The accuracy for this classifier is  0.6357069647127727\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.79      0.70      2727\n",
            "           2       0.37      0.04      0.08      1305\n",
            "           3       0.42      0.04      0.08      1549\n",
            "           4       0.37      0.06      0.10      4099\n",
            "           5       0.65      0.97      0.78     10809\n",
            "\n",
            "    accuracy                           0.64     20489\n",
            "   macro avg       0.49      0.38      0.35     20489\n",
            "weighted avg       0.56      0.64      0.54     20489\n",
            "\n",
            "--------------------------\n",
            "MODEL: Logistic Regression\n",
            "--------------------------\n",
            "The precision for this classifier is 0.6698716384401386\n",
            "The recall for this classifier is    0.6698716384401386\n",
            "The f1 for this classifier is        0.6698716384401386\n",
            "The accuracy for this classifier is  0.6698716384401386\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.67      0.80      0.73      2727\n",
            "           2       0.41      0.22      0.28      1305\n",
            "           3       0.39      0.25      0.31      1549\n",
            "           4       0.44      0.20      0.28      4099\n",
            "           5       0.73      0.93      0.82     10809\n",
            "\n",
            "    accuracy                           0.67     20489\n",
            "   macro avg       0.53      0.48      0.48     20489\n",
            "weighted avg       0.62      0.67      0.63     20489\n",
            "\n",
            "--------------------\n",
            "MODEL: Decision Tree\n",
            "--------------------\n",
            "The precision for this classifier is 0.49890184977304897\n",
            "The recall for this classifier is    0.49890184977304897\n",
            "The f1 for this classifier is        0.49890184977304897\n",
            "The accuracy for this classifier is  0.49890184977304897\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.51      0.49      0.50      2727\n",
            "           2       0.17      0.18      0.17      1305\n",
            "           3       0.15      0.15      0.15      1549\n",
            "           4       0.26      0.27      0.27      4099\n",
            "           5       0.69      0.67      0.68     10809\n",
            "\n",
            "    accuracy                           0.50     20489\n",
            "   macro avg       0.36      0.35      0.35     20489\n",
            "weighted avg       0.50      0.50      0.50     20489\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg-oS2ahehje"
      },
      "source": [
        "# DAO_TEST_GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO2bnwOHek4a"
      },
      "source": [
        "df = tfidf_doc_vec_Glove\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3El-dxh4euAU"
      },
      "source": [
        "   df = pd.concat([df, concat_df], axis=1, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx3dJiSQezR4"
      },
      "source": [
        " # Specify train/valid/test size.\n",
        "train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "8LJahzQxe3Ip",
        "outputId": "af66c545-8a80-4a39-e576-344eb61d5968"
      },
      "source": [
        "clfs = [RandomForestClassifier(), LogisticRegression(),DecisionTreeClassifier()]\n",
        "clf_names = ['Random Forest', 'Logistic Regression','Decision Tree']\n",
        "\n",
        "NBResults = {}\n",
        "for (i, clf_) in enumerate(clfs):\n",
        "    clf = clf_.fit(train_X, train_y)\n",
        "    preds = clf.predict(test_X)\n",
        "    \n",
        "    precision = metrics.precision_score(test_y, preds,average ='micro')\n",
        "    recall = metrics.recall_score(test_y, preds,average ='micro')\n",
        "    f1 = metrics.f1_score(test_y, preds,average ='micro')\n",
        "    accuracy = accuracy_score(test_y, preds)\n",
        "    report = classification_report(test_y, preds)\n",
        "    matrix = metrics.confusion_matrix(test_y, preds, labels=label_keys)\n",
        "    \n",
        "    data = {'precision':precision,\n",
        "            'recall':recall,\n",
        "            'f1_score':f1,\n",
        "            'accuracy':accuracy,\n",
        "            'clf_report':report,\n",
        "            'clf_matrix':matrix,\n",
        "            'y_predicted':preds}\n",
        "    \n",
        "    NBResults[clf_names[i]] = data\n",
        "\n",
        "cols = ['precision', 'recall', 'f1_score', 'accuracy']\n",
        "pd.DataFrame(NBResults).T[cols].T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Decision Tree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.605398</td>\n",
              "      <td>0.661965</td>\n",
              "      <td>0.450388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.605398</td>\n",
              "      <td>0.661965</td>\n",
              "      <td>0.450388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.605398</td>\n",
              "      <td>0.661965</td>\n",
              "      <td>0.450388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.605398</td>\n",
              "      <td>0.661965</td>\n",
              "      <td>0.450388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Random Forest Logistic Regression Decision Tree\n",
              "precision      0.605398            0.661965      0.450388\n",
              "recall         0.605398            0.661965      0.450388\n",
              "f1_score       0.605398            0.661965      0.450388\n",
              "accuracy       0.605398            0.661965      0.450388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDrafpJ_kP8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sDzeAQIe4Js",
        "outputId": "9545dcaa-5440-47a9-b254-001292831754"
      },
      "source": [
        "for model, val in NBResults.items():\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('MODEL:', model)\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('The precision for this classifier is ' + str(val['precision']))\n",
        "    print ('The recall for this classifier is    ' + str(val['recall']))\n",
        "    print ('The f1 for this classifier is        ' + str(val['f1_score']))\n",
        "    print ('The accuracy for this classifier is  ' + str(val['accuracy']))\n",
        "    print ('Here is the classification report:')\n",
        "    print (val['clf_report'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "MODEL: Random Forest\n",
            "--------------------\n",
            "The precision for this classifier is 0.6053980184489238\n",
            "The recall for this classifier is    0.6053980184489238\n",
            "The f1 for this classifier is        0.6053980184489238\n",
            "The accuracy for this classifier is  0.6053980184489238\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      0.63      0.63      2727\n",
            "           2       0.38      0.00      0.01      1305\n",
            "           3       0.50      0.00      0.01      1549\n",
            "           4       0.32      0.02      0.05      4099\n",
            "           5       0.61      0.98      0.75     10809\n",
            "\n",
            "    accuracy                           0.61     20489\n",
            "   macro avg       0.49      0.33      0.29     20489\n",
            "weighted avg       0.53      0.61      0.49     20489\n",
            "\n",
            "--------------------------\n",
            "MODEL: Logistic Regression\n",
            "--------------------------\n",
            "The precision for this classifier is 0.661964956806091\n",
            "The recall for this classifier is    0.661964956806091\n",
            "The f1 for this classifier is        0.661964956806091\n",
            "The accuracy for this classifier is  0.661964956806091\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.78      0.71      2727\n",
            "           2       0.38      0.20      0.26      1305\n",
            "           3       0.38      0.24      0.29      1549\n",
            "           4       0.45      0.20      0.27      4099\n",
            "           5       0.72      0.93      0.81     10809\n",
            "\n",
            "    accuracy                           0.66     20489\n",
            "   macro avg       0.52      0.47      0.47     20489\n",
            "weighted avg       0.61      0.66      0.62     20489\n",
            "\n",
            "--------------------\n",
            "MODEL: Decision Tree\n",
            "--------------------\n",
            "The precision for this classifier is 0.4503880130801894\n",
            "The recall for this classifier is    0.4503880130801894\n",
            "The f1 for this classifier is        0.4503880130801894\n",
            "The accuracy for this classifier is  0.4503880130801894\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.39      0.40      2727\n",
            "           2       0.12      0.14      0.13      1305\n",
            "           3       0.11      0.11      0.11      1549\n",
            "           4       0.23      0.24      0.24      4099\n",
            "           5       0.64      0.63      0.64     10809\n",
            "\n",
            "    accuracy                           0.45     20489\n",
            "   macro avg       0.30      0.30      0.30     20489\n",
            "weighted avg       0.46      0.45      0.45     20489\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "bca-Lu2flBjP",
        "outputId": "b3f00b1e-6bbd-4015-b7c9-af4a69faa3cd"
      },
      "source": [
        "df = tfidf_doc_vec_Glove\n",
        "#concate = False  # set to True.\n",
        "#concat_df = dm_doc_vec\n",
        "\n",
        "#df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
        "\n",
        " # Specify train/valid/test size.\n",
        "train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)\n",
        "\n",
        "clfs = [RandomForestClassifier(), LogisticRegression(),DecisionTreeClassifier()]\n",
        "clf_names = ['Random Forest', 'Logistic Regression','Decision Tree']\n",
        "\n",
        "NBResults = {}\n",
        "for (i, clf_) in enumerate(clfs):\n",
        "    clf = clf_.fit(train_X, train_y)\n",
        "    preds = clf.predict(test_X)\n",
        "    \n",
        "    precision = metrics.precision_score(test_y, preds,average ='micro')\n",
        "    recall = metrics.recall_score(test_y, preds,average ='micro')\n",
        "    f1 = metrics.f1_score(test_y, preds,average ='micro')\n",
        "    accuracy = accuracy_score(test_y, preds)\n",
        "    report = classification_report(test_y, preds)\n",
        "    matrix = metrics.confusion_matrix(test_y, preds, labels=label_keys)\n",
        "    \n",
        "    data = {'precision':precision,\n",
        "            'recall':recall,\n",
        "            'f1_score':f1,\n",
        "            'accuracy':accuracy,\n",
        "            'clf_report':report,\n",
        "            'clf_matrix':matrix,\n",
        "            'y_predicted':preds}\n",
        "    \n",
        "    NBResults[clf_names[i]] = data\n",
        "\n",
        "cols = ['precision', 'recall', 'f1_score', 'accuracy']\n",
        "pd.DataFrame(NBResults).T[cols].T\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Decision Tree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.604519</td>\n",
              "      <td>0.636195</td>\n",
              "      <td>0.448631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.604519</td>\n",
              "      <td>0.636195</td>\n",
              "      <td>0.448631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.604519</td>\n",
              "      <td>0.636195</td>\n",
              "      <td>0.448631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.604519</td>\n",
              "      <td>0.636195</td>\n",
              "      <td>0.448631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Random Forest Logistic Regression Decision Tree\n",
              "precision      0.604519            0.636195      0.448631\n",
              "recall         0.604519            0.636195      0.448631\n",
              "f1_score       0.604519            0.636195      0.448631\n",
              "accuracy       0.604519            0.636195      0.448631"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fey7n7ibl24-",
        "outputId": "9ce1713d-9bc0-4f06-8dea-cefa12d47086"
      },
      "source": [
        "for model, val in NBResults.items():\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('MODEL:', model)\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('The precision for this classifier is ' + str(val['precision']))\n",
        "    print ('The recall for this classifier is    ' + str(val['recall']))\n",
        "    print ('The f1 for this classifier is        ' + str(val['f1_score']))\n",
        "    print ('The accuracy for this classifier is  ' + str(val['accuracy']))\n",
        "    print ('Here is the classification report:')\n",
        "    print (val['clf_report'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "MODEL: Random Forest\n",
            "--------------------\n",
            "The precision for this classifier is 0.604519498267363\n",
            "The recall for this classifier is    0.604519498267363\n",
            "The f1 for this classifier is        0.604519498267363\n",
            "The accuracy for this classifier is  0.604519498267363\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.65      0.64      2727\n",
            "           2       0.18      0.00      0.01      1305\n",
            "           3       0.62      0.01      0.01      1549\n",
            "           4       0.28      0.03      0.05      4099\n",
            "           5       0.61      0.97      0.75     10809\n",
            "\n",
            "    accuracy                           0.60     20489\n",
            "   macro avg       0.46      0.33      0.29     20489\n",
            "weighted avg       0.52      0.60      0.49     20489\n",
            "\n",
            "--------------------------\n",
            "MODEL: Logistic Regression\n",
            "--------------------------\n",
            "The precision for this classifier is 0.6361950314803065\n",
            "The recall for this classifier is    0.6361950314803065\n",
            "The f1 for this classifier is        0.6361950314803065\n",
            "The accuracy for this classifier is  0.6361950314803065\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.74      0.68      2727\n",
            "           2       0.36      0.13      0.19      1305\n",
            "           3       0.34      0.16      0.21      1549\n",
            "           4       0.39      0.13      0.20      4099\n",
            "           5       0.68      0.93      0.79     10809\n",
            "\n",
            "    accuracy                           0.64     20489\n",
            "   macro avg       0.48      0.42      0.41     20489\n",
            "weighted avg       0.57      0.64      0.57     20489\n",
            "\n",
            "--------------------\n",
            "MODEL: Decision Tree\n",
            "--------------------\n",
            "The precision for this classifier is 0.4486309727170677\n",
            "The recall for this classifier is    0.4486309727170677\n",
            "The f1 for this classifier is        0.4486309727170677\n",
            "The accuracy for this classifier is  0.4486309727170677\n",
            "Here is the classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.43      0.39      0.41      2727\n",
            "           2       0.14      0.15      0.15      1305\n",
            "           3       0.11      0.12      0.11      1549\n",
            "           4       0.23      0.24      0.23      4099\n",
            "           5       0.64      0.63      0.63     10809\n",
            "\n",
            "    accuracy                           0.45     20489\n",
            "   macro avg       0.31      0.31      0.31     20489\n",
            "weighted avg       0.46      0.45      0.45     20489\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "tz7UiGlwoabN",
        "outputId": "3cbe0863-da5b-4c4c-e0ca-8d5dfa251723"
      },
      "source": [
        "df = doc_vec_Glove\n",
        "#concate = False  # set to True.\n",
        "#concat_df = dm_doc_vec\n",
        "\n",
        "#df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
        "\n",
        " # Specify train/valid/test size.\n",
        "train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)\n",
        "\n",
        "clfs = [RandomForestClassifier(), LogisticRegression(),DecisionTreeClassifier()]\n",
        "clf_names = ['Random Forest', 'Logistic Regression','Decision Tree']\n",
        "\n",
        "NBResults = {}\n",
        "for (i, clf_) in enumerate(clfs):\n",
        "    clf = clf_.fit(train_X, train_y)\n",
        "    preds = clf.predict(test_X)\n",
        "    \n",
        "    precision = metrics.precision_score(test_y, preds,average ='micro')\n",
        "    recall = metrics.recall_score(test_y, preds,average ='micro')\n",
        "    f1 = metrics.f1_score(test_y, preds,average ='micro')\n",
        "    accuracy = accuracy_score(test_y, preds)\n",
        "    report = classification_report(test_y, preds)\n",
        "    matrix = metrics.confusion_matrix(test_y, preds, labels=label_keys)\n",
        "    \n",
        "    data = {'precision':precision,\n",
        "            'recall':recall,\n",
        "            'f1_score':f1,\n",
        "            'accuracy':accuracy,\n",
        "            'clf_report':report,\n",
        "            'clf_matrix':matrix,\n",
        "            'y_predicted':preds}\n",
        "    \n",
        "    NBResults[clf_names[i]] = data\n",
        "\n",
        "cols = ['precision', 'recall', 'f1_score', 'accuracy']\n",
        "pd.DataFrame(NBResults).T[cols].T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Decision Tree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.618527</td>\n",
              "      <td>0.648543</td>\n",
              "      <td>0.470692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.618527</td>\n",
              "      <td>0.648543</td>\n",
              "      <td>0.470692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.618527</td>\n",
              "      <td>0.648543</td>\n",
              "      <td>0.470692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.618527</td>\n",
              "      <td>0.648543</td>\n",
              "      <td>0.470692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Random Forest Logistic Regression Decision Tree\n",
              "precision      0.618527            0.648543      0.470692\n",
              "recall         0.618527            0.648543      0.470692\n",
              "f1_score       0.618527            0.648543      0.470692\n",
              "accuracy       0.618527            0.648543      0.470692"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVygINFqIdH"
      },
      "source": [
        "for model, val in NBResults.items():\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('MODEL:', model)\n",
        "    print ('-------'+'-'*len(model))\n",
        "    print ('The precision for this classifier is ' + str(val['precision']))\n",
        "    print ('The recall for this classifier is    ' + str(val['recall']))\n",
        "    print ('The f1 for this classifier is        ' + str(val['f1_score']))\n",
        "    print ('The accuracy for this classifier is  ' + str(val['accuracy']))\n",
        "    print ('Here is the classification report:')\n",
        "    print (val['clf_report'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "1jn6U_Nbp6t-",
        "outputId": "1f0b55b5-48a1-4f9b-8bbd-4aaa9da104ef"
      },
      "source": [
        "df = doc_vec_Glove\n",
        "concate = False  # set to True.\n",
        "concat_df = dm_doc_vec\n",
        "\n",
        "df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
        "\n",
        " # Specify train/valid/test size.\n",
        "train_size, valid_size, test_size = split_size(df, train=0.8, valid=0.)  # no need to use valid dataset here\n",
        "    # Prepare test dataset.\n",
        "train_X, test_X, train_y, test_y = train_test_split(df,\n",
        "                                                    target_labels,\n",
        "                                                    test_size=test_size,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=target_labels)\n",
        "\n",
        "clfs = [RandomForestClassifier(), LogisticRegression(),DecisionTreeClassifier()]\n",
        "clf_names = ['Random Forest', 'Logistic Regression','Decision Tree']\n",
        "\n",
        "NBResults = {}\n",
        "for (i, clf_) in enumerate(clfs):\n",
        "    clf = clf_.fit(train_X, train_y)\n",
        "    preds = clf.predict(test_X)\n",
        "    \n",
        "    precision = metrics.precision_score(test_y, preds,average ='micro')\n",
        "    recall = metrics.recall_score(test_y, preds,average ='micro')\n",
        "    f1 = metrics.f1_score(test_y, preds,average ='micro')\n",
        "    accuracy = accuracy_score(test_y, preds)\n",
        "    report = classification_report(test_y, preds)\n",
        "    matrix = metrics.confusion_matrix(test_y, preds, labels=label_keys)\n",
        "    \n",
        "    data = {'precision':precision,\n",
        "            'recall':recall,\n",
        "            'f1_score':f1,\n",
        "            'accuracy':accuracy,\n",
        "            'clf_report':report,\n",
        "            'clf_matrix':matrix,\n",
        "            'y_predicted':preds}\n",
        "    \n",
        "    NBResults[clf_names[i]] = data\n",
        "\n",
        "cols = ['precision', 'recall', 'f1_score', 'accuracy']\n",
        "pd.DataFrame(NBResults).T[cols].T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Random Forest</th>\n",
              "      <th>Logistic Regression</th>\n",
              "      <th>Decision Tree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.615208</td>\n",
              "      <td>0.666846</td>\n",
              "      <td>0.476841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.615208</td>\n",
              "      <td>0.666846</td>\n",
              "      <td>0.476841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_score</th>\n",
              "      <td>0.615208</td>\n",
              "      <td>0.666846</td>\n",
              "      <td>0.476841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.615208</td>\n",
              "      <td>0.666846</td>\n",
              "      <td>0.476841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Random Forest Logistic Regression Decision Tree\n",
              "precision      0.615208            0.666846      0.476841\n",
              "recall         0.615208            0.666846      0.476841\n",
              "f1_score       0.615208            0.666846      0.476841\n",
              "accuracy       0.615208            0.666846      0.476841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNdhTbVTAFx5"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP2LM834AHly"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "\n",
        "\n",
        "# Masking layer for pre-trained embeddings\n",
        "model.add(Masking(mask_value=0.0))\n",
        "\n",
        "# Recurrent layer\n",
        "model.add(LSTM(64, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJnlrRjzFALY"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Create callbacks\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
        "             ModelCheckpoint(filepath = 'yelp_lstm_gru_weights.hdf5', save_best_only=True, \n",
        "                             save_weights_only=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2N7jKSOFT1w"
      },
      "source": [
        "history = model.fit(X_train,  y_train, \n",
        "                    batch_size=2048, epochs=150,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "BinVOu8aHUvH",
        "outputId": "da40f19b-b54d-4661-9eb0-f08e4ea2caae"
      },
      "source": [
        "tfidf_doc_vec.info()\n",
        "tfidf_doc_vec.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 102442 entries, 0 to 102441\n",
            "Columns: 200 entries, 0 to 199\n",
            "dtypes: float64(200)\n",
            "memory usage: 156.3 MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.241534</td>\n",
              "      <td>-0.346359</td>\n",
              "      <td>-3.430429</td>\n",
              "      <td>1.740263</td>\n",
              "      <td>-0.881717</td>\n",
              "      <td>0.311229</td>\n",
              "      <td>-1.979204</td>\n",
              "      <td>-0.181652</td>\n",
              "      <td>-0.644655</td>\n",
              "      <td>-1.254866</td>\n",
              "      <td>-0.764523</td>\n",
              "      <td>-1.593858</td>\n",
              "      <td>-0.710255</td>\n",
              "      <td>5.791879</td>\n",
              "      <td>6.531380</td>\n",
              "      <td>-0.094709</td>\n",
              "      <td>4.021795</td>\n",
              "      <td>3.892041</td>\n",
              "      <td>-2.560342</td>\n",
              "      <td>0.974002</td>\n",
              "      <td>-3.033901</td>\n",
              "      <td>-5.886605</td>\n",
              "      <td>-1.866182</td>\n",
              "      <td>-4.861610</td>\n",
              "      <td>-0.216919</td>\n",
              "      <td>-1.368133</td>\n",
              "      <td>-5.397421</td>\n",
              "      <td>-1.443106</td>\n",
              "      <td>1.039358</td>\n",
              "      <td>2.114127</td>\n",
              "      <td>-5.281143</td>\n",
              "      <td>4.021247</td>\n",
              "      <td>3.425811</td>\n",
              "      <td>0.292956</td>\n",
              "      <td>0.945736</td>\n",
              "      <td>0.002539</td>\n",
              "      <td>0.709782</td>\n",
              "      <td>-2.012698</td>\n",
              "      <td>2.450704</td>\n",
              "      <td>-3.295984</td>\n",
              "      <td>...</td>\n",
              "      <td>2.739785</td>\n",
              "      <td>-1.350479</td>\n",
              "      <td>4.276579</td>\n",
              "      <td>-5.200751</td>\n",
              "      <td>-1.748180</td>\n",
              "      <td>-2.766015</td>\n",
              "      <td>-1.576548</td>\n",
              "      <td>-2.170029</td>\n",
              "      <td>-2.850688</td>\n",
              "      <td>1.789694</td>\n",
              "      <td>-0.243569</td>\n",
              "      <td>0.094091</td>\n",
              "      <td>-2.139257</td>\n",
              "      <td>3.867438</td>\n",
              "      <td>-1.345510</td>\n",
              "      <td>-1.114932</td>\n",
              "      <td>4.069699</td>\n",
              "      <td>1.102934</td>\n",
              "      <td>-3.819638</td>\n",
              "      <td>-1.040834</td>\n",
              "      <td>-1.044949</td>\n",
              "      <td>-1.570153</td>\n",
              "      <td>-1.827440</td>\n",
              "      <td>-0.271423</td>\n",
              "      <td>3.345941</td>\n",
              "      <td>1.119662</td>\n",
              "      <td>-2.546671</td>\n",
              "      <td>2.289669</td>\n",
              "      <td>-1.249396</td>\n",
              "      <td>4.977643</td>\n",
              "      <td>-5.243020</td>\n",
              "      <td>5.681765</td>\n",
              "      <td>0.641808</td>\n",
              "      <td>5.069246</td>\n",
              "      <td>-0.921466</td>\n",
              "      <td>-1.215854</td>\n",
              "      <td>-3.658603</td>\n",
              "      <td>-0.201084</td>\n",
              "      <td>2.376911</td>\n",
              "      <td>-2.882553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-4.256276</td>\n",
              "      <td>7.571361</td>\n",
              "      <td>6.658780</td>\n",
              "      <td>-4.826914</td>\n",
              "      <td>2.247503</td>\n",
              "      <td>-8.215537</td>\n",
              "      <td>2.946768</td>\n",
              "      <td>6.347083</td>\n",
              "      <td>0.474824</td>\n",
              "      <td>-5.031849</td>\n",
              "      <td>4.728989</td>\n",
              "      <td>-1.718058</td>\n",
              "      <td>4.690131</td>\n",
              "      <td>2.545589</td>\n",
              "      <td>0.813820</td>\n",
              "      <td>-0.709955</td>\n",
              "      <td>1.880499</td>\n",
              "      <td>0.800798</td>\n",
              "      <td>-2.287667</td>\n",
              "      <td>-1.955160</td>\n",
              "      <td>0.674691</td>\n",
              "      <td>-5.252521</td>\n",
              "      <td>0.695431</td>\n",
              "      <td>4.107460</td>\n",
              "      <td>-4.718019</td>\n",
              "      <td>1.965088</td>\n",
              "      <td>5.104800</td>\n",
              "      <td>3.402157</td>\n",
              "      <td>-0.837587</td>\n",
              "      <td>2.011100</td>\n",
              "      <td>2.745435</td>\n",
              "      <td>-3.664337</td>\n",
              "      <td>-3.892045</td>\n",
              "      <td>-7.367734</td>\n",
              "      <td>-3.790738</td>\n",
              "      <td>-2.515052</td>\n",
              "      <td>-5.052354</td>\n",
              "      <td>4.241956</td>\n",
              "      <td>-0.099118</td>\n",
              "      <td>2.694211</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.071032</td>\n",
              "      <td>4.440019</td>\n",
              "      <td>-0.080174</td>\n",
              "      <td>0.850033</td>\n",
              "      <td>5.172100</td>\n",
              "      <td>6.572573</td>\n",
              "      <td>1.007506</td>\n",
              "      <td>-1.624678</td>\n",
              "      <td>-1.139039</td>\n",
              "      <td>4.209938</td>\n",
              "      <td>6.290622</td>\n",
              "      <td>-3.402455</td>\n",
              "      <td>-3.605804</td>\n",
              "      <td>-6.764954</td>\n",
              "      <td>2.966686</td>\n",
              "      <td>-3.687046</td>\n",
              "      <td>-1.158803</td>\n",
              "      <td>1.585174</td>\n",
              "      <td>1.999677</td>\n",
              "      <td>-1.591179</td>\n",
              "      <td>2.933650</td>\n",
              "      <td>-1.906195</td>\n",
              "      <td>3.296059</td>\n",
              "      <td>-2.718283</td>\n",
              "      <td>-1.080722</td>\n",
              "      <td>-2.002822</td>\n",
              "      <td>-0.129199</td>\n",
              "      <td>-1.778279</td>\n",
              "      <td>-3.614841</td>\n",
              "      <td>-6.437204</td>\n",
              "      <td>1.577998</td>\n",
              "      <td>4.989604</td>\n",
              "      <td>3.143239</td>\n",
              "      <td>0.496878</td>\n",
              "      <td>5.884332</td>\n",
              "      <td>-2.108940</td>\n",
              "      <td>1.566612</td>\n",
              "      <td>-3.506336</td>\n",
              "      <td>-5.213051</td>\n",
              "      <td>-2.940182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.655971</td>\n",
              "      <td>2.690683</td>\n",
              "      <td>-0.239536</td>\n",
              "      <td>-1.089188</td>\n",
              "      <td>1.030210</td>\n",
              "      <td>1.825364</td>\n",
              "      <td>1.064760</td>\n",
              "      <td>2.333301</td>\n",
              "      <td>0.073958</td>\n",
              "      <td>-1.068034</td>\n",
              "      <td>1.397268</td>\n",
              "      <td>0.406684</td>\n",
              "      <td>-1.991326</td>\n",
              "      <td>1.669880</td>\n",
              "      <td>0.834133</td>\n",
              "      <td>-0.736294</td>\n",
              "      <td>-3.435335</td>\n",
              "      <td>-0.990510</td>\n",
              "      <td>-0.073842</td>\n",
              "      <td>-0.899898</td>\n",
              "      <td>-3.570755</td>\n",
              "      <td>0.048199</td>\n",
              "      <td>-1.467797</td>\n",
              "      <td>1.780982</td>\n",
              "      <td>-1.141539</td>\n",
              "      <td>-2.411931</td>\n",
              "      <td>1.425550</td>\n",
              "      <td>0.820652</td>\n",
              "      <td>0.430510</td>\n",
              "      <td>1.073295</td>\n",
              "      <td>0.629742</td>\n",
              "      <td>1.864826</td>\n",
              "      <td>1.177647</td>\n",
              "      <td>-1.807969</td>\n",
              "      <td>-0.794460</td>\n",
              "      <td>2.097760</td>\n",
              "      <td>2.439232</td>\n",
              "      <td>2.420574</td>\n",
              "      <td>3.230696</td>\n",
              "      <td>0.632595</td>\n",
              "      <td>...</td>\n",
              "      <td>1.843291</td>\n",
              "      <td>2.876377</td>\n",
              "      <td>-1.803974</td>\n",
              "      <td>-1.793558</td>\n",
              "      <td>-0.390911</td>\n",
              "      <td>-1.739682</td>\n",
              "      <td>-3.015334</td>\n",
              "      <td>0.399557</td>\n",
              "      <td>-2.420814</td>\n",
              "      <td>2.205678</td>\n",
              "      <td>-2.252358</td>\n",
              "      <td>0.618523</td>\n",
              "      <td>2.635010</td>\n",
              "      <td>2.740447</td>\n",
              "      <td>2.677685</td>\n",
              "      <td>-6.293159</td>\n",
              "      <td>-1.923197</td>\n",
              "      <td>1.295902</td>\n",
              "      <td>-0.751943</td>\n",
              "      <td>-4.281597</td>\n",
              "      <td>-2.643601</td>\n",
              "      <td>-0.853006</td>\n",
              "      <td>5.780793</td>\n",
              "      <td>0.947092</td>\n",
              "      <td>0.206086</td>\n",
              "      <td>-1.130668</td>\n",
              "      <td>0.023397</td>\n",
              "      <td>1.084631</td>\n",
              "      <td>2.764695</td>\n",
              "      <td>-1.054006</td>\n",
              "      <td>-2.653031</td>\n",
              "      <td>1.762260</td>\n",
              "      <td>0.672126</td>\n",
              "      <td>1.644587</td>\n",
              "      <td>2.107002</td>\n",
              "      <td>3.911186</td>\n",
              "      <td>2.789545</td>\n",
              "      <td>-4.561905</td>\n",
              "      <td>-0.893687</td>\n",
              "      <td>-3.496453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.036750</td>\n",
              "      <td>-4.824581</td>\n",
              "      <td>4.206439</td>\n",
              "      <td>-2.848737</td>\n",
              "      <td>1.616262</td>\n",
              "      <td>-3.578638</td>\n",
              "      <td>-1.332959</td>\n",
              "      <td>-1.482494</td>\n",
              "      <td>1.217810</td>\n",
              "      <td>-0.974078</td>\n",
              "      <td>-1.421062</td>\n",
              "      <td>-2.796275</td>\n",
              "      <td>-1.916879</td>\n",
              "      <td>1.660348</td>\n",
              "      <td>-1.376542</td>\n",
              "      <td>-2.248796</td>\n",
              "      <td>-1.713045</td>\n",
              "      <td>0.813905</td>\n",
              "      <td>-0.667315</td>\n",
              "      <td>-2.356043</td>\n",
              "      <td>0.979270</td>\n",
              "      <td>-0.081409</td>\n",
              "      <td>-1.116760</td>\n",
              "      <td>1.621463</td>\n",
              "      <td>-1.324630</td>\n",
              "      <td>-0.408252</td>\n",
              "      <td>4.920693</td>\n",
              "      <td>-0.525330</td>\n",
              "      <td>0.363341</td>\n",
              "      <td>-0.319720</td>\n",
              "      <td>-3.637743</td>\n",
              "      <td>-0.820320</td>\n",
              "      <td>3.643006</td>\n",
              "      <td>2.674333</td>\n",
              "      <td>-3.114033</td>\n",
              "      <td>-3.036408</td>\n",
              "      <td>2.789954</td>\n",
              "      <td>-2.692105</td>\n",
              "      <td>1.084654</td>\n",
              "      <td>3.167500</td>\n",
              "      <td>...</td>\n",
              "      <td>4.865672</td>\n",
              "      <td>-2.302689</td>\n",
              "      <td>4.509201</td>\n",
              "      <td>2.358053</td>\n",
              "      <td>-1.162396</td>\n",
              "      <td>1.977106</td>\n",
              "      <td>0.350616</td>\n",
              "      <td>0.651786</td>\n",
              "      <td>2.509765</td>\n",
              "      <td>3.054227</td>\n",
              "      <td>2.250558</td>\n",
              "      <td>1.893524</td>\n",
              "      <td>-0.308948</td>\n",
              "      <td>0.329153</td>\n",
              "      <td>-1.069556</td>\n",
              "      <td>7.370651</td>\n",
              "      <td>0.400089</td>\n",
              "      <td>2.070981</td>\n",
              "      <td>0.203354</td>\n",
              "      <td>1.388923</td>\n",
              "      <td>0.570043</td>\n",
              "      <td>-0.057124</td>\n",
              "      <td>4.142462</td>\n",
              "      <td>0.414542</td>\n",
              "      <td>-0.128299</td>\n",
              "      <td>-1.227175</td>\n",
              "      <td>0.095076</td>\n",
              "      <td>0.577437</td>\n",
              "      <td>-3.428213</td>\n",
              "      <td>-0.731980</td>\n",
              "      <td>2.572657</td>\n",
              "      <td>3.157101</td>\n",
              "      <td>-3.124300</td>\n",
              "      <td>-2.856102</td>\n",
              "      <td>-2.432476</td>\n",
              "      <td>1.572566</td>\n",
              "      <td>1.261692</td>\n",
              "      <td>-5.538431</td>\n",
              "      <td>-1.663208</td>\n",
              "      <td>5.698810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.407166</td>\n",
              "      <td>-1.889094</td>\n",
              "      <td>3.155638</td>\n",
              "      <td>-0.763353</td>\n",
              "      <td>0.410485</td>\n",
              "      <td>-1.845323</td>\n",
              "      <td>-4.484404</td>\n",
              "      <td>-3.674747</td>\n",
              "      <td>0.105887</td>\n",
              "      <td>2.348912</td>\n",
              "      <td>3.278773</td>\n",
              "      <td>-1.409052</td>\n",
              "      <td>-2.402828</td>\n",
              "      <td>0.514554</td>\n",
              "      <td>0.610011</td>\n",
              "      <td>-1.077408</td>\n",
              "      <td>-1.824826</td>\n",
              "      <td>-0.865337</td>\n",
              "      <td>-0.888725</td>\n",
              "      <td>-2.125015</td>\n",
              "      <td>-1.113216</td>\n",
              "      <td>-1.629092</td>\n",
              "      <td>-1.249926</td>\n",
              "      <td>-2.004465</td>\n",
              "      <td>-0.209894</td>\n",
              "      <td>2.297168</td>\n",
              "      <td>3.067438</td>\n",
              "      <td>-3.503512</td>\n",
              "      <td>1.606484</td>\n",
              "      <td>-0.844252</td>\n",
              "      <td>1.130460</td>\n",
              "      <td>2.512684</td>\n",
              "      <td>0.436818</td>\n",
              "      <td>2.080024</td>\n",
              "      <td>-6.421183</td>\n",
              "      <td>-4.124078</td>\n",
              "      <td>1.182407</td>\n",
              "      <td>0.216152</td>\n",
              "      <td>2.032705</td>\n",
              "      <td>4.664596</td>\n",
              "      <td>...</td>\n",
              "      <td>7.615785</td>\n",
              "      <td>-0.463298</td>\n",
              "      <td>2.431035</td>\n",
              "      <td>2.073502</td>\n",
              "      <td>3.424167</td>\n",
              "      <td>3.150250</td>\n",
              "      <td>2.360209</td>\n",
              "      <td>1.179626</td>\n",
              "      <td>-0.517552</td>\n",
              "      <td>-0.493751</td>\n",
              "      <td>3.996435</td>\n",
              "      <td>1.174502</td>\n",
              "      <td>-2.608114</td>\n",
              "      <td>1.326632</td>\n",
              "      <td>-0.256042</td>\n",
              "      <td>3.152467</td>\n",
              "      <td>-2.050098</td>\n",
              "      <td>-3.396377</td>\n",
              "      <td>2.757633</td>\n",
              "      <td>1.831535</td>\n",
              "      <td>-0.138438</td>\n",
              "      <td>-0.902062</td>\n",
              "      <td>-0.626832</td>\n",
              "      <td>0.175600</td>\n",
              "      <td>1.505481</td>\n",
              "      <td>-1.170212</td>\n",
              "      <td>0.986665</td>\n",
              "      <td>-0.466880</td>\n",
              "      <td>5.122849</td>\n",
              "      <td>4.613226</td>\n",
              "      <td>-0.041371</td>\n",
              "      <td>3.270757</td>\n",
              "      <td>-5.278025</td>\n",
              "      <td>-1.001384</td>\n",
              "      <td>2.151574</td>\n",
              "      <td>-0.159190</td>\n",
              "      <td>-1.968791</td>\n",
              "      <td>-0.348048</td>\n",
              "      <td>1.632455</td>\n",
              "      <td>4.105619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       197       198       199\n",
              "0  4.241534 -0.346359 -3.430429  ... -0.201084  2.376911 -2.882553\n",
              "1 -4.256276  7.571361  6.658780  ... -3.506336 -5.213051 -2.940182\n",
              "2 -1.655971  2.690683 -0.239536  ... -4.561905 -0.893687 -3.496453\n",
              "3 -2.036750 -4.824581  4.206439  ... -5.538431 -1.663208  5.698810\n",
              "4 -3.407166 -1.889094  3.155638  ... -0.348048  1.632455  4.105619\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-fIDGPKJuSO"
      },
      "source": [
        "model = model  # or choose sgd.\n",
        "df = tfidf_doc_vec\n",
        "concate = True  # set to True.\n",
        "concat_df = dm_doc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2HEfGpMqFdSn",
        "outputId": "065d985b-3339-42e7-e83a-764630e32bcc"
      },
      "source": [
        "clf, train_X, valid_X, test_X, train_y, valid_y, test_y = main(model, \n",
        "                                                               df, \n",
        "                                                               concate=concate, \n",
        "                                                               concat_df=concat_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_X: (81953, 300)\n",
            "Shape of valid_X: (0, 0)\n",
            "Shape of text_X: (20489, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-e4af4d0cb4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                                \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                \u001b[0mconcate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                                concat_df=concat_df)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d741efe96557>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, df, concate, concat_df)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of text_X: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:389 call\n        outputs = layer(inputs, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:660 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 300)\n"
          ]
        }
      ]
    }
  ]
}